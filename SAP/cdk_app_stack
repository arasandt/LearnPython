"""
This is the CDK Stack creation file
"""
import json
import os
import re
import time
import sys
import subprocess
from pathlib import Path
from constructs import Construct
from botocore.config import Config
import aws_cdk as core
import boto3
from dotenv import dotenv_values
from aws_cdk import (
    Stack,
    CfnOutput,
    Duration,
    RemovalPolicy,
    aws_s3 as s3,
    aws_glue as glue,
    aws_kms as kms,
    aws_ssm as ssm,
    aws_ec2 as ec2,
    aws_s3_deployment as s3deploy,
    aws_secretsmanager as secretsmanager,
    aws_logs as logs,
    aws_cloudfront_origins as origins,
    aws_certificatemanager as acm,
    aws_cloudfront as cloudfront,
    aws_iam as iam,
    aws_events as events,
    aws_events_targets as targets,
    aws_lambda as lambda_,
    aws_s3_notifications as s3_notify,
    aws_cloudwatch as cloudwatch,
    aws_cognito as cognito,
    aws_dynamodb as dynamodb,
    aws_cloudwatch_actions,
    aws_sns as sns,
    aws_sns_subscriptions as subs,
    aws_apigateway as apigateway,
    aws_sqs as sqs,
    aws_lambda_event_sources as les,
)

PYTHON_RUNTIME = lambda_.Runtime.PYTHON_3_9
PYTHON_VERSION = "3.9"


class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value

    def from_secrets_manager(self, arn):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return secretsmanager.Secret.from_secret_complete_arn(self, arn, arn)

    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        self.logging.info(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()
        try:
            data = data.replace("KMS_ROLE", self.kms_role.role_arn)
        except:
            pass
        try:
            data = data.replace("ACCOUNT_ID", self.account_id)
        except:
            pass
        try:
            data = data.replace("SNS_ARN", self.sns_key.role_arn)
        except:
            pass
        data = json.loads(data)
        return data

    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.cfnoutput_parameter = self.cfnoutput_parameter.replace("#", self.app_name)
        self.database_connection_parameter = (
            f"{self.cfnoutput_parameter}{self.database_connection_parameter}"
        )

    # Function to create an CloudWatch Alarm for Lambda Failures
    def create_lambda_failure_cloudwatch_alarm(self, lambda_function, lambda_name):
        """
        This function will create cloudwatch alarm for all lambda functions
        """
        lambda_errors_metric = cloudwatch.Metric(
            metric_name="Errors",
            statistic="max",
            namespace="AWS/Lambda",
            dimensions_map={"FunctionName": lambda_function.function_name},
        )
        # alarm_name = re.sub("[\W_]+", "", lambda_name)
        alarm_name = lambda_name
        alarm_lambda_errors = cloudwatch.Alarm(
            self,
            metric=lambda_errors_metric,
            id=f"{alarm_name}_errors",
            alarm_name=f"{alarm_name}_errors",
            treat_missing_data=cloudwatch.TreatMissingData.NOT_BREACHING,
            evaluation_periods=1,
            threshold=1,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_OR_EQUAL_TO_THRESHOLD,
            datapoints_to_alarm=1,
        )
        alarm_lambda_errors.add_alarm_action(
            aws_cloudwatch_actions.SnsAction(self.topic_support_group)
        )

    def common_setup_for_lambda(
        self, lambda_function, lambda_name, policy_file, asset_location
    ):
        """
        This function is for apply common properties to Lambda
        """
        # Add permissions for lambda
        lambda_policy = self.get_access_policy(
            Path(self.policies_path).joinpath(policy_file)
        )
        for policy in lambda_policy:
            lambda_function.add_to_role_policy(iam.PolicyStatement().from_json(policy))

        # Add environment vairables
        lambda_function.add_environment("APP_ENVIRONMENT", self.app_environment)
        lambda_function.add_environment("APP_NAME_TAG", self.app_name_tag)
        lambda_function.add_environment("MODULE_NAME_TAG", self.module_name_tag)
        lambda_function.add_environment("LOG_LEVEL", self.log_level)
        for envs in dotenv_values(Path(asset_location).joinpath(".env")).items():
            key, value = envs
            lambda_function.add_environment(key, value)

        # Set Lambda Retries
        lambda_function.configure_async_invoke(retry_attempts=0)

        # Create an CloudWatch Alarm for Lambda Failures
        self.create_lambda_failure_cloudwatch_alarm(lambda_function, lambda_name)

        return lambda_function

    def lambda_attributes(self, name):
        name = name.split("_")[1:]
        name = "_".join(name)
        return (
            f"{self.app_name}{getattr(self, name)}",
            getattr(self, f"{name}_description"),
            f"{name}_policy.json",
            getattr(self, name),
        )

    def create_lambda_layers(self):
        """
        This function will create lambda layers
        """
        self.lambda_layers = []

        for layer in list(filter(None, self.lambda_layers_build.split(";"))):
            asset_location = str(Path(self.code_location).joinpath(layer))
            self.logging.info(f"Code Location for {layer}: {asset_location}")
            if not self.is_velocity:
                cmds = f"pip3 install --platform manylinux2014_x86_64 --implementation cp --python {PYTHON_VERSION} --only-binary=:all: -r {asset_location}/python/requirements.txt -t {asset_location}/python"
                subprocess.check_call([cmds], shell=True)
            lambda_layers_build = lambda_.LayerVersion(
                self,
                f"{self.app_name}{layer}",
                layer_version_name=f"{self.app_name}{layer}",
                code=lambda_.Code.from_asset(asset_location),
                compatible_runtimes=[PYTHON_RUNTIME],
                compatible_architectures=[lambda_.Architecture.X86_64],
                description=f"{self.app_name_tag}-{self.module_name_tag}-{self.app_name}-{layer}",
            )
            self.lambda_layers.append((layer, lambda_layers_build))

            ssm.StringParameter(
                self,
                f"{self.app_name}{layer}parameter",
                parameter_name=f"{self.cfnoutput_parameter}/{layer}",
                string_value=lambda_layers_build.layer_version_arn,
                type=ssm.ParameterType.STRING,
            )

        # for layer in list(filter(None, self.lambda_layers_existing.split(";"))):
        #     layer_arn = self.from_ssm(f"{self.cfnoutput_parameter}/{layer}")
        #     lambda_layers_existing = lambda_.LayerVersion.from_layer_version_arn(
        #         self, layer, layer_arn
        #     )
        #     self.lambda_layers.append((layer, lambda_layers_existing))

        # for layer in list(filter(None, self.lambda_layers_arn.split(";"))):
        #     lambda_.LayerVersion.get_layer_version_by_arn(
        #         self, "checklayerversion", layer_version_arn=layer
        #     )
        #     lambda_layers_arn = lambda_.LayerVersion.from_layer_version_arn(
        #         self, layer, layer
        #     )
        #     self.lambda_layers.append((layer, lambda_layers_arn))

    @staticmethod
    def enable_ssl(bucket):
        bucket.add_to_resource_policy(
            iam.PolicyStatement(
                effect=iam.Effect.DENY,
                actions=["s3:*"],
                resources=[
                    f"{bucket.bucket_arn}/*",
                    f"{bucket.bucket_arn}",
                ],
                principals=[iam.StarPrincipal()],
                conditions={"Bool": {"aws:SecureTransport": False}},
            )
        )

    def create_buckets(self):
        """
        This function will create buckets and perform deployment
        """
        self.certificate_bucket = s3.Bucket(
            self,
            f"{self.app_name}-{self.glue_rds_connection_certificate_bucket}",
            bucket_name=f"{self.app_name}-{self.glue_rds_connection_certificate_bucket}",
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
            removal_policy=RemovalPolicy.DESTROY,
        )
        self.enable_ssl(self.certificate_bucket)

        CfnOutput(
            self,
            f"{self.app_name}certificatebucket",
            value=self.certificate_bucket.bucket_name,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}{self.glue_rds_connection_certificate_parameter}",
            parameter_name=f"{self.cfnoutput_parameter}/{self.glue_rds_connection_certificate_parameter}",
            string_value=self.certificate_bucket.bucket_name,
            type=ssm.ParameterType.STRING,
        )

    def create_kms_keys(self):
        self.kms_role = iam.Role(
            self,
            f"{self.app_name}kmsrole",
            assumed_by=iam.AccountPrincipal(self.account_id),
            description="This role will administer KMS",
            role_name=f"{self.app_name}{self.kms_role}",
        )
        self.kms_role.add_managed_policy(
            iam.ManagedPolicy.from_aws_managed_policy_name(
                "AWSKeyManagementServicePowerUser"
            )
        )
        self.kms_role.add_managed_policy(
            iam.ManagedPolicy.from_aws_managed_policy_name(
                "ResourceGroupsandTagEditorFullAccess"
            )
        )

        kms_policy = self.get_access_policy(
            Path(self.policies_path).joinpath("kmsrole_policy.json")
        )

        self.sns_key = kms.Key(
            self,
            f"{self.app_name}{self.sns_cmk_name}",
            alias=f"{self.app_name}{self.sns_cmk_name}",
            description="This key will be used to encrypt SNS Topics",
        )

        self.glue_key = kms.Key(
            self,
            f"{self.app_name}{self.glue_cmk_name}",
            alias=f"{self.app_name}{self.glue_cmk_name}",
            description="This key will be used to encrypt Glue Passwords",
        )

        for policy in kms_policy:
            self.sns_key.add_to_resource_policy(iam.PolicyStatement().from_json(policy))
            self.glue_key.add_to_resource_policy(
                iam.PolicyStatement().from_json(policy)
            )

    def create_sns(self):
        save_topic_support_group = self.topic_support_group[::]
        self.topic_support_group = sns.Topic(
            self,
            f"{self.app_name}{self.topic_support_group}",
            topic_name=f"{self.app_name}{self.topic_support_group}",
            master_key=self.sns_key,
            display_name=f"{self.app_name}{self.topic_support_group}",
        )

        for email in self.topic_support_group_email_addresses.split(";"):
            self.topic_support_group.add_subscription(subs.EmailSubscription(email))

        sns_policy = self.get_access_policy(
            Path(self.policies_path).joinpath("sns_policy.json")
        )
        for policy in sns_policy:
            self.topic_support_group.add_to_resource_policy(
                iam.PolicyStatement().from_json(policy)
            )

        ssm.StringParameter(
            self,
            f"{self.app_name}{save_topic_support_group}parameter",
            parameter_name=f"{self.cfnoutput_parameter}/{save_topic_support_group}",
            string_value=self.topic_support_group.topic_arn,
            type=ssm.ParameterType.STRING,
        )

        # Create a topic with generic sns key for basic notification
        save_topic_support_group_generic = f"{save_topic_support_group}generic"
        self.topic_support_group_generic = save_topic_support_group_generic
        self.topic_support_group_generic = sns.Topic(
            self,
            f"{self.app_name}{self.topic_support_group_generic}",
            topic_name=f"{self.app_name}{self.topic_support_group_generic}",
            master_key=kms.Alias.from_alias_name(
                self, "snsmanagedkey", "alias/aws/sns"
            ),
            display_name=f"{self.app_name}{self.topic_support_group_generic}",
        )

        for email in self.topic_support_group_email_addresses.split(";"):
            self.topic_support_group_generic.add_subscription(
                subs.EmailSubscription(email)
            )

        sns_policy = self.get_access_policy(
            Path(self.policies_path).joinpath("sns_policy.json")
        )
        for policy in sns_policy:
            self.topic_support_group_generic.add_to_resource_policy(
                iam.PolicyStatement().from_json(policy)
            )

        ssm.StringParameter(
            self,
            f"{self.app_name}{save_topic_support_group_generic}parameter",
            parameter_name=f"{self.cfnoutput_parameter}/{save_topic_support_group_generic}",
            string_value=self.topic_support_group_generic.topic_arn,
            type=ssm.ParameterType.STRING,
        )

    def create_alerts(self):
        events.Rule(
            self,
            f"{self.app_name}StepFunctionerrors",
            rule_name=f"{self.app_name}StepFunctionerrors",
            description="Notify failure or timeout or abort of Step Function",
            targets=[targets.SnsTopic(self.topic_support_group)],
            event_pattern=events.EventPattern(
                source=["aws.states"],
                detail_type=["Step Functions Execution Status Change"],
                detail={"status": ["FAILED", "TIMED_OUT", "ABORTED"]},
            ),
        )

        events.Rule(
            self,
            f"{self.app_name}GlueJoberrors",
            rule_name=f"{self.app_name}GlueJoberrors",
            description="Notify failure of Glue Job",
            targets=[targets.SnsTopic(self.topic_support_group)],
            event_pattern=events.EventPattern(
                source=["aws.glue"],
                detail_type=["Glue Job State Change"],
                detail={"state": ["FAILED"]},
            ),
        )

    def create_glue_connection(self):
        physical_conn_reqs = glue.CfnConnection.PhysicalConnectionRequirementsProperty

        database = self.from_ssm(f"{self.database_connection_parameter}/name")
        database_secretarn = self.from_ssm(
            f"{self.database_connection_parameter}/secretarnname"
        )

        database_secret = self.from_secrets_manager(database_secretarn)
        database_cluster = database_secret.secret_value_from_json(
            "dbClusterIdentifier"
        ).to_string()
        database_host = database_secret.secret_value_from_json("host").to_string()
        database_port = database_secret.secret_value_from_json("port").to_string()
        database_username = database_secret.secret_value_from_json(
            "username"
        ).to_string()
        database_password = database_secret.secret_value_from_json(
            "password"
        ).to_string()

        glue_az = self.from_ssm(f"{self.database_connection_parameter}/glueaz")
        glue_sg = self.from_ssm(
            f"{self.database_connection_parameter}/gluesecuritygroupid"
        )
        glue_subnet = self.from_ssm(f"{self.database_connection_parameter}/gluesubnet")

        glue_rds_connection = glue.CfnConnection(
            self,
            f"{self.app_name}glueconnection",
            catalog_id=self.account_id,
            connection_input=glue.CfnConnection.ConnectionInputProperty(
                connection_type="JDBC",
                name=f"{self.app_name}{database_cluster}-{database}",
                connection_properties={
                    "JDBC_CONNECTION_URL": f"jdbc:postgresql://{database_host}:{database_port}/{database}",
                    "JDBC_ENFORCE_SSL": True,
                    "CUSTOM_JDBC_CERT": f"s3://{self.certificate_bucket.bucket_name}/AmazonRootCA1.pem",
                    "SKIP_CUSTOM_JDBC_CERT_VALIDATION": False,
                    "USERNAME": database_username,
                    "PASSWORD": database_password,
                },
                physical_connection_requirements=physical_conn_reqs(
                    subnet_id=glue_subnet,
                    availability_zone=glue_az,
                    security_group_id_list=[glue_sg],
                ),
            ),
        )
        # glue_rds_connection.add_depends_on(cfn_data_catalog_encrypt_settings)

    def create_cors_validator_lambda(self):
        """
        This function will create cors validator lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment("VALID_CORS_ORIGIN", self.valid_cors_origin)

        self.cors_validator_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

        self.cors_validator_lambda = lambda_.Alias(
            self,
            f"{lambda_name}alias",
            alias_name="provisioned",
            version=self.cors_validator_lambda.current_version,
            provisioned_concurrent_executions=1,
            retry_attempts=0,
        )

        ssm.StringParameter(
            self,
            f"{lambda_name}parameter",
            parameter_name=f"{self.cfnoutput_parameter}/corsvalidator",
            string_value=self.cors_validator_lambda.function_arn,
            type=ssm.ParameterType.STRING,
        )

    def create_api_gateway_domain(self):
        self.domain_name = apigateway.DomainName(
            self,
            f"{self.app_name}apidomainname",
            domain_name=self.api_custom_domain_name,
            endpoint_type=apigateway.EndpointType.REGIONAL,
            certificate=acm.Certificate.from_certificate_arn(
                self, "apicertificate", self.api_custom_domain_name_certificate
            ),
        )

    def create_cloudfront_for_api(self):
        response_headers_policy = cloudfront.ResponseHeadersPolicy(
            self,
            f"{self.app_name}responseheaderspolicy",
            comment=f"Policy for allowing Cloudfront to connect to Regional API Gateway Endpoint for app {self.app_name}",
            response_headers_policy_name=f"{self.app_name}SecurityHeadersPolicy",
            cors_behavior=cloudfront.ResponseHeadersCorsBehavior(
                access_control_allow_credentials=False,
                access_control_allow_headers=["*"],
                access_control_allow_methods=["ALL"],
                access_control_allow_origins=["*"],
                access_control_expose_headers=["*"],
                origin_override=False,
            ),
            security_headers_behavior=cloudfront.ResponseSecurityHeadersBehavior(
                content_security_policy=cloudfront.ResponseHeadersContentSecurityPolicy(
                    content_security_policy="default-src 'self'; img-src 'self' data: https://*; child-src 'none'; object-src 'none'; script-src 'unsafe-inline' 'self' 'unsafe-eval'; style-src 'unsafe-inline' 'self'; font-src 'self' data:;",
                    override=True,
                ),
                content_type_options=cloudfront.ResponseHeadersContentTypeOptions(
                    override=True
                ),
                frame_options=cloudfront.ResponseHeadersFrameOptions(
                    frame_option=cloudfront.HeadersFrameOption.DENY, override=True
                ),
                referrer_policy=cloudfront.ResponseHeadersReferrerPolicy(
                    referrer_policy=cloudfront.HeadersReferrerPolicy.NO_REFERRER,
                    override=True,
                ),
                strict_transport_security=cloudfront.ResponseHeadersStrictTransportSecurity(
                    access_control_max_age=Duration.seconds(31536000),
                    include_subdomains=True,
                    override=True,
                    preload=True,
                ),
                xss_protection=cloudfront.ResponseHeadersXSSProtection(
                    protection=True,
                    mode_block=True,
                    override=True,
                ),
            ),
        )

        origin_request_policy = cloudfront.OriginRequestPolicy(
            self,
            f"{self.app_name}originrequestpolicy",
            comment=f"Policy for allowing Cloudfront to connect to Regional API Gateway Endpoint for app {self.app_name}",
            origin_request_policy_name=f"{self.app_name}originrequestpolicy",
            cookie_behavior=cloudfront.OriginRequestCookieBehavior.all(),
            header_behavior=cloudfront.OriginRequestHeaderBehavior.all(),
            query_string_behavior=cloudfront.OriginRequestQueryStringBehavior.all(),
        )

        cf_logs_bucket = s3.Bucket(
            self,
            f"{self.app_name}cflogsbucket",
            block_public_access=s3.BlockPublicAccess.BLOCK_ALL,
            object_ownership=s3.ObjectOwnership.BUCKET_OWNER_PREFERRED,
            removal_policy=RemovalPolicy.DESTROY,
            encryption=s3.BucketEncryption.S3_MANAGED,
        )
        cf_logs_bucket.add_to_resource_policy(
            iam.PolicyStatement(
                actions=["s3:GetBucketAcl", "s3:PutBucketAcl"],
                resources=[cf_logs_bucket.bucket_arn],
                principals=[iam.ServicePrincipal("cloudfront.amazonaws.com")],
            )
        )

        api_distribution = cloudfront.Distribution(
            self,
            f"{self.app_name}apicloudfront",
            default_behavior=cloudfront.BehaviorOptions(
                origin=origins.HttpOrigin(
                    domain_name=self.domain_name.domain_name_alias_domain_name
                ),
                viewer_protocol_policy=cloudfront.ViewerProtocolPolicy.HTTPS_ONLY,
                cache_policy=cloudfront.CachePolicy.CACHING_DISABLED,
                allowed_methods=cloudfront.AllowedMethods.ALLOW_ALL,
                origin_request_policy=origin_request_policy,
                response_headers_policy=response_headers_policy,
            ),
            enable_logging=True,
            log_bucket=cf_logs_bucket,
            certificate=acm.Certificate.from_certificate_arn(
                self,
                f"{self.app_name}cloudfrontcertificate",
                self.api_cloudfront_certificate,
            ),
            domain_names=[self.domain_name.domain_name],
            geo_restriction=cloudfront.GeoRestriction.allowlist("US", "IN", "CA"),
        )

        api_distribution.node.add_dependency(self.domain_name)
        api_distribution.node.add_dependency(cf_logs_bucket)

        ssm.StringParameter(
            self,
            f"{self.app_name}{self.api_custom_domain_name_parameter}nameparameter",
            parameter_name=f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/name",
            string_value=self.domain_name.domain_name,
            type=ssm.ParameterType.STRING,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}{self.api_custom_domain_name_parameter}aliasparameter",
            parameter_name=f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/alias",
            string_value=self.domain_name.domain_name_alias_domain_name,
            type=ssm.ParameterType.STRING,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}{self.api_custom_domain_name_parameter}zoneidparameter",
            parameter_name=f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/zoneid",
            string_value=self.domain_name.domain_name_alias_hosted_zone_id,
            type=ssm.ParameterType.STRING,
        )

    def assign_permissions(self):
        """
        Add permissions to the resources
        """
        self.sns_key.grant_admin(self.kms_role)
        self.sns_key.grant_encrypt_decrypt(self.kms_role)
        self.sns_key.grant_encrypt_decrypt(iam.ServicePrincipal("glue.amazonaws.com"))

        self.glue_key.grant_admin(self.kms_role)
        self.sns_key.grant_encrypt_decrypt(self.kms_role)
        self.sns_key.grant_encrypt_decrypt(iam.ServicePrincipal("events.amazonaws.com"))
        self.sns_key.grant_encrypt_decrypt(
            iam.ServicePrincipal("cloudwatch.amazonaws.com")
        )

    def load_buckets(self):
        s3deploy.BucketDeployment(
            self,
            f"{self.app_name}loadgluecertificate",
            sources=[
                s3deploy.Source.asset(
                    f"{self.asset_location}/{self.glue_rds_connection_certificate_bucket}"
                )
            ],
            destination_bucket=self.certificate_bucket,
        )

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.logging = kwargs.pop("logging")
        self.config = vars(kwargs.pop("config"))
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")
        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()

        self.create_kms_keys()

        self.create_sns()

        self.create_lambda_layers()

        self.create_alerts()

        self.create_buckets()

        self.create_glue_connection()

        self.create_cors_validator_lambda()

        self.create_api_gateway_domain()

        self.create_cloudfront_for_api()

        self.assign_permissions()

        self.load_buckets()
        
"""
This is the CDK Stack creation file
"""
from pathlib import Path
import json
import os
import re
import sys
import subprocess
import glob
import boto3
from dotenv import dotenv_values
from aws_cdk import (
    Stack,
    Duration,
    CfnOutput,
    RemovalPolicy,
    aws_s3 as s3,
    aws_s3_deployment as s3deploy,
    aws_glue as glue,
    aws_kms as kms,
    aws_sqs as sqs,
    aws_ssm as ssm,
    aws_dynamodb as dynamodb,
    aws_sns_subscriptions as subscriptions,
    aws_lambda_destinations as destinations,
    aws_logs as logs,
    aws_iam as iam,
    aws_lambda as lambda_,
    aws_apigateway as apigateway,
    aws_s3_notifications as s3_notify,
    aws_cloudwatch as cloudwatch,
    aws_cloudwatch_actions,
    aws_lambda_event_sources as les,
    aws_sns as sns,
    aws_ec2 as ec2,
)
from constructs import Construct
from botocore.config import Config

PYTHON_RUNTIME = lambda_.Runtime.PYTHON_3_9
PYTHON_VERSION = "3.9"


class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value

    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        self.logging.info(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()
        data = json.loads(data)
        return data

    # Function to create an CloudWatch Alarm for Lambda Failures
    def create_lambda_failure_cloudwatch_alarm(self, lambda_function, lambda_name):
        """
        This function will create cloudwatch alarm for all lambda functions
        """
        lambda_errors_metric = cloudwatch.Metric(
            metric_name="Errors",
            statistic="max",
            namespace="AWS/Lambda",
            dimensions_map={"FunctionName": lambda_function.function_name},
        )
        # alarm_name = re.sub("[\W_]+", "", lambda_name)
        alarm_name = lambda_name
        alarm_lambda_errors = cloudwatch.Alarm(
            self,
            metric=lambda_errors_metric,
            id=f"{alarm_name}_errors",
            alarm_name=f"{alarm_name}_errors",
            treat_missing_data=cloudwatch.TreatMissingData.NOT_BREACHING,
            evaluation_periods=1,
            threshold=1,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_OR_EQUAL_TO_THRESHOLD,
            datapoints_to_alarm=1,
        )
        alarm_lambda_errors.add_alarm_action(
            aws_cloudwatch_actions.SnsAction(self.failure_notification_topic)
        )

    def common_setup_for_lambda(
        self,
        lambda_function,
        lambda_name,
        policy_file,
        asset_location,
        retry_attempts=0,
    ):
        """
        This function is for apply common properties to Lambda
        """
        # Add permissions for lambda
        lambda_policy = self.get_access_policy(
            Path(self.policies_path).joinpath(policy_file)
        )
        for policy in lambda_policy:
            lambda_function.add_to_role_policy(iam.PolicyStatement().from_json(policy))

        # Add environment vairables
        lambda_function.add_environment("APP_ENVIRONMENT", self.app_environment)
        lambda_function.add_environment("APP_NAME_TAG", self.app_name_tag)
        lambda_function.add_environment("MODULE_NAME_TAG", self.module_name_tag)
        lambda_function.add_environment("LOG_LEVEL", self.log_level)
        for envs in dotenv_values(Path(asset_location).joinpath(".env")).items():
            key, value = envs
            lambda_function.add_environment(key, value)

        # Set Lambda Retries
        lambda_function.configure_async_invoke(retry_attempts=retry_attempts)

        # Create an CloudWatch Alarm for Lambda Failures
        self.create_lambda_failure_cloudwatch_alarm(lambda_function, lambda_name)

        return lambda_function

    def lambda_attributes(self, name):
        name = name.split("_")[1:]
        name = "_".join(name)
        return (
            f"{self.app_name}{getattr(self, name)}",
            getattr(self, f"{name}_description"),
            f"{name}_policy.json",
            getattr(self, name),
        )

    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.mea_api_credential_parameter = self.mea_api_credential_parameter.replace(
            "#", self.app_name
        )
        self.cfnoutput_parameter = self.cfnoutput_parameter.replace("#", self.app_name)
        self.database_connection_parameter = (
            f"{self.cfnoutput_parameter}{self.database_connection_parameter}"
        )
        self.redis_connection_parameter = (
            f"{self.cfnoutput_parameter}{self.redis_connection_parameter}"
        )

        self.mea_webhook_usage_api_key_parameter = (
            self.mea_webhook_usage_api_key_parameter.replace("#", self.app_name)
        )

        self.failure_notification_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.failure_notification_topic_parameter}"
        )
        self.failure_notification_topic = sns.Topic.from_topic_arn(
            self,
            id="mainalarmtopic",
            topic_arn=self.failure_notification_topic,
        )

        self.api_custom_domain_name = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/name",
        )
        self.api_custom_domain_name_alias = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/alias",
        )
        self.api_custom_domain_name_zoneid = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/zoneid",
        )

        self.database_connection_parameter = self.from_ssm(
            self.database_connection_parameter
        )

        self.mea_webhook_usage_api_key = self.from_ssm(
            self.mea_webhook_usage_api_key_parameter
        )

        self.email_extract_bucket = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.email_extract_bucket_parameter}"
        )
        self.email_extract_bucket = s3.Bucket.from_bucket_name(
            self,
            f"{self.app_name}{self.email_extract_bucket_parameter}",
            bucket_name=self.email_extract_bucket,
        )

        self.email_processed_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.email_processed_topic_parameter}"
        )
        self.email_processed_topic = sns.Topic.from_topic_arn(
            self,
            f"{self.app_name}{self.email_processed_topic_parameter}",
            topic_arn=self.email_processed_topic,
        )

        self.appetite_landing_bucket = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.appetite_landing_bucket_parameter}"
        )
        self.appetite_landing_bucket = s3.Bucket.from_bucket_arn(
            self,
            f"{self.app_name}{self.appetite_landing_bucket_parameter}",
            f"arn:aws:s3:::{self.appetite_landing_bucket}",
        )

        self.docingestion_bucket = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.docingestion_bucket_parameter}"
        )
        self.docingestion_bucket = s3.Bucket.from_bucket_arn(
            self,
            f"{self.app_name}{self.docingestion_bucket_parameter}",
            f"arn:aws:s3:::{self.docingestion_bucket}",
        )

        self.compute_security_group = ec2.SecurityGroup.from_security_group_id(
            self,
            f"{self.app_name}{self.compute_security_group_parameter}",
            security_group_id=self.from_ssm(
                f"{self.cfnoutput_parameter}/{self.compute_security_group_parameter}"
            ),
        )

        vpc_id = ssm.StringParameter.value_from_lookup(
            self, parameter_name=f"{self.cfnoutput_parameter}/{self.vpc_parameter}"
        )
        self.vpc = ec2.Vpc.from_lookup(self, "VPC", vpc_id=vpc_id)

    def lambda_vpc_setup(self):
        """
        This function is for getting VPC info
        """
        self.vpc = ec2.Vpc.from_lookup(self, "VPC", vpc_name=self.database_vpc)

        self.compute_security_group = ec2.SecurityGroup.from_lookup_by_name(
            self,
            "LambdaSecurityGroup",
            security_group_name=self.compute_security_group,
            vpc=self.vpc,
        )

    def assign_lambda_layers(self, layers=["All"]):
        if layers[0] == "All":
            lambda_layers = [layer_info[1] for layer_info in self.lambda_layers]
            return lambda_layers

        lambda_layers = []
        for layer_name in layers:
            for layer_info in self.lambda_layers:
                if layer_name in layer_info[0]:
                    lambda_layers.append(layer_info[1])
        return lambda_layers

    def create_lambda_layers(self):
        """
        This function will create lambda layers
        """
        self.lambda_layers = []

        for layer in list(filter(None, self.lambda_layers_build.split(";"))):
            asset_location = str(Path(self.code_location).joinpath(layer))
            self.logging.info(f"Code Location for {layer}: {asset_location}")
            if not self.is_velocity:
                cmds = f"pip3 install --platform manylinux2014_x86_64 --implementation cp --python {PYTHON_VERSION} --only-binary=:all: -r {asset_location}/python/requirements.txt -t {asset_location}/python"
                subprocess.check_call([cmds], shell=True)
            lambda_layers_build = lambda_.LayerVersion(
                self,
                f"{self.app_name}{layer}",
                layer_version_name=f"{self.app_name}{layer}",
                code=lambda_.Code.from_asset(asset_location),
                compatible_runtimes=[PYTHON_RUNTIME],
                compatible_architectures=[lambda_.Architecture.X86_64],
                description=f"{self.app_name_tag}-{self.module_name_tag}-{layer}",
            )
            self.lambda_layers.append((layer, lambda_layers_build))

        for layer in list(filter(None, self.lambda_layers_existing.split(";"))):
            layer_arn = self.from_ssm(f"{self.cfnoutput_parameter}/{layer}")
            lambda_layers_existing = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer_arn
            )
            self.lambda_layers.append((layer, lambda_layers_existing))

        for layer in list(filter(None, self.lambda_layers_arn.split(";"))):
            # self.lambda_client.get_layer_version_by_arn(Arn=layer)
            lambda_layers_arn = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer
            )
            self.lambda_layers.append((layer, lambda_layers_arn))

    def create_mea_webhook_lambda(self):
        """
        This function will create trigger the glu jobs and check for its completion
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(180),
            memory_size=128,
            layers=self.assign_lambda_layers(),
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        lambda_function.add_event_source(
            les.SqsEventSource(self.cipmea_webhook_intake_queue)
        )

        # Add custom environment variables
        lambda_function.add_environment("FETCH_URL", self.mea_fetch_lambda_func_url.url)
        lambda_function.add_environment("APP_NAME", self.app_name)

        self.mea_webhook_lambda = self.common_setup_for_lambda(
            lambda_function,
            lambda_name,
            lambda_policy_file,
            asset_location,
            # destination=destinations.SqsDestination(self.webhook_errors_queue),
        )

    def create_mea_fetch_lambda(self):
        """
        This function will create trigger the glu jobs and check for its completion
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(180),
            memory_size=128,
            layers=self.assign_lambda_layers(),
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
            vpc=self.vpc,
            security_groups=[self.compute_security_group],
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "MEA_API_CREDENTIAL_PARAMETER", self.mea_api_credential_parameter
        )
        lambda_function.add_environment("MEA_BASE_URL", self.mea_base_url)
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )
        lambda_function.add_environment(
            "REDIS_CONNECTION_PARAMETER", self.redis_connection_parameter
        )
        lambda_function.add_environment("REDIS_TTL", self.redis_ttl)

        # Add function URL
        cors_options = lambda_.FunctionUrlCorsOptions(
            allowed_origins=["*"],
            allowed_methods=[lambda_.HttpMethod.POST],
            allowed_headers=["*"],
        )
        self.mea_fetch_lambda_func_url = lambda_function.add_function_url(
            auth_type=lambda_.FunctionUrlAuthType.NONE,
            # auth_type=lambda_.FunctionUrlAuthType.NONE, cors=cors_options
        )

        self.mea_fetch_lambda = self.common_setup_for_lambda(
            lambda_function,
            lambda_name,
            lambda_policy_file,
            asset_location,
            retry_attempts=2,
        )

    def create_docingestion_submitter_lambda(self):
        """
        This function will submit the email attachments to document ingestion module
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            # layers=self.assign_lambda_layers(),
            handler="index.lambda_handler",
            timeout=Duration.seconds(600),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add SNS Trigger to Lambda
        lambda_function.add_event_source(les.SnsEventSource(self.email_processed_topic))

        # Add custom environment variables
        lambda_function.add_environment(
            "DOCINGESTION_BUCKET", self.docingestion_bucket.bucket_name
        )

        self.docingestion_submitter_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_mea_submitter_lambda(self):
        """
        This function will submit the email attachments to MEA
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            layers=self.assign_lambda_layers(),
            handler="index.lambda_handler",
            timeout=Duration.seconds(120),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
            vpc=self.vpc,
            security_groups=[self.compute_security_group],
        )

        # Add Queue Trigger to Lambda
        lambda_function.add_event_source(les.SqsEventSource(self.mea_intake_queue))

        # Add custom environment variables
        lambda_function.add_environment(
            "EMAIL_EXTRACT_BUCKET", self.email_extract_bucket.bucket_name
        )
        lambda_function.add_environment(
            "MEA_API_CREDENTIAL_PARAMETER", self.mea_api_credential_parameter
        )
        lambda_function.add_environment(
            "REDIS_CONNECTION_PARAMETER", self.redis_connection_parameter
        )
        lambda_function.add_environment(
            "MEASUBMITLOG_TABLE", self.measubmitlog_table.table_name
        )
        lambda_function.add_environment("MEA_BASE_URL", self.mea_base_url)
        lambda_function.add_environment("APP_NAME", self.app_name)
        lambda_function.add_environment("REDIS_TTL", self.redis_ttl)

        self.mea_submitter_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_sns_sqs_integration_kms_key(self):
        self.sns_sqs_key = kms.Key(
            self,
            f"{self.app_name}snssqskey",
            description="KMS Key for SNS topic & SQS queue",
            alias=f"{self.app_name}snssqskey",
            enable_key_rotation=True,
            pending_window=Duration.days(7),
            removal_policy=RemovalPolicy.DESTROY,
        )

        # KMS Key Policy
        self.sns_sqs_key.add_to_resource_policy(
            iam.PolicyStatement(
                effect=iam.Effect.ALLOW,
                actions=[
                    "kms:CreateGrant",
                    "kms:Decrypt",
                    "kms:DescribeKey",
                    "kms:Encrypt",
                    "kms:GenerateDataKey",
                    "kms:GenerateDataKeyPair",
                    "kms:GenerateDataKeyPairWithoutPlaintext",
                    "kms:GenerateDataKeyWithoutPlaintext",
                    "kms:ReEncryptTo",
                    "kms:ReEncryptFrom",
                    "kms:ListAliases",
                    "kms:ListGrants",
                    "kms:ListKeys",
                    "kms:ListKeyPolicies",
                ],
                resources=["*"],
                principals=[
                    iam.ServicePrincipal("sqs.amazonaws.com"),
                    iam.ServicePrincipal("sns.amazonaws.com"),
                ],
            )
        )

    def create_queue(
        self,
        queue_name,
        visibility_timeout,
        encryption_master_key=None,
    ):
        """
        This function will create queue and dlqs
        """
        queue_dlq = sqs.Queue(
            self,
            f"DLQ_{self.app_name}{queue_name}",
            queue_name=f"DLQ_{self.app_name}{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            retention_period=Duration.seconds(1209600),
        )
        queue = sqs.Queue(
            self,
            f"{self.app_name}{queue_name}",
            queue_name=f"{self.app_name}{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            visibility_timeout=Duration.seconds(visibility_timeout),
            dead_letter_queue=sqs.DeadLetterQueue(max_receive_count=3, queue=queue_dlq),
            retention_period=Duration.seconds(1209600),
        )
        return queue

    @staticmethod
    def add_options_to_apigateway_method(apimethod):
        """
        This function will create OPTIONS method for API Gateway resource
        """
        options_ig = apigateway.IntegrationResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": "'Content-Type,Authorization,X-Amz-Date,X-Api-Key,X-Amz-Security-Token'",
                "method.response.header.Access-Control-Allow-Methods": "'DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT'",
                "method.response.header.Access-Control-Allow-Origin": "'*'",
            },
            # response_templates={"application/json": ""},
        )

        options_mr = apigateway.MethodResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": True,
                "method.response.header.Access-Control-Allow-Methods": True,
                "method.response.header.Access-Control-Allow-Origin": True,
            },
            # response_models={"application/json": ""},
        )

        apimethod.add_method(
            "OPTIONS",
            integration=apigateway.MockIntegration(
                request_templates={"application/json": '{ "statusCode": 200 }'},
                integration_responses=[options_ig],
            ),
            method_responses=[options_mr],
        )

        return apimethod

    def create_api_gateway(self):
        """
        This function will create API Gateway and resources
        """

        self.sqs_integration_role = iam.Role(
            self,
            f"{self.app_name}sqsintegrationrole",
            assumed_by=iam.ServicePrincipal("apigateway.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name("AmazonSQSFullAccess")
            ],
        )

        self.mea_api_name = f"{self.app_name}{self.mea_api_name}"

        log_group = logs.LogGroup(
            self,
            f"{self.mea_api_name}apilogs",
            log_group_name=f"{self.mea_api_name}apilogs",
        )

        mea_api = apigateway.RestApi(
            self,
            self.mea_api_name,
            rest_api_name=self.mea_api_name,
            retain_deployments=False,
            deploy=False,
            endpoint_types=[apigateway.EndpointType.REGIONAL],
            # disable_execute_api_endpoint=True,
        )

        mea_webhook_resource = mea_api.root.add_resource(self.mea_api_resource_name)
        # mea_webhook_resource.add_method(
        #     "POST",
        #     apigateway.LambdaIntegration(
        #         self.mea_webhook_lambda, allow_test_invoke=False
        #     ),
        #     api_key_required=True,
        # )

        mea_webhook_sqs_integration_response = apigateway.IntegrationResponse(
            status_code="200",
            response_templates={"application/json": ""},
        )
        mea_webhook_sqs_integration_options = apigateway.IntegrationOptions(
            credentials_role=self.sqs_integration_role,
            integration_responses=[mea_webhook_sqs_integration_response],
            request_templates={
                "application/json": "Action=SendMessage&MessageBody=$util.urlEncode($input.body)"
            },
            passthrough_behavior=apigateway.PassthroughBehavior.NEVER,
            request_parameters={
                "integration.request.header.Content-Type": "'application/x-www-form-urlencoded'"
            },
        )

        mea_webhook_sqs_integration = apigateway.AwsIntegration(
            service="sqs",
            integration_http_method="POST",
            path="{}/{}".format(
                self.account_id, self.cipmea_webhook_intake_queue.queue_name
            ),
            options=mea_webhook_sqs_integration_options,
        )

        mea_webhook_sqs_method_response = apigateway.MethodResponse(status_code="200")

        # Add the API GW Integration to the "example" API GW Resource
        mea_webhook_resource.add_method(
            "POST",
            mea_webhook_sqs_integration,
            method_responses=[mea_webhook_sqs_method_response],
            api_key_required=True,
        )
        mea_webhook_resource = self.add_options_to_apigateway_method(
            mea_webhook_resource
        )

        deployment = apigateway.Deployment(self, "Deployment1", api=mea_api)

        stage = apigateway.Stage(
            self,
            self.app_environment,
            stage_name=self.app_environment,
            deployment=deployment,
            logging_level=apigateway.MethodLoggingLevel.INFO,
            data_trace_enabled=True,
            access_log_destination=apigateway.LogGroupLogDestination(log_group),
            access_log_format=apigateway.AccessLogFormat.json_with_standard_fields(
                caller=False,
                http_method=True,
                ip=True,
                protocol=True,
                request_time=True,
                resource_path=True,
                response_length=True,
                status=True,
                user=True,
            ),
        )

        mea_api_key = apigateway.ApiKey(
            self,
            f"{self.mea_api_name}apikey",
            api_key_name=self.mea_api_name,
            value=self.mea_webhook_usage_api_key,
        )

        mea_plan = mea_api.add_usage_plan(
            f"{self.mea_api_name}usageplan",
            name=self.mea_api_name,
            throttle={"rate_limit": 100, "burst_limit": 100},
            api_stages=[apigateway.UsagePlanPerApiStage(api=mea_api, stage=stage)],
        )

        mea_plan.add_api_key(mea_api_key)

        domain_name = apigateway.DomainName.from_domain_name_attributes(
            self,
            self.api_custom_domain_name_parameter,
            domain_name=self.api_custom_domain_name,
            domain_name_alias_hosted_zone_id=self.api_custom_domain_name_zoneid,
            domain_name_alias_target=self.api_custom_domain_name_alias,
        )

        apigateway.BasePathMapping(
            self,
            f"{self.mea_api_name}pathmapping",
            domain_name=domain_name,
            rest_api=mea_api,
            base_path=self.mea_api_name.replace(self.app_name, ""),
            stage=stage,
        )

    def assign_permissions(self):
        """
        Add permissions to the resources
        """
        self.mea_intake_queue.grant_consume_messages(self.mea_submitter_lambda)
        self.email_extract_bucket.grant_read(self.docingestion_submitter_lambda)

        self.email_extract_bucket.grant_read_write(self.mea_submitter_lambda)
        self.appetite_landing_bucket.grant_read_write(self.mea_fetch_lambda)
        self.docingestion_bucket.grant_read_write(self.docingestion_submitter_lambda)

        self.measubmitlog_table.grant_read_write_data(self.mea_submitter_lambda)

    def create_dynamodb(self):
        self.measubmitlog_table = dynamodb.Table(
            self,
            f"{self.app_name}-{self.log_table}",
            table_name=f"{self.app_name}-{self.log_table}",
            partition_key=dynamodb.Attribute(
                name="applicationid", type=dynamodb.AttributeType.STRING
            ),
            removal_policy=RemovalPolicy.RETAIN,
            billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,
            point_in_time_recovery=True,
        )

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.config = vars(kwargs.pop("config"))
        self.logging = kwargs.pop("logging")
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")

        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()

        self.create_dynamodb()

        self.create_lambda_layers()

        self.create_sns_sqs_integration_kms_key()

        self.mea_intake_queue = self.create_queue(
            self.mea_intake_queue, 120, encryption_master_key=self.sns_sqs_key
        )

        self.email_processed_topic.add_subscription(
            subscriptions.SqsSubscription(self.mea_intake_queue)
        )

        self.create_mea_submitter_lambda()

        self.cipmea_webhook_intake_queue = self.create_queue(
            self.cipmea_webhook_intake_queue, 180
        )

        self.create_mea_fetch_lambda()

        self.create_mea_webhook_lambda()

        self.create_docingestion_submitter_lambda()

        self.create_api_gateway()

        self.assign_permissions()
"""
This is the CDK Stack creation file
"""
from pathlib import Path
import json
import os
import re
import sys

# import secrets
import subprocess
from dotenv import dotenv_values
from aws_cdk import (
    Stack,
    Duration,
    CfnOutput,
    RemovalPolicy,
    aws_s3 as s3,
    aws_iam as iam,
    aws_ssm as ssm,
    aws_ec2 as ec2,
    aws_rds as rds,
    aws_secretsmanager as secretsmanager,
    aws_elasticache as elasticache,
)
from constructs import Construct

from aws_cdk.custom_resources import (
    AwsCustomResource,
    AwsCustomResourcePolicy,
    AwsSdkCall,
    PhysicalResourceId,
)


class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value

    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        self.logging.info(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()
        # try:
        #     data = data.replace("MODEL_BUCKET", self.model_bucket.bucket_name)
        # except Exception:
        #     pass
        data = json.loads(data)
        return data

    def lambda_attributes(self, name):
        name = name.split("_")[1:]
        name = "_".join(name)
        return (
            f"{self.app_name}{getattr(self, name)}",
            getattr(self, f"{name}_description"),
            f"{name}_policy.json",
            getattr(self, name),
        )

    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.cfnoutput_parameter = self.cfnoutput_parameter.replace("#", self.app_name)
        self.redis_cache_name = f"{self.app_name}{self.redis_cache_name}"
        self.redis_default_port = int(self.redis_default_port)

    def create_vpc(self):
        self.vpc = ec2.Vpc(
            self,
            f"{self.app_name}-vpc",
            vpc_name=f"{self.app_name}-vpc",
            flow_logs={"FlowLog": {}},
        )

        CfnOutput(
            self,
            f"{self.app_name}-vpcid",
            export_name=f"{self.app_name}-VpcId",
            value=self.vpc.vpc_id,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}vpcid",
            parameter_name=f"{self.cfnoutput_parameter}/vpcid",
            string_value=self.vpc.vpc_id,
        )

        self.redis_subnets = [i.subnet_id for i in self.vpc.private_subnets]
        self.redis_azs = [i for i in self.vpc.availability_zones]

    def create_sg(self):
        self.compute_security_group = ec2.SecurityGroup(
            self,
            f"{self.app_name}-compute-sg",
            security_group_name=f"{self.app_name}-compute-sg",
            allow_all_outbound=False,
            vpc=self.vpc,
        )
        self.compute_security_group.connections.allow_to(
            ec2.Peer.any_ipv4(),
            ec2.Port.tcp(443),
            "Allow HTTPS",
        )
        self.compute_security_group.connections.allow_to(
            ec2.Peer.any_ipv4(),
            ec2.Port.tcp(80),
            "Allow HTTP",
        )
        self.compute_security_group.connections.allow_from(
            ec2.Peer.any_ipv4(),
            ec2.Port.tcp(443),
            "Allow HTTPS",
        )
        self.compute_security_group.connections.allow_from(
            ec2.Peer.any_ipv4(),
            ec2.Port.tcp(80),
            "Allow HTTP",
        )
        self.compute_security_group.connections.allow_to(
            ec2.Peer.any_ipv4(),
            ec2.Port.tcp(9997),
            "Allow Splunk",
        )
        self.compute_security_group.connections.allow_from(
            self.compute_security_group,
            ec2.Port.all_traffic(),
            "Glue connection",
        )
        self.compute_security_group.connections.allow_to(
            self.compute_security_group,
            ec2.Port.all_traffic(),
            "Glue connection",
        )

        self.db_security_group = ec2.SecurityGroup(
            self,
            f"{self.app_name}-database-sg",
            security_group_name=f"{self.app_name}-database-sg",
            allow_all_outbound=False,
            vpc=self.vpc,
        )
        self.db_security_group.connections.allow_from(
            self.compute_security_group,
            ec2.Port.tcp(5432),
            "Allow PostgreSQL",
        )
        self.db_security_group.connections.allow_to(
            self.compute_security_group,
            ec2.Port.tcp(5432),
            "Allow PostgreSQL",
        )
        self.db_security_group.connections.allow_to(
            self.compute_security_group,
            ec2.Port.tcp_range(1024, 65535),
            "Allow ephemeral ports",
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}computesecuritygroupid",
            parameter_name=f"{self.cfnoutput_parameter}/computesecuritygroupid",
            string_value=self.compute_security_group.security_group_id,
        )
        ssm.StringParameter(
            self,
            f"{self.app_name}databasesecuritygroupid",
            parameter_name=f"{self.cfnoutput_parameter}/databasesecuritygroupid",
            string_value=self.db_security_group.security_group_id,
        )

    def create_aurora_db(self):
        database_name = f"{self.app_name}{self.database_name}"

        self.aurora_db = rds.ServerlessCluster(
            self,
            f"{database_name}databasecluster",
            cluster_identifier=f"{database_name}databasecluster",
            engine=rds.DatabaseClusterEngine.aurora_postgres(
                version=rds.AuroraPostgresEngineVersion.VER_13_9
            ),
            vpc=self.vpc,
            # parameter_group=rds.ParameterGroup.from_parameter_group_name(
            #     self, "ParameterGroup", "default.aurora-postgresql10"
            # ),
            scaling=rds.ServerlessScalingOptions(
                auto_pause=Duration.minutes(0),
                min_capacity=rds.AuroraCapacityUnit.ACU_2,
                max_capacity=rds.AuroraCapacityUnit.ACU_64,
            ),
            backup_retention=Duration.days(10),
            security_groups=[self.db_security_group],
            enable_data_api=True,
            default_database_name=database_name,
        )

        # self.aurora_db_v2 = rds.DatabaseCluster(
        #     self,
        #     "DbCluster",
        #     engine=rds.DatabaseClusterEngine.aurora_postgres(
        #         version=rds.AuroraPostgresEngineVersion.VER_14_6
        #     ),
        #     cluster_identifier=f"{database_name}databaseclusterV2",
        #     instances=2,  # DEFAULT: 2 (WRITER & READER)
        #     instance_props=rds.InstanceProps(
        #         vpc=self.vpc,
        #         instance_type=ec2.InstanceType("serverless"),
        #         auto_minor_version_upgrade=True,
        #         publicly_accessible=True,
        #         security_groups=[self.db_security_group],
        #         vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PUBLIC),
        #         enable_performance_insights=True,
        #     ),
        #     port=5432,
        #     storage_encrypted=True,
        # )

        # # get the RDS target group
        # rds_target_group = self.aurora_db_v2.node.find_child("Resource")
        # # Override Settings to ensure defaults for Serverless v2
        # rds_target_group.add_property_override(
        #     "ServerlessV2ScalingConfiguration",
        #     {"MinCapacity": "0.5", "MaxCapacity": "2"},
        # )

        ssm.StringParameter(
            self,
            f"{self.app_name}databaseconfig",
            parameter_name=f"{self.cfnoutput_parameter}/database/config",
            string_value=json.dumps(
                {
                    "name": database_name,
                    "clusterarn": self.aurora_db.cluster_arn,
                    "secretarnname": self.aurora_db.secret.secret_arn,
                }
            ),
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}databasesecretarn",
            parameter_name=f"{self.cfnoutput_parameter}/database/config/secretarnname",
            string_value=self.aurora_db.secret.secret_arn,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}databasename",
            parameter_name=f"{self.cfnoutput_parameter}/database/config/name",
            string_value=database_name,
        )
        ssm.StringParameter(
            self,
            f"{self.app_name}databaseclusterarn",
            parameter_name=f"{self.cfnoutput_parameter}/database/config/clusterarn",
            string_value=database_name,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}databaseglueaz",
            parameter_name=f"{self.cfnoutput_parameter}/database/config/glueaz",
            string_value=self.vpc.availability_zones[0],
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}databasegluesubnet",
            parameter_name=f"{self.cfnoutput_parameter}/database/config/gluesubnet",
            string_value=self.vpc.private_subnets[0].subnet_id,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}databasegluesecuritygroupid",
            parameter_name=f"{self.cfnoutput_parameter}/database/config/gluesecuritygroupid",
            string_value=self.compute_security_group.security_group_id,
        )

    def revoke_sg_egress(self):
        # adapted from https://github.com/aws/aws-cdk/issues/1606

        ingress_parameters = {
            "GroupId": self.vpc.vpc_default_security_group,
            "IpPermissions": [
                {
                    "IpProtocol": "-1",
                    "UserIdGroupPairs": [
                        {
                            "GroupId": self.vpc.vpc_default_security_group,
                        },
                    ],
                },
            ],
        }

        AwsCustomResource(
            self,
            f"{self.app_name}restrictsecuritygroupingress",
            on_create=AwsSdkCall(
                service="EC2",
                action="revokeSecurityGroupIngress",
                parameters=ingress_parameters,
                physical_resource_id=PhysicalResourceId.of(
                    f"restrict-ingress-${self.vpc.vpc_id}-"
                    f"${self.vpc.vpc_default_security_group}"
                ),
            ),
            policy=AwsCustomResourcePolicy.from_statements(
                statements=[
                    iam.PolicyStatement(
                        effect=iam.Effect.ALLOW,
                        actions=["ec2:revokeSecurityGroupIngress"],
                        resources=[
                            f"arn:aws:ec2:{self.region_name}:{self.account_id}:"
                            f"security-group/{self.vpc.vpc_default_security_group}"
                        ],
                    )
                ]
            ),
        )

        egress_parameters = {
            "GroupId": self.vpc.vpc_default_security_group,
            "IpPermissions": [
                {
                    "IpProtocol": "-1",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0",
                        },
                    ],
                },
            ],
        }

        AwsCustomResource(
            self,
            f"{self.app_name}restrictsecuritygroupegress",
            on_create=AwsSdkCall(
                service="EC2",
                action="revokeSecurityGroupEgress",
                parameters=egress_parameters,
                physical_resource_id=PhysicalResourceId.of(
                    f"restrict-egress-${self.vpc.vpc_id}-"
                    f"${self.vpc.vpc_default_security_group}"
                ),
            ),
            policy=AwsCustomResourcePolicy.from_statements(
                statements=[
                    iam.PolicyStatement(
                        effect=iam.Effect.ALLOW,
                        actions=["ec2:revokeSecurityGroupEgress"],
                        resources=[
                            f"arn:aws:ec2:{self.region_name}:{self.account_id}:"
                            f"security-group/{self.vpc.vpc_default_security_group}"
                        ],
                    )
                ]
            ),
        )

    def create_compute_instance(self):
        # amzn_linux = ec2.MachineImage.latest_amazon_linux(
        #     generation=ec2.AmazonLinuxGeneration.AMAZON_LINUX_2,
        #     edition=ec2.AmazonLinuxEdition.STANDARD,
        #     virtualization=ec2.AmazonLinuxVirt.HVM,
        #     storage=ec2.AmazonLinuxStorage.GENERAL_PURPOSE,
        # )

        amzn_linux_ami = ec2.MachineImage.latest_amazon_linux(
            generation=ec2.AmazonLinuxGeneration.AMAZON_LINUX_2
        )

        bastion_role = iam.Role(
            self,
            f"{self.app_name}instancessmrole",
            assumed_by=iam.ServicePrincipal("ec2.amazonaws.com"),
        )
        bastion_role.add_managed_policy(
            iam.ManagedPolicy.from_aws_managed_policy_name(
                "AmazonSSMManagedInstanceCore"
            )
        )

        bastion_instance = ec2.Instance(
            self,
            f"{self.app_name}bastion",
            instance_name=f"{self.app_name}bastion",
            instance_type=ec2.InstanceType("t3.medium"),
            machine_image=amzn_linux_ami,
            vpc=self.vpc,
            vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PUBLIC),
            role=bastion_role,
            security_group=self.compute_security_group,
            block_devices=[
                ec2.BlockDevice(
                    device_name="/dev/xvda",
                    volume=ec2.BlockDeviceVolume.ebs(
                        volume_size=10, encrypted=True, delete_on_termination=True
                    ),
                )
            ],
        )

    def create_redis_cluster(self):
        # redis_password = secrets.token_urlsafe(32)

        # redis_secret = secretsmanager.CfnSecret(
        #     self,
        #     f"{self.app_name}rediscredentials",
        #     name=f"{self.app_name}redis-credentials",
        #     secret_string=json.dumps({"token": redis_password}),
        # )

        redis_secret = secretsmanager.Secret(
            self,
            f"{self.app_name}rediscredentials",
            # secret_name=f"{self.app_name}rediscredentials",
            generate_secret_string=secretsmanager.SecretStringGenerator(
                exclude_punctuation=True,
                password_length=30,
                generate_string_key="token",
                secret_string_template='{"type": "redistoken"}',
            ),
        )

        redis_security_group = ec2.SecurityGroup(
            self,
            f"{self.app_name}-redis-sg",
            security_group_name=f"{self.app_name}-redis-sg",
            allow_all_outbound=False,
            vpc=self.vpc,
        )

        cfn_subnet_group = elasticache.CfnSubnetGroup(
            self,
            f"{self.app_name}reditsubnetgroup",
            description="Subnet group for the Redis cluster",
            subnet_ids=self.redis_subnets,
            cache_subnet_group_name=f"{self.redis_cache_name}-subnet-group",
        )

        redis_cluster = elasticache.CfnReplicationGroup(
            self,
            f"{self.app_name}redisreplicationgroup",
            replication_group_description="Redis replication group",
            preferred_cache_cluster_a_zs=self.redis_azs,
            at_rest_encryption_enabled=True,
            transit_encryption_enabled=True,
            auth_token=redis_secret.secret_value_from_json("token").to_string(),
            automatic_failover_enabled=True,
            cache_node_type="cache.r6g.large",
            num_cache_clusters=len(self.redis_azs),
            cache_parameter_group_name="default.redis7",
            cache_subnet_group_name=cfn_subnet_group.cache_subnet_group_name,
            replication_group_id=self.redis_cache_name,
            security_group_ids=[redis_security_group.security_group_id],
            engine="Redis",
        )
        redis_cluster.node.add_dependency(cfn_subnet_group)

        # self.redis_default_port = (
        #     redis_cluster.attr_primary_end_point_port
        #     if redis_cluster.attr_primary_end_point_port != ""
        #     else self.redis_default_port
        # )

        ssm.StringParameter(
            self,
            f"{self.app_name}redisconnectionparameter",
            parameter_name=f"{self.cfnoutput_parameter}/redis/config",
            string_value=json.dumps(
                {
                    "endpoint": f"{redis_cluster.attr_primary_end_point_address}:{self.redis_default_port}",
                    "secretarnname": redis_secret.secret_arn,
                }
            ),
        )

        redis_security_group.connections.allow_to(
            self.compute_security_group,
            ec2.Port.tcp_range(1024, 65535),
            "Allow ephemeral ports",
        )

        redis_security_group.connections.allow_from(
            self.compute_security_group,
            ec2.Port.tcp(self.redis_default_port),
            "Allow computes",
        )

    def assign_permissions(self):
        """
        Add permissions to the resources
        """
        pass

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        # self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.logging = kwargs.pop("logging")
        self.config = vars(kwargs.pop("config"))
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")
        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()

        self.create_vpc()

        self.create_sg()

        self.create_aurora_db()

        self.revoke_sg_egress()

        self.create_compute_instance()

        self.create_redis_cluster()

        self.assign_permissions()

"""
This is the CDK Stack creation file
"""
from pathlib import Path
import json
import os
import re
import sys
import subprocess
from dotenv import dotenv_values
from aws_cdk import (
    Stack,
    Duration,
    Size,
    CfnOutput,
    RemovalPolicy,
    aws_s3 as s3,
    aws_s3_deployment as s3deploy,
    aws_iam as iam,
    aws_logs as logs,
    aws_ssm as ssm,
    aws_kms as kms,
    aws_lambda as lambda_,
    aws_dynamodb as dynamodb,
    aws_s3_notifications as s3_notify,
    aws_cloudwatch as cloudwatch,
    aws_cloudwatch_actions,
    aws_apigateway as apigateway,
    aws_lambda_event_sources as les,
    aws_sqs as sqs,
    aws_sns as sns,
    aws_sam as sam,
    aws_athena as athena,
)
from constructs import Construct
from .stepfunctions_service import StepFunctionService

PYTHON_RUNTIME = lambda_.Runtime.PYTHON_3_9
PYTHON_VERSION = "3.9"


class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value

    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        self.logging.info(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()
        # try:
        #     data = data.replace("MODEL_BUCKET", self.model_bucket.bucket_name)
        # except Exception:
        #     pass
        data = json.loads(data)
        return data

    # Function to create an CloudWatch Alarm for Lambda Failures
    def create_lambda_failure_cloudwatch_alarm(self, lambda_function, lambda_name):
        """
        This function will create cloudwatch alarm for all lambda functions
        """
        lambda_errors_metric = cloudwatch.Metric(
            metric_name="Errors",
            statistic="max",
            namespace="AWS/Lambda",
            dimensions_map={"FunctionName": lambda_function.function_name},
        )
        # alarm_name = re.sub("[\W_]+", "", lambda_name)
        alarm_name = lambda_name
        alarm_lambda_errors = cloudwatch.Alarm(
            self,
            metric=lambda_errors_metric,
            id=f"{alarm_name}_errors",
            alarm_name=f"{alarm_name}_errors",
            treat_missing_data=cloudwatch.TreatMissingData.NOT_BREACHING,
            evaluation_periods=1,
            threshold=1,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_OR_EQUAL_TO_THRESHOLD,
            datapoints_to_alarm=1,
        )
        alarm_lambda_errors.add_alarm_action(
            aws_cloudwatch_actions.SnsAction(self.failure_notification_topic)
        )

    def common_setup_for_lambda(
        self, lambda_function, lambda_name, policy_file, asset_location
    ):
        """
        This function is for apply common properties to Lambda
        """
        # Add permissions for lambda
        lambda_policy = self.get_access_policy(
            Path(self.policies_path).joinpath(policy_file)
        )
        for policy in lambda_policy:
            lambda_function.add_to_role_policy(iam.PolicyStatement().from_json(policy))

        # Add environment vairables
        lambda_function.add_environment("APP_ENVIRONMENT", self.app_environment)
        lambda_function.add_environment("APP_NAME_TAG", self.app_name_tag)
        lambda_function.add_environment("MODULE_NAME_TAG", self.module_name_tag)
        lambda_function.add_environment("LOG_LEVEL", self.log_level)
        lambda_function.add_environment("INSTANCE", self.app_name)
        for envs in dotenv_values(Path(asset_location).joinpath(".env")).items():
            key, value = envs
            lambda_function.add_environment(key, value)

        # Set Lambda Retries
        lambda_function.configure_async_invoke(retry_attempts=0)

        # Create an CloudWatch Alarm for Lambda Failures
        self.create_lambda_failure_cloudwatch_alarm(lambda_function, lambda_name)

        return lambda_function

    def lambda_attributes(self, name):
        name = name.split("_")[1:]
        name = "_".join(name)
        return (
            f"{self.app_name}{getattr(self, name)}",
            getattr(self, f"{name}_description"),
            f"{name}_policy.json",
            getattr(self, name),
        )

    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.cfnoutput_parameter = self.cfnoutput_parameter.replace("#", self.app_name)
        self.endpoint_parameter = self.endpoint_parameter.replace("#", self.app_name)

        self.failure_notification_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.failure_notification_topic_parameter}"
        )
        self.failure_notification_topic = sns.Topic.from_topic_arn(
            self,
            id="mainalarmtopic",
            topic_arn=self.failure_notification_topic,
        )
        # self.sagemaker_endpoint_step_function_arn = f"arn:aws:states:{self.region_name}:{self.account_id}:stateMachine:{self.sagemaker_endpoint_step_function}"
        # self.comprehend_step_function_arn = f"arn:aws:states:{self.region_name}:{self.account_id}:stateMachine:{self.comprehend_step_function}"
        # can't refer to step function object arn in the lambda because lambda is created before step function.

        self.api_custom_domain_name = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/name",
        )
        self.api_custom_domain_name_alias = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/alias",
        )
        self.api_custom_domain_name_zoneid = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/zoneid",
        )

    def assign_lambda_layers(self, layers=["All"]):
        if layers[0] == "All":
            lambda_layers = [layer_info[1] for layer_info in self.lambda_layers]
            return lambda_layers

        lambda_layers = []
        for layer_name in layers:
            for layer_info in self.lambda_layers:
                if layer_name in layer_info[0]:
                    lambda_layers.append(layer_info[1])
        return lambda_layers

    def create_lambda_layers(self):
        """
        This function will create lambda layers
        """
        self.lambda_layers = []

        for layer in list(filter(None, self.lambda_layers_build.split(";"))):
            asset_location = str(Path(self.code_location).joinpath(layer))
            self.logging.info(f"Code Location for {layer}: {asset_location}")
            if not self.is_velocity:
                cmds = f"pip3 install --platform manylinux2014_x86_64 --implementation cp --python {PYTHON_VERSION} --only-binary=:all: -r {asset_location}/python/requirements.txt -t {asset_location}/python"
                subprocess.check_call([cmds], shell=True)
            lambda_layers_build = lambda_.LayerVersion(
                self,
                f"{self.app_name}{layer}",
                layer_version_name=f"{self.app_name}{layer}",
                code=lambda_.Code.from_asset(asset_location),
                compatible_runtimes=[PYTHON_RUNTIME],
                compatible_architectures=[lambda_.Architecture.X86_64],
                description=f"{self.app_name_tag}-{self.module_name_tag}-{self.app_name}-{layer}",
            )
            self.lambda_layers.append((layer, lambda_layers_build))

        for layer in list(filter(None, self.lambda_layers_existing.split(";"))):
            layer_arn = self.from_ssm(f"{self.cfnoutput_parameter}/{layer}")
            lambda_layers_existing = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer_arn
            )
            self.lambda_layers.append((layer, lambda_layers_existing))

        for layer in list(filter(None, self.lambda_layers_arn.split(";"))):
            lambda_.LayerVersion.get_layer_version_by_arn(
                self, "checklayerversion", layer_version_arn=layer
            )
            lambda_layers_arn = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer
            )
            self.lambda_layers.append((layer, lambda_layers_arn))

    def create_sagemaker_endpoint_creator_lambda(self):
        """
        This function will create sagemaker endpoint creator lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.DockerImageFunction(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.DockerImageCode().from_image_asset(directory=asset_location),
            description=lambda_desc,
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
        )

        # Add custom environment variables
        lambda_function.add_environment("ENDPOINT_PARAMETER", self.endpoint_parameter)
        lambda_function.add_environment(
            "SAGEMAKER_DEPLOY_ROLE_ARN", self.sagemaker_deploy_role.role_arn
        )

        self.sagemaker_endpoint_creator_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_stepfunction_launcher_lambda(self):
        """
        This function will create stepfunction launcher lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.model_bucket)
        self.model_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(suffix=self.sagemaker_model_trigger_suffix),
        )
        self.model_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(suffix=self.comprehend_trigger_suffix),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "SAGEMAKER_ENDPOINT_STEP_FUNCTION_CONFIG",
            f"{self.sagemaker_state_machine.state_machine_arn}#{self.sagemaker_model_trigger_suffix}",
        )
        lambda_function.add_environment(
            "COMPREHEND_STEP_FUNCTION_CONFIG",
            f"{self.comprehend_state_machine.state_machine_arn}#{self.comprehend_trigger_suffix}",
        )

        self.stepfunction_launcher_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_comprehend_trainer_watcher_lambda(self):
        """
        This function will create comprehend trainer watcher lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Create a role for Comprehend to assume to start training
        # comprehend_training_policy = self.get_access_policy(
        #     Path(self.policies_path).joinpath("comprehend_training_policy.json")
        # )
        # all_statements = []
        # for statement in comprehend_training_policy:
        #     all_statements.append(iam.PolicyStatement().from_json(statement))
        self.comprehend_training_role = iam.Role(
            self,
            f"{self.app_name}ComprehendTrainingRole",
            assumed_by=iam.ServicePrincipal(service="comprehend.amazonaws.com"),
            description="Allow for Comprehend to run training jobs with all necessary access",
            # inline_policies=dict(
            #     inline_policy_from_cdk=iam.PolicyDocument(statements=all_statements)
            # ),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "COMPREHEND_DATAACCESS_ROLE", self.comprehend_training_role.role_arn
        )

        self.comprehend_trainer_watcher_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_model_inference_lambda(self):
        """
        This function will take in all request for inferences
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")

        lambda_function = lambda_.DockerImageFunction(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.DockerImageCode().from_image_asset(directory=asset_location),
            description=lambda_desc,
            timeout=Duration.seconds(900),
            memory_size=10240,
            ephemeral_storage_size=Size.mebibytes(10240),
            tracing=lambda_.Tracing.ACTIVE,
        )

        # Add custom environment variables
        lambda_function.add_environment("ENDPOINT_PARAMETER", self.endpoint_parameter)
        lambda_function.add_environment("LOG_TABLE", self.log_table.table_name)
        lambda_function.add_environment(
            "TEXTRACT_TABLE", self.textract_table.table_name
        )
        lambda_function.add_environment(
            "TEXTRACT_BUCKET", self.textract_bucket.bucket_name
        )

        # Add function URL
        cors_options = lambda_.FunctionUrlCorsOptions(
            allowed_origins=["*"],
            allowed_methods=[lambda_.HttpMethod.ALL],
            allowed_headers=["*"],
        )
        self.model_inference_lambda_func_url = lambda_function.add_function_url(
            auth_type=lambda_.FunctionUrlAuthType.NONE, cors=cors_options
        )

        CfnOutput(
            self,
            f"{self.app_name}modelinferencefunctionurl",
            value=self.model_inference_lambda_func_url.url,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}modelinferenceurlparameter",
            parameter_name=f"{self.cfnoutput_parameter}/modelinference",
            string_value=self.model_inference_lambda_func_url.url,
            type=ssm.ParameterType.STRING,
        )

        self.model_inference_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_training_dataset_lambda(self):
        """
        This function will received request to create training dataset
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")

        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(900),
            memory_size=512,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment("LOG_TABLE", self.log_table.table_name)
        lambda_function.add_environment(
            "TRAINING_DATASET_BUCKET", self.training_dataset_bucket.bucket_name
        )
        lambda_function.add_environment(
            "INCREMENTAL_TRAINING_PREFIX", self.incremental_training_prefix
        )

        # # Add function URL
        # cors_options = lambda_.FunctionUrlCorsOptions(
        #     allowed_origins=["*"],
        #     allowed_methods=[lambda_.HttpMethod.ALL],
        #     allowed_headers=["*"],
        # )
        # self.training_dataset_lambda_func_url = lambda_function.add_function_url(
        #     auth_type=lambda_.FunctionUrlAuthType.NONE, cors=cors_options
        # )

        # CfnOutput(
        #     self,
        #     f"{self.app_name}trainingdatasetfunctionurl",
        #     value=self.training_dataset_lambda_func_url.url,
        # )

        # ssm.StringParameter(
        #     self,
        #     f"{self.app_name}trainingdataseturlparameter",
        #     parameter_name=f"{self.cfnoutput_parameter}/trainingdataset",
        #     string_value=self.training_dataset_lambda_func_url.url,
        #     type=ssm.ParameterType.STRING,
        # )

        self.training_dataset_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_comprehend_endpoint_watcher_lambda(self):
        """
        This function will create comprehend endpoint watcher lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment("ENDPOINT_PARAMETER", self.endpoint_parameter)

        self.comprehend_endpoint_watcher_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_ocr_textract_lambda(self):
        """
        This function will perform ocr textract on pdf file
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.DockerImageFunction(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.DockerImageCode().from_image_asset(directory=asset_location),
            description=lambda_desc,
            timeout=Duration.seconds(900),
            memory_size=10240,
            ephemeral_storage_size=Size.mebibytes(10240),
            tracing=lambda_.Tracing.ACTIVE,
        )

        # Add Queue Trigger to Lambda
        lambda_function.add_event_source(les.SqsEventSource(self.converted_queue))

        # Add custom environment variables
        lambda_function.add_environment(
            "TEXTRACT_TABLE", self.textract_table.table_name
        )
        lambda_function.add_environment(
            "TEXTRACT_BUCKET", self.textract_bucket.bucket_name
        )
        lambda_function.add_environment("CONVERTED_PREFIX", self.converted_prefix)
        lambda_function.add_environment(
            "DATASETREADY_TOPIC_ARN", self.datasetready_sns_topic.topic_arn
        )
        lambda_function.add_environment(
            "INCREMENTAL_TRAINING_PREFIX", self.incremental_training_prefix
        )
        lambda_function.add_environment(
            "INITIAL_TRAINING_PREFIX", self.initial_training_prefix
        )

        self.ocr_textract_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_pdfconverter_inference_lambda(self):
        """
        This function will create pdf converter lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.DockerImageFunction(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.DockerImageCode().from_image_asset(directory=asset_location),
            description=lambda_desc,
            timeout=Duration.seconds(60),
            memory_size=1024,
            tracing=lambda_.Tracing.ACTIVE,
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.training_dataset_bucket)
        for extension in self.allowed_training_file_types.split(";"):
            self.training_dataset_bucket.add_object_created_notification(
                notification,
                s3.NotificationKeyFilter(
                    prefix=self.initial_training_prefix, suffix=extension
                ),
            )
            self.training_dataset_bucket.add_object_created_notification(
                notification,
                s3.NotificationKeyFilter(
                    prefix=self.adhoc_training_prefix, suffix=extension
                ),
            )
            self.training_dataset_bucket.add_object_created_notification(
                notification,
                s3.NotificationKeyFilter(
                    prefix=self.incremental_training_prefix, suffix=extension
                ),
            )

        # Add custom environment variables
        lambda_function.add_environment(
            "ALLOWED_TRAINING_FILE_TYPES", self.allowed_training_file_types
        )
        lambda_function.add_environment(
            "CONVERTED_QUEUE", self.converted_queue.queue_name
        )
        lambda_function.add_environment("CONVERTED_PREFIX", self.converted_prefix)

        self.pdfconverter_inference_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_step_functions(self):
        """
        This function will create step functions
        """

        self.sagemaker_endpoint_step_function = (
            f"{self.app_name}{self.sagemaker_endpoint_step_function}"
        )
        self.comprehend_step_function = (
            f"{self.app_name}{self.comprehend_step_function}"
        )

        # Create State Machine and trigger
        self.sagemaker_state_machine = (
            StepFunctionService.create_sagemaker_endpoint_step_function(
                self,
                self.sagemaker_endpoint_creator_lambda,
            )
        )

        self.comprehend_state_machine = (
            StepFunctionService.create_comprehend_classification_step_function(
                self,
                self.comprehend_trainer_watcher_lambda,
                self.comprehend_endpoint_watcher_lambda,
            )
        )

    @staticmethod
    def enable_ssl(bucket):
        bucket.add_to_resource_policy(
            iam.PolicyStatement(
                effect=iam.Effect.DENY,
                actions=["s3:*"],
                resources=[
                    f"{bucket.bucket_arn}/*",
                    f"{bucket.bucket_arn}",
                ],
                principals=[iam.StarPrincipal()],
                conditions={"Bool": {"aws:SecureTransport": False}},
            )
        )

    def create_buckets(self):
        """
        This function will create buckets
        """

        self.model_bucket = s3.Bucket(
            self,
            f"{self.app_name}-{self.model_bucket}-{self.account_id}-{self.region_name}",
            bucket_name=f"{self.app_name}-{self.model_bucket}-{self.account_id}-{self.region_name}"[:63],
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
        )
        self.enable_ssl(self.model_bucket)

        CfnOutput(
            self,
            f"{self.app_name}modelbucket",
            export_name=f"{self.app_name}modelbucket",
            value=self.model_bucket.bucket_name,
        )

        self.textract_bucket = s3.Bucket(
            self,
            f"{self.app_name}-{self.textract_bucket}",
            bucket_name=f"{self.app_name}-{self.textract_bucket}",
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
        )
        self.enable_ssl(self.textract_bucket)

        CfnOutput(
            self,
            f"{self.app_name}textractbucket",
            export_name=f"{self.app_name}textractbucket",
            value=self.textract_bucket.bucket_name,
        )

        self.training_dataset_bucket = s3.Bucket(
            self,
            f"{self.app_name}-{self.training_dataset_bucket}",
            bucket_name=f"{self.app_name}-{self.training_dataset_bucket}",
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
        )
        self.enable_ssl(self.training_dataset_bucket)

        CfnOutput(
            self,
            f"{self.app_name}trainingdatasetbucket",
            export_name=f"{self.app_name}trainingdatasetbucket",
            value=self.training_dataset_bucket.bucket_name,
        )

    def create_role(self):
        """
        This function will create role
        """
        # sagemaker_deploy_policy = self.get_access_policy(
        #     Path(self.policies_path).joinpath("sagemaker_deploy_policy.json")
        # )

        # all_statements = []
        # for statement in sagemaker_deploy_policy:
        #     all_statements.append(iam.PolicyStatement().from_json(statement))

        self.sagemaker_deploy_role = iam.Role(
            self,
            f"{self.app_name}SagemakerExecutionRole",
            assumed_by=iam.ServicePrincipal(service="sagemaker.amazonaws.com"),
            description="Allow for Sagemaker to deploy endpoints",
            # inline_policies=dict(
            #     inline_policy_from_cdk=iam.PolicyDocument(statements=all_statements)
            # ),
        )

    def create_dynamodb(self):
        self.log_table = dynamodb.Table(
            self,
            f"{self.app_name}-{self.log_table}",
            table_name=f"{self.app_name}-{self.log_table}",
            partition_key=dynamodb.Attribute(
                name="submissionid", type=dynamodb.AttributeType.STRING
            ),
            sort_key=dynamodb.Attribute(
                name="date",
                type=dynamodb.AttributeType.STRING,
            ),
            removal_policy=RemovalPolicy.RETAIN,
            billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,
            # stream=dynamodb.StreamViewType.NEW_IMAGE,
            point_in_time_recovery=True,
        )

        self.textract_table = dynamodb.Table(
            self,
            f"{self.app_name}-{self.textract_table}",
            table_name=f"{self.app_name}-{self.textract_table}",
            partition_key=dynamodb.Attribute(
                name="filehash", type=dynamodb.AttributeType.STRING
            ),
            removal_policy=RemovalPolicy.RETAIN,
            billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,
            point_in_time_recovery=True,
        )

    def create_athena_dynamodb_catalog(self):
        athena_spill_bucket = s3.Bucket(
            self,
            f"{self.app_name}-athena_spill_bucket",
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True,
            bucket_name=f"{self.app_name}-athena-spill-bucket-{self.account_id}-{self.region_name}"[:63].strip("-"),
        )

        catalog_name = f"{self.app_name}dynamodbcatalog"
        dynamodb_connector = sam.CfnApplication(
            self,
            f"{self.app_name}DynamodbConnectorLambda",
            location={
                "applicationId": f"arn:aws:serverlessrepo:us-east-1:292517598671:applications/AthenaDynamoDBConnector",
                "semanticVersion": "2022.22.1",
            },
            parameters={
                "AthenaCatalogName": f"{catalog_name}",
                "DisableSpillEncryption": "false",
                "LambdaMemory": "3008",
                "LambdaTimeout": "900",
                "SpillBucket": athena_spill_bucket.bucket_name,
                "SpillPrefix": "athena-spill",
            },
        )

        cfn_data_catalog = athena.CfnDataCatalog(
            self,
            f"{self.app_name}DynamodbFederatedQueryCatalog",
            name=catalog_name,
            type="LAMBDA",
            # the properties below are optional
            description="Uses federated query to query dynamodb",
            parameters={
                "function": f"arn:aws:lambda:{self.region_name}:{self.account_id}:function:{catalog_name}"
            },
        )
        cfn_data_catalog.add_depends_on(dynamodb_connector)

        self.logging.info(
            f"*** Manually provide access to invoke arn:aws:lambda:{self.region_name}:{self.account_id}:function:{catalog_name} ****"
        )
        self.logging.info(
            f"*** to default quicksight service role aws-quicksight-service-role-v0 ****"
        )
        self.logging.info(
            f"Since Athena table is queried through Quicksight, the spill bucket {athena_spill_bucket.bucket_name} access is provided to Quicksight Role"
        )

    def create_topic(self, topic_name):
        snskey = kms.Alias.from_alias_name(self, "managedsnskey", "alias/aws/sns")
        self.datasetready_sns_topic = sns.Topic(
            self,
            f"{self.app_name}{topic_name}",
            display_name=f"{self.app_name}{topic_name}",
            topic_name=f"{self.app_name}{topic_name}",
            master_key=snskey,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}{topic_name}parameter",
            parameter_name=f"{self.cfnoutput_parameter}/{topic_name}",
            string_value=self.datasetready_sns_topic.topic_arn,
            type=ssm.ParameterType.STRING,
        )

    def load_buckets(self):
        for model in self.models.split(";"):
            if model:
                s3deploy.BucketDeployment(
                    self,
                    f"initialloadfor{model}",
                    sources=[
                        s3deploy.Source.asset(
                            f"{self.asset_location}/{model}", exclude=["**/images"]
                        ),
                    ],
                    destination_bucket=self.training_dataset_bucket,
                    destination_key_prefix=f"{self.initial_training_prefix}/{model}",
                    memory_limit=2048,
                    ephemeral_storage_size=Size.mebibytes(10240),
                )

    def create_queue(
        self,
        queue_name,
        visibility_timeout,
        encryption_master_key=None,
    ):
        """
        This function will create queue and dlqs
        """
        queue_dlq = sqs.Queue(
            self,
            f"DLQ_{self.app_name}{queue_name}",
            queue_name=f"DLQ_{self.app_name}{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            retention_period=Duration.seconds(1209600),
        )
        queue = sqs.Queue(
            self,
            f"{self.app_name}{queue_name}",
            queue_name=f"{self.app_name}{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            visibility_timeout=Duration.seconds(visibility_timeout),
            dead_letter_queue=sqs.DeadLetterQueue(max_receive_count=3, queue=queue_dlq),
        )
        return queue

    @staticmethod
    def add_options_to_apigateway_method(apimethod):
        """
        This function will create OPTIONS method for API Gateway resource
        """
        options_ig = apigateway.IntegrationResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": "'Content-Type,Authorization,X-Amz-Date,X-Api-Key,X-Amz-Security-Token'",
                "method.response.header.Access-Control-Allow-Methods": "'DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT'",
                "method.response.header.Access-Control-Allow-Origin": "'*'",
            },
            # response_templates={"application/json": ""},
        )

        options_mr = apigateway.MethodResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": True,
                "method.response.header.Access-Control-Allow-Methods": True,
                "method.response.header.Access-Control-Allow-Origin": True,
            },
            # response_models={"application/json": ""},
        )

        apimethod.add_method(
            "OPTIONS",
            integration=apigateway.MockIntegration(
                request_templates={"application/json": '{ "statusCode": 200 }'},
                integration_responses=[options_ig],
            ),
            method_responses=[options_mr],
        )

        return apimethod

    @staticmethod
    def add_method_response_security_headers():
        method_response_output = apigateway.MethodResponse(
            response_models={
                "application/json": apigateway.Model.EMPTY_MODEL,
            },
            response_parameters={
                "method.response.header.Strict-Transport-Security": True,
                "method.response.header.X-Content-Type-Options": True,
                "method.response.header.X-XSS-Protection": True,
                "method.response.header.Cache-Control": True,
                "method.response.header.X-Frame-Options": True,
                "method.response.header.Content-Security-Policy": True,
            },
            status_code="200",
        )

        return method_response_output

    @staticmethod
    def add_integration_response_security_headers():
        method_response_output = apigateway.IntegrationResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Strict-Transport-Security": "'max-age=31536000; includeSubDomains;'"
                # "method.response.header.Strict-Transport-Security": "max-age=31536000; includeSubdomains; preload",
                # "method.response.header.X-Content-Type-Options": "nosniff",
                # "method.response.header.X-XSS-Protection": "1; mode=block",
                # "method.response.header.Cache-Control": "no-cache",
                # "method.response.header.X-Frame-Options": "DENY",
                # "method.response.header.Content-Security-Policy": "default-src *;",
            },
        )

        return method_response_output

    def create_api_gateway_inference(self):
        """
        This function will create API Gateway and resources
        """

        # taken from https://oozio.medium.com/serverless-discord-bot-55f95f26f743
        # requestSchema = '##  See http:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/api-gateway-mapping-template-reference.html\r\n##  This template will pass through all parameters including path, querystring, header, stage variables, and context through to the integration endpoint via the body\/payload\r\n##  \'rawBody\' allows passthrough of the (unsurprisingly) raw request body; similar to flask.request.data\r\n#set($allParams = $input.params())\r\n{\r\n"rawBody": "$util.escapeJavaScript($input.body).replace("\\\'", "\'")",\r\n"body-json" : $input.json(\'$\'),\r\n"params" : {\r\n#foreach($type in $allParams.keySet())\r\n    #set($params = $allParams.get($type))\r\n"$type" : {\r\n    #foreach($paramName in $params.keySet())\r\n    "$paramName" : "$util.escapeJavaScript($params.get($paramName))"\r\n        #if($foreach.hasNext),#end\r\n    #end\r\n}\r\n    #if($foreach.hasNext),#end\r\n#end\r\n},\r\n"stage-variables" : {\r\n#foreach($key in $stageVariables.keySet())\r\n"$key" : "$util.escapeJavaScript($stageVariables.get($key))"\r\n    #if($foreach.hasNext),#end\r\n#end\r\n},\r\n"context" : {\r\n    "account-id" : "$context.identity.accountId",\r\n    "api-id" : "$context.apiId",\r\n    "api-key" : "$context.identity.apiKey",\r\n    "authorizer-principal-id" : "$context.authorizer.principalId",\r\n    "caller" : "$context.identity.caller",\r\n    "cognito-authentication-provider" : "$context.identity.cognitoAuthenticationProvider",\r\n    "cognito-authentication-type" : "$context.identity.cognitoAuthenticationType",\r\n    "cognito-identity-id" : "$context.identity.cognitoIdentityId",\r\n    "cognito-identity-pool-id" : "$context.identity.cognitoIdentityPoolId",\r\n    "http-method" : "$context.httpMethod",\r\n    "stage" : "$context.stage",\r\n    "source-ip" : "$context.identity.sourceIp",\r\n    "user" : "$context.identity.user",\r\n    "user-agent" : "$context.identity.userAgent",\r\n    "user-arn" : "$context.identity.userArn",\r\n    "request-id" : "$context.requestId",\r\n    "resource-id" : "$context.resourceId",\r\n    "resource-path" : "$context.resourcePath"\r\n    }\r\n}'

        self.inference_api_name = f"{self.app_name}{self.inference_api_name}"

        log_group = logs.LogGroup(
            self,
            f"{self.inference_api_name}apilogs",
            log_group_name=f"{self.inference_api_name}apilogs",
        )

        inference_api = apigateway.RestApi(
            self,
            self.inference_api_name,
            rest_api_name=self.inference_api_name,
            retain_deployments=False,
            deploy=False,
            endpoint_types=[apigateway.EndpointType.REGIONAL],
            # disable_execute_api_endpoint=True,
        )

        inference_data_request_validator = inference_api.add_request_validator(
            "DefaultValidator", validate_request_parameters=True
        )

        inference_resource = inference_api.root.add_resource(
            self.inference_api_resource_name
        )
        inference_resource.add_method(
            "POST",
            apigateway.LambdaIntegration(
                self.training_dataset_lambda,
                allow_test_invoke=False,
                proxy=False,
                request_parameters={
                    "integration.request.header.X-Amz-Invocation-Type": "method.request.header.InvocationType",
                },
                # request_templates={"application/json": requestSchema},
                integration_responses=[
                    self.add_integration_response_security_headers()
                ],
            ),
            request_validator=inference_data_request_validator,
            request_parameters={
                "method.request.header.InvocationType": True,
            },
            method_responses=[self.add_method_response_security_headers()],
        )
        inference_resource = self.add_options_to_apigateway_method(inference_resource)

        deployment = apigateway.Deployment(
            self, f"{self.inference_api_name}Deployment", api=inference_api
        )

        self.stage = apigateway.Stage(
            self,
            f"{self.inference_api_name}{self.app_environment}",
            stage_name=self.app_environment,
            deployment=deployment,
            logging_level=apigateway.MethodLoggingLevel.INFO,
            data_trace_enabled=True,
            access_log_destination=apigateway.LogGroupLogDestination(log_group),
            access_log_format=apigateway.AccessLogFormat.json_with_standard_fields(
                caller=False,
                http_method=True,
                ip=True,
                protocol=True,
                request_time=True,
                resource_path=True,
                response_length=True,
                status=True,
                user=True,
            ),
        )

        # ssm.StringParameter(
        #     self,
        #     f"{self.app_name}trainingapiparameter",
        #     parameter_name=f"{self.cfnoutput_parameter}/{self.inference_api_resource_name}",
        #     string_value=self.stage.url_for_path(
        #         f"/{self.inference_api_resource_name}"
        #     ),
        #     type=ssm.ParameterType.STRING,
        # )

        domain_name = apigateway.DomainName.from_domain_name_attributes(
            self,
            self.api_custom_domain_name_parameter,
            domain_name=self.api_custom_domain_name,
            domain_name_alias_hosted_zone_id=self.api_custom_domain_name_zoneid,
            domain_name_alias_target=self.api_custom_domain_name_alias,
        )

        apigateway.BasePathMapping(
            self,
            f"{self.inference_api_name}pathmapping",
            domain_name=domain_name,
            rest_api=inference_api,
            base_path=self.inference_api_name.replace(self.app_name, ""),
            stage=self.stage,
        )

    def assign_permissions(self):
        """
        Add permissions to the resources
        """
        self.model_bucket.grant_read_write(self.comprehend_endpoint_watcher_lambda)
        self.model_bucket.grant_read_write(self.comprehend_trainer_watcher_lambda)
        self.model_bucket.grant_read_write(self.sagemaker_endpoint_creator_lambda)
        self.model_bucket.grant_read_write(self.comprehend_training_role)
        self.model_bucket.grant_read_write(self.sagemaker_deploy_role)
        self.textract_bucket.grant_read_write(self.model_inference_lambda)
        self.training_dataset_bucket.grant_read_write(self.ocr_textract_lambda)
        self.training_dataset_bucket.grant_read_write(self.training_dataset_lambda)
        self.training_dataset_bucket.grant_read_write(
            self.pdfconverter_inference_lambda
        )

        self.sagemaker_state_machine.grant_start_execution(
            self.stepfunction_launcher_lambda
        )
        self.comprehend_state_machine.grant_start_execution(
            self.stepfunction_launcher_lambda
        )

        self.log_table.grant_read_write_data(self.model_inference_lambda)
        self.log_table.grant_read_data(self.training_dataset_lambda)
        self.textract_table.grant_read_write_data(self.model_inference_lambda)
        self.textract_table.grant_read_write_data(self.ocr_textract_lambda)

        self.converted_queue.grant_send_messages(self.pdfconverter_inference_lambda)

        self.datasetready_sns_topic.grant_publish(self.ocr_textract_lambda)

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.logging = kwargs.pop("logging")
        self.config = vars(kwargs.pop("config"))
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")
        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()

        self.create_lambda_layers()

        self.create_buckets()

        self.create_dynamodb()

        self.create_role()

        self.create_topic(self.datasetready_topic)

        self.converted_queue = self.create_queue(self.converted_queue, 300)

        self.create_sagemaker_endpoint_creator_lambda()

        self.create_comprehend_trainer_watcher_lambda()

        self.create_comprehend_endpoint_watcher_lambda()

        self.create_step_functions()

        self.create_stepfunction_launcher_lambda()

        self.create_model_inference_lambda()

        self.create_training_dataset_lambda()

        self.create_pdfconverter_inference_lambda()

        self.create_ocr_textract_lambda()

        self.create_api_gateway_inference()

        self.create_athena_dynamodb_catalog()

        self.assign_permissions()

        self.load_buckets()

"""
This is the CDK Stack creation file
"""
from pathlib import Path
import json
import os
import re
import sys
import subprocess
import glob
from dotenv import dotenv_values
import boto3
from aws_cdk import (
    Stack,
    Duration,
    aws_config as config,
    aws_iam as iam,
    aws_lambda as lambda_,
    aws_ssm as ssm,
    aws_kms as kms,
    aws_sns as sns,
    aws_events as events,
    aws_events_targets as targets,
    aws_cloudwatch as cloudwatch,
    aws_cloudwatch_actions,
)
from constructs import Construct
from botocore.config import Config

PYTHON_RUNTIME = lambda_.Runtime.PYTHON_3_9
PYTHON_VERSION = "3.9"


class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value

    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        print(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()
        # try:
        #     data = data.replace(
        #         "ETL_INBOUND_BUCKET", self.etl_inbound_bucket.bucket_name
        #     )
        # except Exception:
        #     pass
        data = json.loads(data)
        return data

    # Function to create an CloudWatch Alarm for Lambda Failures
    def create_lambda_failure_cloudwatch_alarm(self, lambda_function, lambda_name):
        """
        This function will create cloudwatch alarm for all lambda functions
        """
        lambda_errors_metric = cloudwatch.Metric(
            metric_name="Errors",
            statistic="max",
            namespace="AWS/Lambda",
            dimensions_map={"FunctionName": lambda_function.function_name},
        )
        # alarm_name = re.sub("[\W_]+", "", lambda_name)
        alarm_name = lambda_name
        alarm_lambda_errors = cloudwatch.Alarm(
            self,
            metric=lambda_errors_metric,
            id=f"{alarm_name}_errors",
            alarm_name=f"{alarm_name}_errors",
            treat_missing_data=cloudwatch.TreatMissingData.NOT_BREACHING,
            evaluation_periods=1,
            threshold=1,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_OR_EQUAL_TO_THRESHOLD,
            datapoints_to_alarm=1,
        )
        alarm_lambda_errors.add_alarm_action(
            aws_cloudwatch_actions.SnsAction(self.failure_notification_topic)
        )

    def common_setup_for_lambda(
        self, lambda_function, lambda_name, policy_file, asset_location
    ):
        """
        This function is for apply common properties to Lambda
        """
        # Add permissions for lambda
        lambda_policy = self.get_access_policy(
            Path(self.policies_path).joinpath(policy_file)
        )
        for policy in lambda_policy:
            lambda_function.add_to_role_policy(iam.PolicyStatement().from_json(policy))

        # Add environment vairables
        lambda_function.add_environment("APP_ENVIRONMENT", self.app_environment)
        lambda_function.add_environment("APP_NAME_TAG", self.app_name_tag)
        lambda_function.add_environment("MODULE_NAME_TAG", self.module_name_tag)
        lambda_function.add_environment("LOG_LEVEL", self.log_level)
        for envs in dotenv_values(Path(asset_location).joinpath(".env")).items():
            key, value = envs
            lambda_function.add_environment(key, value)
        # Set Lambda Retries
        lambda_function.configure_async_invoke(retry_attempts=0)

        # Create an CloudWatch Alarm for Lambda Failures
        self.create_lambda_failure_cloudwatch_alarm(
            lambda_function, lambda_name
        )  # the reason its disabled is because you cannot point east-2 sns topic to east-1 lambda.

        return lambda_function

    def lambda_attributes(self, name):
        name = name.split("_")[1:]
        name = "_".join(name)
        return (
            getattr(self, name),
            getattr(self, f"{name}_description"),
            f"{name}_policy.json",
        )

    def env_vars_quick_load(self):
        env_config = dotenv_values()
        for k, v in env_config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        sts = boto3.client("sts")
        self.account_id = sts.get_caller_identity()["Account"]

        self.failure_notification_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.failure_notification_topic_parameter}"
        )
        self.failure_notification_topic = sns.Topic.from_topic_arn(
            self,
            id="mainalarmtopic",
            topic_arn=self.failure_notification_topic,
        )

    def create_ssm_remediation_role(self):
        """
        Give SSM permission to all config remediations

        """
        remediation_policy = self.get_access_policy(
            Path(self.policies_path).joinpath("ssm_remediation_policy.json")
        )

        all_statements = []
        for statement in remediation_policy:
            all_statements.append(iam.PolicyStatement().from_json(statement))

        self.remediation_role = iam.Role(
            self,
            "SSMCommonRemediationRole",
            assumed_by=iam.ServicePrincipal(service="ssm.amazonaws.com"),
            description="Allow SSM to all common config remediations",
            inline_policies=dict(
                inline_policy_from_cdk=iam.PolicyDocument(statements=all_statements)
            ),
        )

    def create_ssm_remediation_action(self):
        """
        Create AWS Config Managed rule and remediation action for Bucket encryption

        """
        s3_default_sse_rule = config.ManagedRule(
            self,
            "S3_Default_SSE_Always_Enabled",
            identifier=config.ManagedRuleIdentifiers.S3_BUCKET_SERVER_SIDE_ENCRYPTION_ENABLED,
            rule_scope=config.RuleScope.from_resources([config.ResourceType.S3_BUCKET]),
        )

        remediation_action = config.CfnRemediationConfiguration(
            self,
            "EnableS3SSERemediationAction",
            automatic=True,
            config_rule_name=s3_default_sse_rule.config_rule_name,
            parameters=dict(
                BucketName=dict(ResourceValue=dict(Value="RESOURCE_ID")),
                AutomationAssumeRole=dict(
                    StaticValue=dict(Values=[self.remediation_role.role_arn])
                ),
            ),
            target_id="AWS-EnableS3BucketEncryption",
            target_type="SSM_DOCUMENT",
            maximum_automatic_attempts=5,
            retry_attempt_seconds=60,
        )

        # To check the status of remediation action please use the below command
        # aws configservice describe-remediation-execution-status --config-rule-name <config-rule-name>

        # Create AWS Config Managed rule and remediation action for SSL encryption
        s3_ssl_bucket_policy_rule = config.ManagedRule(
            self,
            "S3_SSL_Bucket_Policy_Always_Enabled",
            identifier=config.ManagedRuleIdentifiers.S3_BUCKET_SSL_REQUESTS_ONLY,
            rule_scope=config.RuleScope.from_resources([config.ResourceType.S3_BUCKET]),
        )

        remediation_action = config.CfnRemediationConfiguration(
            self,
            "EnableSSLBucketPolicyRemediationAction",
            automatic=True,
            config_rule_name=s3_ssl_bucket_policy_rule.config_rule_name,
            parameters=dict(
                BucketName=dict(ResourceValue=dict(Value="RESOURCE_ID")),
                AutomationAssumeRole=dict(
                    StaticValue=dict(Values=[self.remediation_role.role_arn])
                ),
            ),
            target_id="AWSConfigRemediation-RestrictBucketSSLRequestsOnly",
            target_type="SSM_DOCUMENT",
            maximum_automatic_attempts=5,
            retry_attempt_seconds=60,
        )

        sqs_encryption_rule = config.CustomRule(
            self,
            "SQSEncryptionRule",
            configuration_changes=True,
            lambda_function=self.sqs_encryption_lambda,
            rule_scope=config.RuleScope.from_resource(config.ResourceType.SQS_QUEUE),
        )

        ssm_sqs_encryption_remediation_document = ssm.CfnDocument(
            self,
            "CustomSSMSQSSetDefaultEncryption",
            document_type="Automation",
            content=json.loads(self.sqs_encryption_remediation_script),
        )

        config.CfnRemediationConfiguration(
            self,
            "SQSSetDefaultEncryption",
            automatic=True,
            config_rule_name=sqs_encryption_rule.config_rule_name,
            parameters=dict(
                QueueURL=dict(ResourceValue=dict(Value="RESOURCE_ID")),
                AutomationAssumeRole=dict(
                    StaticValue=dict(Values=[self.remediation_role.role_arn])
                ),
            ),
            target_id=ssm_sqs_encryption_remediation_document.ref,
            target_type="SSM_DOCUMENT",
            maximum_automatic_attempts=5,
            retry_attempt_seconds=60,
        )

        cloud9_securitygroup_rule = config.CustomRule(
            self,
            "Cloud9SecurityGroupRule",
            configuration_changes=True,
            lambda_function=self.cloud9_securitygroup_lambda,
            rule_scope=config.RuleScope.from_resource(
                config.ResourceType.EC2_SECURITY_GROUP
            ),
        )

        ssm_cloud9_securitygroup_remediation_document = ssm.CfnDocument(
            self,
            "CustomSSMCloud9SecurityGroup",
            document_type="Automation",
            content=json.loads(self.cloud9_securitygroup_remediation_script),
        )

        config.CfnRemediationConfiguration(
            self,
            "Cloud9SecurityGroup",
            automatic=True,
            config_rule_name=cloud9_securitygroup_rule.config_rule_name,
            parameters=dict(
                SecurityGroupId=dict(ResourceValue=dict(Value="RESOURCE_ID")),
                AutomationAssumeRole=dict(
                    StaticValue=dict(Values=[self.remediation_role.role_arn])
                ),
            ),
            target_id=ssm_cloud9_securitygroup_remediation_document.ref,
            target_type="SSM_DOCUMENT",
            maximum_automatic_attempts=5,
            retry_attempt_seconds=60,
        )

    def create_sqs_encryption_lambda(self):
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(60),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        # lambda_function.add_environment("DESTINATION_PREFIX", self.destination_prefix)

        self.sqs_encryption_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

        # Create SSM document for remediation of SQS encryption custom rule
        asset_location = str(Path(self.code_location).joinpath(self.ssmdocuments))

        with open(
            Path(asset_location).joinpath(
                "sqs_encryption_remediation_script_handler.py"
            ),
            "r",
        ) as script_file:
            sqs_encryption_remediation_script = script_file.read()
            sqs_encryption_remediation_script = json.dumps(
                sqs_encryption_remediation_script
            )

        ssm_document = Path(self.policies_path).joinpath(
            "sqs_encyption_remediation_ssm_document.json"
        )
        with open(ssm_document, "r") as ssm_file:
            content = ssm_file.read()
        content = content.replace(
            "REMEDIATION_SCRIPT", sqs_encryption_remediation_script
        )

        self.sqs_encryption_remediation_script = content

    def create_cloud9_securitygroup_lambda(self):
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(60),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        # lambda_function.add_environment("DESTINATION_PREFIX", self.destination_prefix)

        self.cloud9_securitygroup_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

        # Create SSM document for remediation of SQS encryption custom rule
        asset_location = str(Path(self.code_location).joinpath(self.ssmdocuments))

        with open(
            Path(asset_location).joinpath(
                "cloud9_securitygroup_remediation_script_handler.py"
            ),
            "r",
        ) as script_file:
            cloud9_securitygroup_remediation_script = script_file.read()
            cloud9_securitygroup_remediation_script = json.dumps(
                cloud9_securitygroup_remediation_script
            )

        ssm_document = Path(self.policies_path).joinpath(
            "cloud9_securitygroup_remediation_ssm_document.json"
        )
        with open(ssm_document, "r") as ssm_file:
            content = ssm_file.read()
        content = content.replace(
            "REMEDIATION_SCRIPT", cloud9_securitygroup_remediation_script
        )

        self.cloud9_securitygroup_remediation_script = content

    def create_ssm_document_maintenance_window(self):
        asset_location = str(Path(self.code_location).joinpath(self.ssmdocuments))

        # Make sure on instances that are going to execute this document has s3 read access to bucket cip-uw-security-tools-us-east-2
        # For Cloud9, make sure AWSCloud9SSMAccessRole has read access to this bucket as well

        with open(
            Path(asset_location).joinpath("instance_bootstrap.sh"), "r"
        ) as script_file:
            bash_commands_to_run = script_file.read()

        run_cmds = {
            "schemaVersion": "2.2",
            "description": "Install Agents for CIP Compliance",
            "parameters": {
                "commands": {
                    "type": "String",
                    "description": "The commands to run or the path to an existing script on the instance.",
                    "default": f"{bash_commands_to_run}",
                }
            },
            "mainSteps": [
                {
                    "action": "aws:runShellScript",
                    "name": "runCommands",
                    "inputs": {
                        "timeoutSeconds": "600",
                        "runCommand": ["{{ commands }}"],
                    },
                }
            ],
        }
        self.ssm_linux_document = ssm.CfnDocument(
            self,
            "AmazonLinuxPatchDocument",
            document_type="Command",
            content=run_cmds,
        )

        instance_maintainence_window = ssm.CfnMaintenanceWindow(
            self,
            id="instancemaintainencewindow",
            name="instancemaintainencewindow",
            cutoff=0,
            schedule="cron(0 0 1 ? * * *)",
            duration=1,
            description="Linux Maintainence Window for installing ACP Compliance agents",
            allow_unassociated_targets=False,
        )

        cloud9_maintainence_target = ssm.CfnMaintenanceWindowTarget(
            self,
            id="instancemaintainencetarget",
            name="instancemaintainencetarget",
            resource_type="INSTANCE",
            targets=[
                # ssm.CfnMaintenanceWindowTarget.TargetsProperty(
                #     key="tag-key", values=["aws:cloud9:environment"]
                # ),
                ssm.CfnMaintenanceWindowTarget.TargetsProperty(
                    key="tag-key", values=[self.maintenance_tag]
                ),
            ],
            window_id=instance_maintainence_window.ref,
        )

        cloud9_maintainence_task = ssm.CfnMaintenanceWindowTask(
            self,
            id="instancemaintainencetask",
            name="instancemaintainencetask",
            max_errors="50",
            max_concurrency="50",
            priority=1,
            targets=[
                {"key": "WindowTargetIds", "values": [cloud9_maintainence_target.ref]}
            ],
            task_type="RUN_COMMAND",
            task_arn=self.ssm_linux_document.ref,
            window_id=instance_maintainence_window.ref,
            task_invocation_parameters=ssm.CfnMaintenanceWindowTask.TaskInvocationParametersProperty(
                maintenance_window_run_command_parameters=ssm.CfnMaintenanceWindowTask.MaintenanceWindowRunCommandParametersProperty(
                    cloud_watch_output_config=ssm.CfnMaintenanceWindowTask.CloudWatchOutputConfigProperty(
                        cloud_watch_output_enabled=True,
                    ),
                )
            ),
        )

        cloud_watch_output_config_property = (
            ssm.CfnMaintenanceWindowTask.CloudWatchOutputConfigProperty(
                cloud_watch_output_enabled=True
            )
        )

    def add_s3readonlyrole_cloud9ssm(self):
        cloud9ssmrole = iam.Role.from_role_arn(
            self,
            "cloud9ssmrole",
            role_arn=f"arn:aws:iam::{self.account_id}:role/service-role/AWSCloud9SSMAccessRole",
        )

        s3_read_policy = iam.Policy(
            self,
            "cdkinlinepolicy",
            statements=[
                iam.PolicyStatement(actions=["s3:Get*", "s3:List*"], resources=["*"]),
            ],
        )

        cloud9ssmrole.attach_inline_policy(s3_read_policy)

    def create_accesskey_report_lambda(self):
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(60),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add CRON Trigger to Lambda
        lambda_rule = events.Rule(
            self,
            f"{lambda_name}trigger",
            rule_name=f"{lambda_name}trigger",
            schedule=events.Schedule.expression(
                self.accesskey_report_frequency_in_minutes
            ),
        )
        lambda_rule.add_target(targets.LambdaFunction(lambda_function))

        # Add custom environment variables
        self.failure_notification_topic_generic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.failure_notification_topic_parameter}generic"
        )
        self.failure_notification_topic_generic = sns.Topic.from_topic_arn(
            self,
            id="topicgeneric",
            topic_arn=self.failure_notification_topic_generic,
        )

        lambda_function.add_environment(
            "SNS_NOTIFY_ARN", self.failure_notification_topic_generic.topic_arn
        )

        self.accesskey_report_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_instance_start_lambda(self):
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(60),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add CRON Trigger to Lambda
        lambda_rule = events.Rule(
            self,
            f"{lambda_name}trigger",
            rule_name=f"{lambda_name}trigger",
            schedule=events.Schedule.expression(
                self.instance_start_frequency_in_minutes
            ),
        )
        lambda_rule.add_target(targets.LambdaFunction(lambda_function))

        # Add custom environment variables
        lambda_function.add_environment("MAINTENANCE_TAG", self.maintenance_tag)

        self.instance_start_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_instance_stop_lambda(self):
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(60),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add CRON Trigger to Lambda
        lambda_rule = events.Rule(
            self,
            f"{lambda_name}trigger",
            rule_name=f"{lambda_name}trigger",
            schedule=events.Schedule.expression(
                self.instance_stop_frequency_in_minutes
            ),
        )
        lambda_rule.add_target(targets.LambdaFunction(lambda_function))

        # Add custom environment variables
        # lambda_function.add_environment("MAINTENANCE_TAG", self.maintenance_tag)

        self.instance_stop_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.count = kwargs.pop("count")
        self.region_name = kwargs["env"].region
        super().__init__(scope, construct_id, **kwargs)

        self.ssmdocuments = "ssmdocuments"
        self.maintenance_tag = "AutomaticMaintenance"

        self.initialize_constants()

        self.create_ssm_remediation_role()

        self.create_sqs_encryption_lambda()

        self.create_instance_start_lambda()

        self.create_instance_stop_lambda()

        self.create_cloud9_securitygroup_lambda()

        self.create_ssm_remediation_action()

        self.create_ssm_document_maintenance_window()

        print() if self.count else self.add_s3readonlyrole_cloud9ssm()
        print() if self.count else self.create_accesskey_report_lambda()

"""
This is the CDK Stack creation file
"""
import json
import os
import sys
import re
import subprocess
from pathlib import Path
import aws_cdk as core
from dotenv import dotenv_values
from aws_cdk import (
    Stack,
    Duration,
    CfnOutput,
    RemovalPolicy,
    aws_s3 as s3,
    aws_iam as iam,
    aws_kms as kms,
    aws_ses as ses,
    aws_ssm as ssm,
    aws_events as events,
    aws_ses_actions as ses_actions,
    aws_lambda as lambda_,
    aws_s3_notifications as s3_notify,
    aws_cloudwatch as cloudwatch,
    aws_events_targets as targets,
    aws_dynamodb as dynamodb,
    aws_cloudwatch_actions,
    aws_sns as sns,
    aws_sqs as sqs,
    aws_lambda_event_sources as les,
    aws_ec2 as ec2,
)
from constructs import Construct
import boto3
from botocore.config import Config

NODEJS_RUNTIME = lambda_.Runtime.NODEJS_14_X
PYTHON_RUNTIME = lambda_.Runtime.PYTHON_3_9
PYTHON_VERSION = "3.9"


class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value

    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        self.logging.info(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()

        data = data.replace("REGION_NAME", self.region_name)
        data = json.loads(data)
        return data

    # Function to create an CloudWatch Alarm for Lambda Failures
    def create_lambda_failure_cloudwatch_alarm(self, lambda_function, lambda_name):
        """
        This function will create cloudwatch alarm for all lambda functions
        """
        lambda_errors_metric = cloudwatch.Metric(
            metric_name="Errors",
            statistic="max",
            namespace="AWS/Lambda",
            dimensions_map={"FunctionName": lambda_function.function_name},
        )
        # alarm_name = re.sub("[\W_]+", "", lambda_name)
        alarm_name = lambda_name
        alarm_lambda_errors = cloudwatch.Alarm(
            self,
            metric=lambda_errors_metric,
            id=f"{alarm_name}_errors",
            alarm_name=f"{alarm_name}_errors",
            treat_missing_data=cloudwatch.TreatMissingData.NOT_BREACHING,
            evaluation_periods=1,
            threshold=1,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_OR_EQUAL_TO_THRESHOLD,
            datapoints_to_alarm=1,
        )
        alarm_lambda_errors.add_alarm_action(
            aws_cloudwatch_actions.SnsAction(self.failure_notification_topic)
        )

    def common_setup_for_lambda(
        self, lambda_function, lambda_name, policy_file, asset_location
    ):
        """
        This function is for apply common properties to Lambda
        """
        # Add permissions for lambda
        lambda_policy = self.get_access_policy(
            Path(self.policies_path).joinpath(policy_file)
        )
        for policy in lambda_policy:
            lambda_function.add_to_role_policy(iam.PolicyStatement().from_json(policy))

        # Add environment vairables
        lambda_function.add_environment("APP_ENVIRONMENT", self.app_environment)
        lambda_function.add_environment("APP_NAME_TAG", self.app_name_tag)
        lambda_function.add_environment("MODULE_NAME_TAG", self.module_name_tag)
        lambda_function.add_environment("LOG_LEVEL", self.log_level)
        for envs in dotenv_values(Path(asset_location).joinpath(".env")).items():
            key, value = envs
            lambda_function.add_environment(key, value)

        # Set Lambda Retries
        lambda_function.configure_async_invoke(retry_attempts=0)

        # Create an CloudWatch Alarm for Lambda Failures
        self.create_lambda_failure_cloudwatch_alarm(lambda_function, lambda_name)

        return lambda_function

    def lambda_attributes(self, name):
        name = name.split("_")[1:]
        name = "_".join(name)
        return (
            f"{self.app_name}{getattr(self, name)}",
            getattr(self, f"{name}_description"),
            f"{name}_policy.json",
            getattr(self, name),
        )

    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.workmail_mailboxes_parameter = self.workmail_mailboxes_parameter.replace(
            "#", self.app_name
        )
        self.cfnoutput_parameter = self.cfnoutput_parameter.replace("#", self.app_name)
        self.database_connection_parameter = (
            f"{self.cfnoutput_parameter}{self.database_connection_parameter}"
        )
        self.redis_connection_parameter = (
            f"{self.cfnoutput_parameter}{self.redis_connection_parameter}"
        )

        self.no_reply_emailaddress_parameter = (
            self.no_reply_emailaddress_parameter.replace("#", self.app_name)
        )

        self.no_reply_emailaddress = self.from_ssm(self.no_reply_emailaddress_parameter)

        self.failure_notification_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.failure_notification_topic_parameter}"
        )
        self.failure_notification_topic = sns.Topic.from_topic_arn(
            self,
            id="mainalarmtopic",
            topic_arn=self.failure_notification_topic,
        )

        self.compute_security_group = ec2.SecurityGroup.from_security_group_id(
            self,
            "LambdaSecurityGroup",
            security_group_id=self.from_ssm(
                f"{self.cfnoutput_parameter}/{self.compute_security_group_parameter}"
            ),
        )

        vpc_id = ssm.StringParameter.value_from_lookup(
            self, parameter_name=f"{self.cfnoutput_parameter}/{self.vpc_parameter}"
        )
        self.vpc = ec2.Vpc.from_lookup(self, "VPC", vpc_id=vpc_id)

        self.workmail_mailboxes = ssm.StringParameter.value_from_lookup(
            self,
            parameter_name=f"{self.workmail_mailboxes_parameter}",
        )
        self.workmail_mailboxes = self.workmail_mailboxes.split(";")

    @staticmethod
    def enable_ssl(bucket):
        bucket.add_to_resource_policy(
            iam.PolicyStatement(
                effect=iam.Effect.DENY,
                actions=["s3:*"],
                resources=[
                    f"{bucket.bucket_arn}/*",
                    f"{bucket.bucket_arn}",
                ],
                principals=[iam.StarPrincipal()],
                conditions={"Bool": {"aws:SecureTransport": False}},
            )
        )

    def create_buckets(self):
        """
        This function will create S3 buckets
        """
        expiry_days = 2555

        self.inbound_bucket = s3.Bucket(
            self,
            f"{self.app_name}-{self.bucket_for_email_inbound}",
            bucket_name=f"{self.app_name}-{self.bucket_for_email_inbound}",
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
            lifecycle_rules=[
                s3.LifecycleRule(
                    id=f"ExpireAfter{str(expiry_days)}Days",
                    expiration=Duration.days(expiry_days),
                ),
            ],
        )
        self.enable_ssl(self.inbound_bucket)

        self.exported_inbound_bucket = self.inbound_bucket

        self.extract_bucket = s3.Bucket(
            self,
            f"{self.app_name}-{self.bucket_for_email_extract}",
            bucket_name=f"{self.app_name}-{self.bucket_for_email_extract}",
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
            lifecycle_rules=[
                s3.LifecycleRule(
                    id=f"ExpireAfter{str(expiry_days)}Days",
                    expiration=Duration.days(expiry_days),
                ),
            ],
        )
        self.enable_ssl(self.extract_bucket)

        CfnOutput(
            self,
            "emailextractbucket",
            value=self.extract_bucket.bucket_name,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}emailextractbucketparameter",
            parameter_name=f"{self.cfnoutput_parameter}/emailextractbucket",
            string_value=self.extract_bucket.bucket_name,
            type=ssm.ParameterType.STRING,
        )

    def assign_lambda_layers(self, layers=["All"]):
        if layers[0] == "All":
            lambda_layers = [layer_info[1] for layer_info in self.lambda_layers]
            return lambda_layers

        lambda_layers = []
        for layer_name in layers:
            for layer_info in self.lambda_layers:
                if layer_name in layer_info[0]:
                    lambda_layers.append(layer_info[1])
        return lambda_layers

    def create_lambda_layers(self):
        """
        This function will create lambda layers
        """
        self.lambda_layers = []

        for layer in list(filter(None, self.lambda_layers_build.split(";"))):
            asset_location = str(Path(self.code_location).joinpath(layer))
            self.logging.info(f"Code Location for {layer}: {asset_location}")
            if not self.is_velocity:
                cmds = f"pip3 install --platform manylinux2014_x86_64 --implementation cp --python {PYTHON_VERSION} --only-binary=:all: -r {asset_location}/python/requirements.txt -t {asset_location}/python"
                subprocess.check_call([cmds], shell=True)
            lambda_layers_build = lambda_.LayerVersion(
                self,
                f"{self.app_name}{layer}",
                layer_version_name=f"{self.app_name}{layer}",
                code=lambda_.Code.from_asset(asset_location),
                compatible_runtimes=[PYTHON_RUNTIME],
                compatible_architectures=[lambda_.Architecture.X86_64],
                description=f"{self.app_name_tag}-{self.module_name_tag}-{layer}",
            )
            self.lambda_layers.append((layer, lambda_layers_build))

        for layer in list(filter(None, self.lambda_layers_existing.split(";"))):
            layer_arn = self.from_ssm(f"{self.cfnoutput_parameter}/{layer}")
            lambda_layers_existing = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer_arn
            )
            self.lambda_layers.append((layer, lambda_layers_existing))

        for layer in list(filter(None, self.lambda_layers_arn.split(";"))):
            # self.lambda_client.get_layer_version_by_arn(Arn=layer)
            lambda_layers_arn = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer
            )
            self.lambda_layers.append((layer, lambda_layers_arn))

    def create_email_scanner_lambda(self):
        """
        This function will create email scanner lambda
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["emailreceiptutilities", "dbutilities"]),
            timeout=Duration.seconds(300),
            memory_size=256,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
            vpc=self.vpc,
            security_groups=[self.compute_security_group],
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.inbound_bucket)
        self.inbound_bucket.add_object_created_notification(
            notification, s3.NotificationKeyFilter(prefix=self.incoming_prefix)
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "BUCKET_FOR_EMAIL_EXTRACT", self.extract_bucket.bucket_name
        )
        lambda_function.add_environment(
            "ALLOWED_ATTACHMENT_EXTENSIONS",
            self.allowed_attachment_extensions,
        )
        lambda_function.add_environment("METADATA_JSON", self.metadata_json)
        lambda_function.add_environment(
            "NOTIFY_INTAKE_QUEUE", self.notify_intake_queue.queue_name
        )
        lambda_function.add_environment(
            "EXTRACTOR_INTAKE_QUEUE", self.extractor_intake_queue.queue_name
        )
        lambda_function.add_environment(
            "EMAIL_TEMPLATE_BAD_ACK",
            f"{self.email_template_bad_ack}_{self.app_name}",
        )
        lambda_function.add_environment("SERVICETYPE_MODEL", self.servicetype_model)
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "MODEL_INFERENCE_PARAMETER",
            f"{self.cfnoutput_parameter}/{self.model_inference_parameter}",
        )

        self.email_scanner_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_email_extractor_lambda(self):
        """
        This function will create email extractor lambda
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["emailreceiptutilities"]),
            timeout=Duration.seconds(600),
            memory_size=512,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add Queue Trigger to Lambda
        lambda_function.add_event_source(
            les.SqsEventSource(self.extractor_intake_queue)
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "BUCKET_FOR_EMAIL_EXTRACT", self.extract_bucket.bucket_name
        )
        lambda_function.add_environment(
            "ALLOWED_ATTACHMENT_EXTENSIONS",
            self.allowed_attachment_extensions,
        )
        lambda_function.add_environment("METADATA_JSON", self.metadata_json)
        lambda_function.add_environment(
            "NOTIFY_INTAKE_QUEUE", self.notify_intake_queue.queue_name
        )
        lambda_function.add_environment(
            "EMAIL_TEMPLATE_BAD_ACK",
            f"{self.app_name}{self.email_template_bad_ack}",
        )
        lambda_function.add_environment(
            "EMAIL_TEMPLATE_NOATTACH_ACK",
            f"{self.app_name}{self.email_template_noattach_ack}",
        )
        lambda_function.add_environment(
            "MODEL_INFERENCE_PARAMETER",
            f"{self.cfnoutput_parameter}/{self.model_inference_parameter}",
        )
        lambda_function.add_environment(
            "PDFCONVERTER_LAMBDA", self.pdfconverter_lambda.function_arn
        )
        lambda_function.add_environment(
            "ATTACHMENTTYPE_MODEL", self.attachmenttype_model
        )

        self.email_extractor_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_mailbox_monitor_lambda(self):
        """
        This function will create mailbox monitor lambda
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["emailreceiptutilities", "dbutilities"]),
            timeout=Duration.seconds(300),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
            vpc=self.vpc,
            security_groups=[self.compute_security_group],
        )

        # Add CRON Trigger to Lambda
        lambda_rule = events.Rule(
            self,
            f"{lambda_name}trigger",
            rule_name=f"{lambda_name}trigger",
            schedule=events.Schedule.cron(
                minute=self.mailbox_monitor_frequency_in_minutes
            ),
        )
        lambda_rule.add_target(targets.LambdaFunction(lambda_function))

        # Add custom environment variables
        lambda_function.add_environment(
            "MAILBOX_INTAKE_QUEUE", self.mailbox_intake_queue.queue_name
        )
        lambda_function.add_environment(
            "REDIS_CONNECTION_PARAMETER", self.redis_connection_parameter
        )
        lambda_function.add_environment("MAILBOX_TABLE", self.mailbox_table.table_name)
        lambda_function.add_environment("REDIS_TTL", self.redis_ttl)

        self.mailbox_monitor_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_mailbox_sweeper_lambda(self):
        """
        This function will create mailbox sweeper lambda
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["emailreceiptutilities", "dbutilities"]),
            timeout=Duration.seconds(120),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
            vpc=self.vpc,
            security_groups=[self.compute_security_group],
        )

        # Add Queue Trigger to Lambda
        lambda_function.add_event_source(les.SqsEventSource(self.mailbox_intake_queue))

        # Add custom environment variables
        lambda_function.add_environment(
            "MAILBOX_INTAKE_QUEUE", self.mailbox_intake_queue.queue_name
        )
        lambda_function.add_environment(
            "REDIS_CONNECTION_PARAMETER", self.redis_connection_parameter
        )
        lambda_function.add_environment(
            "BUCKET_FOR_EMAIL_INBOUND", self.inbound_bucket.bucket_name
        )
        lambda_function.add_environment(
            "INCOMING_PREFIX_FOR_GRAPHAPI",
            f"{self.incoming_prefix}/{self.graphapi_prefix}",
        )
        lambda_function.add_environment("MAILBOX_TABLE", self.mailbox_table.table_name)
        lambda_function.add_environment("REDIS_TTL", self.redis_ttl)

        self.mailbox_sweeper_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_email_dataload_lambda(self):
        """
        This function will create ingestion launcher lambda
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["emailreceiptutilities", "dbutilities"]),
            timeout=Duration.seconds(600),
            memory_size=512,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
            vpc=self.vpc,
            security_groups=[self.compute_security_group],
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.extract_bucket)
        self.extract_bucket.add_object_created_notification(
            notification, s3.NotificationKeyFilter(suffix=self.trigger_suffix)
        )

        # Add custom environment variables
        lambda_function.add_environment("METADATA_JSON", self.metadata_json)
        lambda_function.add_environment(
            "NOTIFY_INTAKE_QUEUE", self.notify_intake_queue.queue_name
        )
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "SUBMISSION_INTAKE_QUEUE", self.submission_intake_queue.queue_name
        )
        lambda_function.add_environment(
            "EMAIL_TEMPLATE_GOOD_ACK",
            f"{self.app_name}{self.email_template_good_ack}",
        )
        lambda_function.add_environment("MAILBOX_TABLE", self.mailbox_table.table_name)
        lambda_function.add_environment(
            "EMAIL_PROCESSED_TOPIC_ARN", self.email_processed_sns_topic.topic_arn
        )

        self.email_dataload_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_email_notification_lambda(self):
        """
        This function will create email notification lambda
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        cmds = f"npm install --prefix {asset_location}"
        subprocess.check_call([cmds], shell=True)
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.handler",
            timeout=Duration.seconds(120),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=NODEJS_RUNTIME,
        )

        # Add Queue Trigger to Lambda
        lambda_function.add_event_source(les.SqsEventSource(self.notify_intake_queue))

        # Add custom environment variables
        lambda_function.add_environment(
            "NO_REPLY_EMAILADDRESS", self.no_reply_emailaddress
        )
        lambda_function.add_environment("NODE_ENV", self.app_environment)
        lambda_function.add_environment("REGION_NAME", self.region_name_email)

        self.email_notification_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_pdfconverter_lambda(self):
        """
        This function will create pdf converter lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["pdfconverter", "libreoffice-brotli"]),
            timeout=Duration.seconds(60),
            memory_size=1024,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        # lambda_function.add_environment("ENDPOINT_PARAMETER", self.endpoint_parameter)

        self.pdfconverter_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_queue(
        self,
        queue_name,
        visibility_timeout,
        encryption_master_key=None,
    ):
        """
        This function will create queue and dlqs
        """
        queue_dlq = sqs.Queue(
            self,
            f"DLQ_{self.app_name}{queue_name}",
            queue_name=f"DLQ_{self.app_name}{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            retention_period=Duration.seconds(1209600),
        )
        queue = sqs.Queue(
            self,
            f"{self.app_name}{queue_name}",
            queue_name=f"{self.app_name}{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            visibility_timeout=Duration.seconds(visibility_timeout),
            dead_letter_queue=sqs.DeadLetterQueue(max_receive_count=3, queue=queue_dlq),
        )
        return queue

    def create_topic(self, topic_name):
        snskey = kms.Alias.from_alias_name(self, "managedsnskey", "alias/aws/sns")
        self.email_processed_sns_topic = sns.Topic(
            self,
            f"{self.app_name}{topic_name}",
            display_name=f"{self.app_name}{topic_name}",
            topic_name=f"{self.app_name}{topic_name}",
            master_key=snskey,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}{topic_name}parameter",
            parameter_name=f"{self.cfnoutput_parameter}/{topic_name}",
            string_value=self.email_processed_sns_topic.topic_arn,
            type=ssm.ParameterType.STRING,
        )

    def create_dynamodb(self):
        self.mailbox_table = dynamodb.Table(
            self,
            f"{self.app_name}-{self.mailbox_table}",
            table_name=f"{self.app_name}-{self.mailbox_table}",
            partition_key=dynamodb.Attribute(
                name="name", type=dynamodb.AttributeType.STRING
            ),
            removal_policy=RemovalPolicy.RETAIN,
            billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,
            point_in_time_recovery=True,
        )

    def assign_permissions(self):
        """
        Add permissions to the resources
        """
        self.inbound_bucket.grant_read_write(self.email_extractor_lambda)
        self.inbound_bucket.grant_read_write(self.email_scanner_lambda)
        self.inbound_bucket.grant_read_write(self.mailbox_sweeper_lambda)

        self.extract_bucket.grant_put(self.email_extractor_lambda)
        self.extract_bucket.grant_read_write(self.email_dataload_lambda)
        self.extract_bucket.grant_read_write(self.pdfconverter_lambda)

        self.mailbox_intake_queue.grant_send_messages(self.mailbox_monitor_lambda)
        self.mailbox_intake_queue.grant_consume_messages(self.mailbox_sweeper_lambda)

        self.extractor_intake_queue.grant_send_messages(self.email_scanner_lambda)
        self.extractor_intake_queue.grant_consume_messages(self.email_extractor_lambda)

        self.submission_intake_queue.grant_send_messages(self.email_dataload_lambda)

        self.notify_intake_queue.grant_send_messages(self.email_scanner_lambda)
        self.notify_intake_queue.grant_send_messages(self.email_extractor_lambda)
        self.notify_intake_queue.grant_send_messages(self.email_dataload_lambda)

        self.email_processed_sns_topic.grant_publish(self.email_dataload_lambda)

        self.mailbox_table.grant_read_data(self.email_dataload_lambda)
        self.mailbox_table.grant_read_data(self.mailbox_monitor_lambda)

        self.pdfconverter_lambda.grant_invoke(self.email_extractor_lambda)

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.config = vars(kwargs.pop("config"))
        self.logging = kwargs.pop("logging")
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")

        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()

        self.create_lambda_layers()

        self.create_buckets()

        self.create_dynamodb()

        self.extractor_intake_queue = self.create_queue(
            self.extractor_intake_queue, 600
        )
        self.mailbox_intake_queue = self.create_queue(self.mailbox_intake_queue, 120)

        self.notify_intake_queue = self.create_queue(self.notify_intake_queue, 120)

        self.submission_intake_queue = self.create_queue(
            self.submission_intake_queue, 600
        )

        self.create_topic(self.email_processed_topic)

        self.create_email_scanner_lambda()

        self.create_pdfconverter_lambda()

        self.create_email_extractor_lambda()

        self.create_email_dataload_lambda()

        self.create_email_notification_lambda()

        self.create_mailbox_monitor_lambda()

        self.create_mailbox_sweeper_lambda()

        self.assign_permissions()


# New stack to deploy in another region since workmail and ses can only work in us-east-1
class CDKAppStackEmail(Stack):
    """
    This stack will create SES resources in us-east-1 region or region where workmail is available
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value

    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.rule_set_name = "INBOUND_MAIL"

    def create_ses_templates(self):
        """
        This function will create ses templates
        """
        ses.CfnTemplate(
            self,
            f"{self.app_name}{self.email_template_good_ack}",
            template=ses.CfnTemplate.TemplateProperty(
                subject_part="{{application_id_tag}}{{subject}}",
                template_name=f"{self.app_name}{self.email_template_good_ack}",
                text_part="Email with subject [{{subject}}] received at {{submission_date}} {{submission_time}} UTC\n\nWe hereby acknowledge the receipt of the below documents from you.\n{{email_body_text}}\n{{invalid_attachments_text}}\nWe will keep you notified on the progress. If you wish to send additional documents or modify existing documents, please forward to this email, attach documents and send it to {{mailbox}}\n\nThank you\n{{application_id_tag}}\n{{outlook_id}}",
                html_part='<div style="display: none;">{{application_id_tag}}</div><p><div>Email with subject <b>{{subject}}</b> received at <b>{{submission_date}} {{submission_time}} UTC</b></div><br><div>We hereby acknowledge the receipt of the below documents from you<br>{{email_body_html}}</div><br><div>{{invalid_attachments_html}}</div><br><div>We will keep you notified on the progress. If you wish to send additional documents or modify existing documents, please forward to this email, attach documents and send it to <a href="mailto:{{mailbox}}">{{mailbox}}</a></div><br>Thank you<br>{{application_id_tag}}</p><div style="display: none;">{{outlook_id}}</div>',
            ),
        )

        ses.CfnTemplate(
            self,
            f"{self.app_name}{self.email_template_bad_ack}",
            template=ses.CfnTemplate.TemplateProperty(
                subject_part="Your document submission status",
                template_name=f"{self.app_name}{self.email_template_bad_ack}",
                text_part="Email with subject [{{subject}}] received at {{submission_date}} {{submission_time}} UTC\n{{invalid_attachments_text}}\nPlease consider resending with valid documents to {{mailbox}}\n\nThank you",
                html_part='<p><div>Email with subject <b>{{subject}}</b> received at <b>{{submission_date}} {{submission_time}} UTC</b></div><br><div>{{invalid_attachments_html}}</div><br><div>Please consider resending with valid documents to <a href="mailto:{{mailbox}}">{{mailbox}}</a></div><br>Thank you</p>',
            ),
        )

        ses.CfnTemplate(
            self,
            f"{self.app_name}{self.email_template_noattach_ack}",
            template=ses.CfnTemplate.TemplateProperty(
                subject_part="You missed the documents",
                template_name=f"{self.app_name}{self.email_template_noattach_ack}",
                text_part="Email with subject [{{subject}}] received at {{submission_date}} {{submission_time}} UTC\n\nWe have not received any documents in your email. Please consider resending with documents to {{mailbox}}.\n\nThank you",
                html_part='<p><div>Email with subject <b>{{subject}}</b> received at <b>{{submission_date}} {{submission_time}} UTC</b></div><br><div>We have not received any documents in your email. Please consider resending with documents to <a href="mailto:{{mailbox}}">{{mailbox}}</a></div><br>Thank you</p>',
            ),
        )

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        inbound_bucket = kwargs.pop("inbound_bucket")
        workmail_mailboxes = kwargs.pop("mailboxes")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.config = vars(kwargs.pop("config"))
        self.logging = kwargs.pop("logging")
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")

        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()

        # create one ses rule for email inbound bucket for every mailbox
        rule_set = ses.ReceiptRuleSet.from_receipt_rule_set_name(
            self, self.rule_set_name, receipt_rule_set_name=self.rule_set_name
        )
        for count, email in enumerate(workmail_mailboxes):
            email_prefix = email.replace("@", "_").replace(".awsapps.com", "")
            rule_set.add_rule(
                f"{self.app_name}FwdToS3_{email_prefix}",
                receipt_rule_name=f"{self.app_name}FwdToS3_{email_prefix}",
                # receipt_rule_name=f"{self.app_name}FwdToS3_{str(count+1)}",
                enabled=True,
                # scan_enabled=True,
                recipients=[
                    email,
                ],
                actions=[
                    ses_actions.S3(
                        bucket=inbound_bucket,
                        object_key_prefix=f"{self.incoming_prefix}/{self.ses_prefix}/{email}",
                    )
                ],
            )
        if workmail_mailboxes:
            self.create_ses_templates()

        self.logging.info(
            "*** Execute workmail.py script again to load workmail addresses into dynamodb ***"
        )

"""
This is the CDK Stack creation file
"""
from pathlib import Path
import json
import os
import re
import sys
import subprocess
import glob
import boto3
from dotenv import dotenv_values
from aws_cdk import (
    Stack,
    Duration,
    CfnOutput,
    RemovalPolicy,
    aws_s3 as s3,
    aws_s3_deployment as s3deploy,
    aws_glue as glue,
    aws_kms as kms,
    aws_sqs as sqs,
    aws_ssm as ssm,
    aws_dynamodb as dynamodb,
    aws_sns_subscriptions as subscriptions,
    aws_lambda_destinations as destinations,
    aws_logs as logs,
    aws_iam as iam,
    aws_lambda as lambda_,
    aws_apigateway as apigateway,
    aws_s3_notifications as s3_notify,
    aws_cloudwatch as cloudwatch,
    aws_cloudwatch_actions,
    aws_lambda_event_sources as les,
    aws_sns as sns,
    aws_ec2 as ec2,
)
from constructs import Construct
from botocore.config import Config

PYTHON_RUNTIME = lambda_.Runtime.PYTHON_3_9
PYTHON_VERSION = "3.9"


class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value

    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        self.logging.info(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()
        data = json.loads(data)
        return data

    # Function to create an CloudWatch Alarm for Lambda Failures
    def create_lambda_failure_cloudwatch_alarm(self, lambda_function, lambda_name):
        """
        This function will create cloudwatch alarm for all lambda functions
        """
        lambda_errors_metric = cloudwatch.Metric(
            metric_name="Errors",
            statistic="max",
            namespace="AWS/Lambda",
            dimensions_map={"FunctionName": lambda_function.function_name},
        )
        # alarm_name = re.sub("[\W_]+", "", lambda_name)
        alarm_name = lambda_name
        alarm_lambda_errors = cloudwatch.Alarm(
            self,
            metric=lambda_errors_metric,
            id=f"{alarm_name}_errors",
            alarm_name=f"{alarm_name}_errors",
            treat_missing_data=cloudwatch.TreatMissingData.NOT_BREACHING,
            evaluation_periods=1,
            threshold=1,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_OR_EQUAL_TO_THRESHOLD,
            datapoints_to_alarm=1,
        )
        alarm_lambda_errors.add_alarm_action(
            aws_cloudwatch_actions.SnsAction(self.failure_notification_topic)
        )

    def common_setup_for_lambda(
        self,
        lambda_function,
        lambda_name,
        policy_file,
        asset_location,
        retry_attempts=0,
    ):
        """
        This function is for apply common properties to Lambda
        """
        # Add permissions for lambda
        lambda_policy = self.get_access_policy(
            Path(self.policies_path).joinpath(policy_file)
        )
        for policy in lambda_policy:
            lambda_function.add_to_role_policy(iam.PolicyStatement().from_json(policy))

        # Add environment vairables
        lambda_function.add_environment("APP_ENVIRONMENT", self.app_environment)
        lambda_function.add_environment("APP_NAME_TAG", self.app_name_tag)
        lambda_function.add_environment("MODULE_NAME_TAG", self.module_name_tag)
        lambda_function.add_environment("LOG_LEVEL", self.log_level)
        for envs in dotenv_values(Path(asset_location).joinpath(".env")).items():
            key, value = envs
            lambda_function.add_environment(key, value)

        # Set Lambda Retries
        lambda_function.configure_async_invoke(retry_attempts=retry_attempts)

        # Create an CloudWatch Alarm for Lambda Failures
        self.create_lambda_failure_cloudwatch_alarm(lambda_function, lambda_name)

        return lambda_function

    def lambda_attributes(self, name):
        name = name.split("_")[1:]
        name = "_".join(name)
        return (
            f"{self.app_name}{getattr(self, name)}",
            getattr(self, f"{name}_description"),
            f"{name}_policy.json",
            getattr(self, name),
        )

    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.mea_api_credential_parameter = self.mea_api_credential_parameter.replace(
            "#", self.app_name
        )
        self.cfnoutput_parameter = self.cfnoutput_parameter.replace("#", self.app_name)
        self.database_connection_parameter = (
            f"{self.cfnoutput_parameter}{self.database_connection_parameter}"
        )
        self.redis_connection_parameter = (
            f"{self.cfnoutput_parameter}{self.redis_connection_parameter}"
        )

        self.mea_webhook_usage_api_key_parameter = (
            self.mea_webhook_usage_api_key_parameter.replace("#", self.app_name)
        )

        self.failure_notification_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.failure_notification_topic_parameter}"
        )
        self.failure_notification_topic = sns.Topic.from_topic_arn(
            self,
            id="mainalarmtopic",
            topic_arn=self.failure_notification_topic,
        )

        self.api_custom_domain_name = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/name",
        )
        self.api_custom_domain_name_alias = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/alias",
        )
        self.api_custom_domain_name_zoneid = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/zoneid",
        )

        self.database_connection_parameter = self.from_ssm(
            self.database_connection_parameter
        )

        self.mea_webhook_usage_api_key = self.from_ssm(
            self.mea_webhook_usage_api_key_parameter
        )

        self.email_extract_bucket = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.email_extract_bucket_parameter}"
        )
        self.email_extract_bucket = s3.Bucket.from_bucket_name(
            self,
            f"{self.app_name}{self.email_extract_bucket_parameter}",
            bucket_name=self.email_extract_bucket,
        )

        self.email_processed_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.email_processed_topic_parameter}"
        )
        self.email_processed_topic = sns.Topic.from_topic_arn(
            self,
            f"{self.app_name}{self.email_processed_topic_parameter}",
            topic_arn=self.email_processed_topic,
        )

        self.appetite_landing_bucket = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.appetite_landing_bucket_parameter}"
        )
        self.appetite_landing_bucket = s3.Bucket.from_bucket_arn(
            self,
            f"{self.app_name}{self.appetite_landing_bucket_parameter}",
            f"arn:aws:s3:::{self.appetite_landing_bucket}",
        )

        self.docingestion_bucket = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.docingestion_bucket_parameter}"
        )
        self.docingestion_bucket = s3.Bucket.from_bucket_arn(
            self,
            f"{self.app_name}{self.docingestion_bucket_parameter}",
            f"arn:aws:s3:::{self.docingestion_bucket}",
        )

        self.compute_security_group = ec2.SecurityGroup.from_security_group_id(
            self,
            f"{self.app_name}{self.compute_security_group_parameter}",
            security_group_id=self.from_ssm(
                f"{self.cfnoutput_parameter}/{self.compute_security_group_parameter}"
            ),
        )

        vpc_id = ssm.StringParameter.value_from_lookup(
            self, parameter_name=f"{self.cfnoutput_parameter}/{self.vpc_parameter}"
        )
        self.vpc = ec2.Vpc.from_lookup(self, "VPC", vpc_id=vpc_id)

    def lambda_vpc_setup(self):
        """
        This function is for getting VPC info
        """
        self.vpc = ec2.Vpc.from_lookup(self, "VPC", vpc_name=self.database_vpc)

        self.compute_security_group = ec2.SecurityGroup.from_lookup_by_name(
            self,
            "LambdaSecurityGroup",
            security_group_name=self.compute_security_group,
            vpc=self.vpc,
        )

    def assign_lambda_layers(self, layers=["All"]):
        if layers[0] == "All":
            lambda_layers = [layer_info[1] for layer_info in self.lambda_layers]
            return lambda_layers

        lambda_layers = []
        for layer_name in layers:
            for layer_info in self.lambda_layers:
                if layer_name in layer_info[0]:
                    lambda_layers.append(layer_info[1])
        return lambda_layers

    def create_lambda_layers(self):
        """
        This function will create lambda layers
        """
        self.lambda_layers = []

        for layer in list(filter(None, self.lambda_layers_build.split(";"))):
            asset_location = str(Path(self.code_location).joinpath(layer))
            self.logging.info(f"Code Location for {layer}: {asset_location}")
            if not self.is_velocity:
                cmds = f"pip3 install --platform manylinux2014_x86_64 --implementation cp --python {PYTHON_VERSION} --only-binary=:all: -r {asset_location}/python/requirements.txt -t {asset_location}/python"
                subprocess.check_call([cmds], shell=True)
            lambda_layers_build = lambda_.LayerVersion(
                self,
                f"{self.app_name}{layer}",
                layer_version_name=f"{self.app_name}{layer}",
                code=lambda_.Code.from_asset(asset_location),
                compatible_runtimes=[PYTHON_RUNTIME],
                compatible_architectures=[lambda_.Architecture.X86_64],
                description=f"{self.app_name_tag}-{self.module_name_tag}-{layer}",
            )
            self.lambda_layers.append((layer, lambda_layers_build))

        for layer in list(filter(None, self.lambda_layers_existing.split(";"))):
            layer_arn = self.from_ssm(f"{self.cfnoutput_parameter}/{layer}")
            lambda_layers_existing = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer_arn
            )
            self.lambda_layers.append((layer, lambda_layers_existing))

        for layer in list(filter(None, self.lambda_layers_arn.split(";"))):
            # self.lambda_client.get_layer_version_by_arn(Arn=layer)
            lambda_layers_arn = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer
            )
            self.lambda_layers.append((layer, lambda_layers_arn))

    def create_mea_webhook_lambda(self):
        """
        This function will create trigger the glu jobs and check for its completion
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(180),
            memory_size=128,
            layers=self.assign_lambda_layers(),
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        lambda_function.add_event_source(
            les.SqsEventSource(self.cipmea_webhook_intake_queue)
        )

        # Add custom environment variables
        lambda_function.add_environment("FETCH_URL", self.mea_fetch_lambda_func_url.url)
        lambda_function.add_environment("APP_NAME", self.app_name)

        self.mea_webhook_lambda = self.common_setup_for_lambda(
            lambda_function,
            lambda_name,
            lambda_policy_file,
            asset_location,
            # destination=destinations.SqsDestination(self.webhook_errors_queue),
        )

    def create_mea_fetch_lambda(self):
        """
        This function will create trigger the glu jobs and check for its completion
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(180),
            memory_size=128,
            layers=self.assign_lambda_layers(),
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
            vpc=self.vpc,
            security_groups=[self.compute_security_group],
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "MEA_API_CREDENTIAL_PARAMETER", self.mea_api_credential_parameter
        )
        lambda_function.add_environment("MEA_BASE_URL", self.mea_base_url)
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )
        lambda_function.add_environment(
            "REDIS_CONNECTION_PARAMETER", self.redis_connection_parameter
        )
        lambda_function.add_environment("REDIS_TTL", self.redis_ttl)

        # Add function URL
        cors_options = lambda_.FunctionUrlCorsOptions(
            allowed_origins=["*"],
            allowed_methods=[lambda_.HttpMethod.POST],
            allowed_headers=["*"],
        )
        self.mea_fetch_lambda_func_url = lambda_function.add_function_url(
            auth_type=lambda_.FunctionUrlAuthType.NONE,
            # auth_type=lambda_.FunctionUrlAuthType.NONE, cors=cors_options
        )

        self.mea_fetch_lambda = self.common_setup_for_lambda(
            lambda_function,
            lambda_name,
            lambda_policy_file,
            asset_location,
            retry_attempts=2,
        )

    def create_docingestion_submitter_lambda(self):
        """
        This function will submit the email attachments to document ingestion module
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            # layers=self.assign_lambda_layers(),
            handler="index.lambda_handler",
            timeout=Duration.seconds(600),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add SNS Trigger to Lambda
        lambda_function.add_event_source(les.SnsEventSource(self.email_processed_topic))

        # Add custom environment variables
        lambda_function.add_environment(
            "DOCINGESTION_BUCKET", self.docingestion_bucket.bucket_name
        )

        self.docingestion_submitter_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_mea_submitter_lambda(self):
        """
        This function will submit the email attachments to MEA
        """
        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            layers=self.assign_lambda_layers(),
            handler="index.lambda_handler",
            timeout=Duration.seconds(120),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
            vpc=self.vpc,
            security_groups=[self.compute_security_group],
        )

        # Add Queue Trigger to Lambda
        lambda_function.add_event_source(les.SqsEventSource(self.mea_intake_queue))

        # Add custom environment variables
        lambda_function.add_environment(
            "EMAIL_EXTRACT_BUCKET", self.email_extract_bucket.bucket_name
        )
        lambda_function.add_environment(
            "MEA_API_CREDENTIAL_PARAMETER", self.mea_api_credential_parameter
        )
        lambda_function.add_environment(
            "REDIS_CONNECTION_PARAMETER", self.redis_connection_parameter
        )
        lambda_function.add_environment(
            "MEASUBMITLOG_TABLE", self.measubmitlog_table.table_name
        )
        lambda_function.add_environment("MEA_BASE_URL", self.mea_base_url)
        lambda_function.add_environment("APP_NAME", self.app_name)
        lambda_function.add_environment("REDIS_TTL", self.redis_ttl)

        self.mea_submitter_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_sns_sqs_integration_kms_key(self):
        self.sns_sqs_key = kms.Key(
            self,
            f"{self.app_name}snssqskey",
            description="KMS Key for SNS topic & SQS queue",
            alias=f"{self.app_name}snssqskey",
            enable_key_rotation=True,
            pending_window=Duration.days(7),
            removal_policy=RemovalPolicy.DESTROY,
        )

        # KMS Key Policy
        self.sns_sqs_key.add_to_resource_policy(
            iam.PolicyStatement(
                effect=iam.Effect.ALLOW,
                actions=[
                    "kms:CreateGrant",
                    "kms:Decrypt",
                    "kms:DescribeKey",
                    "kms:Encrypt",
                    "kms:GenerateDataKey",
                    "kms:GenerateDataKeyPair",
                    "kms:GenerateDataKeyPairWithoutPlaintext",
                    "kms:GenerateDataKeyWithoutPlaintext",
                    "kms:ReEncryptTo",
                    "kms:ReEncryptFrom",
                    "kms:ListAliases",
                    "kms:ListGrants",
                    "kms:ListKeys",
                    "kms:ListKeyPolicies",
                ],
                resources=["*"],
                principals=[
                    iam.ServicePrincipal("sqs.amazonaws.com"),
                    iam.ServicePrincipal("sns.amazonaws.com"),
                ],
            )
        )

    def create_queue(
        self,
        queue_name,
        visibility_timeout,
        encryption_master_key=None,
    ):
        """
        This function will create queue and dlqs
        """
        queue_dlq = sqs.Queue(
            self,
            f"DLQ_{self.app_name}{queue_name}",
            queue_name=f"DLQ_{self.app_name}{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            retention_period=Duration.seconds(1209600),
        )
        queue = sqs.Queue(
            self,
            f"{self.app_name}{queue_name}",
            queue_name=f"{self.app_name}{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            visibility_timeout=Duration.seconds(visibility_timeout),
            dead_letter_queue=sqs.DeadLetterQueue(max_receive_count=3, queue=queue_dlq),
            retention_period=Duration.seconds(1209600),
        )
        return queue

    @staticmethod
    def add_options_to_apigateway_method(apimethod):
        """
        This function will create OPTIONS method for API Gateway resource
        """
        options_ig = apigateway.IntegrationResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": "'Content-Type,Authorization,X-Amz-Date,X-Api-Key,X-Amz-Security-Token'",
                "method.response.header.Access-Control-Allow-Methods": "'DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT'",
                "method.response.header.Access-Control-Allow-Origin": "'*'",
            },
            # response_templates={"application/json": ""},
        )

        options_mr = apigateway.MethodResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": True,
                "method.response.header.Access-Control-Allow-Methods": True,
                "method.response.header.Access-Control-Allow-Origin": True,
            },
            # response_models={"application/json": ""},
        )

        apimethod.add_method(
            "OPTIONS",
            integration=apigateway.MockIntegration(
                request_templates={"application/json": '{ "statusCode": 200 }'},
                integration_responses=[options_ig],
            ),
            method_responses=[options_mr],
        )

        return apimethod

    def create_api_gateway(self):
        """
        This function will create API Gateway and resources
        """

        self.sqs_integration_role = iam.Role(
            self,
            f"{self.app_name}sqsintegrationrole",
            assumed_by=iam.ServicePrincipal("apigateway.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name("AmazonSQSFullAccess")
            ],
        )

        self.mea_api_name = f"{self.app_name}{self.mea_api_name}"

        log_group = logs.LogGroup(
            self,
            f"{self.mea_api_name}apilogs",
            log_group_name=f"{self.mea_api_name}apilogs",
        )

        mea_api = apigateway.RestApi(
            self,
            self.mea_api_name,
            rest_api_name=self.mea_api_name,
            retain_deployments=False,
            deploy=False,
            endpoint_types=[apigateway.EndpointType.REGIONAL],
            # disable_execute_api_endpoint=True,
        )

        mea_webhook_resource = mea_api.root.add_resource(self.mea_api_resource_name)
        # mea_webhook_resource.add_method(
        #     "POST",
        #     apigateway.LambdaIntegration(
        #         self.mea_webhook_lambda, allow_test_invoke=False
        #     ),
        #     api_key_required=True,
        # )

        mea_webhook_sqs_integration_response = apigateway.IntegrationResponse(
            status_code="200",
            response_templates={"application/json": ""},
        )
        mea_webhook_sqs_integration_options = apigateway.IntegrationOptions(
            credentials_role=self.sqs_integration_role,
            integration_responses=[mea_webhook_sqs_integration_response],
            request_templates={
                "application/json": "Action=SendMessage&MessageBody=$util.urlEncode($input.body)"
            },
            passthrough_behavior=apigateway.PassthroughBehavior.NEVER,
            request_parameters={
                "integration.request.header.Content-Type": "'application/x-www-form-urlencoded'"
            },
        )

        mea_webhook_sqs_integration = apigateway.AwsIntegration(
            service="sqs",
            integration_http_method="POST",
            path="{}/{}".format(
                self.account_id, self.cipmea_webhook_intake_queue.queue_name
            ),
            options=mea_webhook_sqs_integration_options,
        )

        mea_webhook_sqs_method_response = apigateway.MethodResponse(status_code="200")

        # Add the API GW Integration to the "example" API GW Resource
        mea_webhook_resource.add_method(
            "POST",
            mea_webhook_sqs_integration,
            method_responses=[mea_webhook_sqs_method_response],
            api_key_required=True,
        )
        mea_webhook_resource = self.add_options_to_apigateway_method(
            mea_webhook_resource
        )

        deployment = apigateway.Deployment(self, "Deployment1", api=mea_api)

        stage = apigateway.Stage(
            self,
            self.app_environment,
            stage_name=self.app_environment,
            deployment=deployment,
            logging_level=apigateway.MethodLoggingLevel.INFO,
            data_trace_enabled=True,
            access_log_destination=apigateway.LogGroupLogDestination(log_group),
            access_log_format=apigateway.AccessLogFormat.json_with_standard_fields(
                caller=False,
                http_method=True,
                ip=True,
                protocol=True,
                request_time=True,
                resource_path=True,
                response_length=True,
                status=True,
                user=True,
            ),
        )

        mea_api_key = apigateway.ApiKey(
            self,
            f"{self.mea_api_name}apikey",
            api_key_name=self.mea_api_name,
            value=self.mea_webhook_usage_api_key,
        )

        mea_plan = mea_api.add_usage_plan(
            f"{self.mea_api_name}usageplan",
            name=self.mea_api_name,
            throttle={"rate_limit": 100, "burst_limit": 100},
            api_stages=[apigateway.UsagePlanPerApiStage(api=mea_api, stage=stage)],
        )

        mea_plan.add_api_key(mea_api_key)

        domain_name = apigateway.DomainName.from_domain_name_attributes(
            self,
            self.api_custom_domain_name_parameter,
            domain_name=self.api_custom_domain_name,
            domain_name_alias_hosted_zone_id=self.api_custom_domain_name_zoneid,
            domain_name_alias_target=self.api_custom_domain_name_alias,
        )

        apigateway.BasePathMapping(
            self,
            f"{self.mea_api_name}pathmapping",
            domain_name=domain_name,
            rest_api=mea_api,
            base_path=self.mea_api_name.replace(self.app_name, ""),
            stage=stage,
        )

    def assign_permissions(self):
        """
        Add permissions to the resources
        """
        self.mea_intake_queue.grant_consume_messages(self.mea_submitter_lambda)
        self.email_extract_bucket.grant_read(self.docingestion_submitter_lambda)

        self.email_extract_bucket.grant_read_write(self.mea_submitter_lambda)
        self.appetite_landing_bucket.grant_read_write(self.mea_fetch_lambda)
        self.docingestion_bucket.grant_read_write(self.docingestion_submitter_lambda)

        self.measubmitlog_table.grant_read_write_data(self.mea_submitter_lambda)

    def create_dynamodb(self):
        self.measubmitlog_table = dynamodb.Table(
            self,
            f"{self.app_name}-{self.log_table}",
            table_name=f"{self.app_name}-{self.log_table}",
            partition_key=dynamodb.Attribute(
                name="applicationid", type=dynamodb.AttributeType.STRING
            ),
            removal_policy=RemovalPolicy.RETAIN,
            billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,
            point_in_time_recovery=True,
        )

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.config = vars(kwargs.pop("config"))
        self.logging = kwargs.pop("logging")
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")

        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()

        self.create_dynamodb()

        self.create_lambda_layers()

        self.create_sns_sqs_integration_kms_key()

        self.mea_intake_queue = self.create_queue(
            self.mea_intake_queue, 120, encryption_master_key=self.sns_sqs_key
        )

        self.email_processed_topic.add_subscription(
            subscriptions.SqsSubscription(self.mea_intake_queue)
        )

        self.create_mea_submitter_lambda()

        self.cipmea_webhook_intake_queue = self.create_queue(
            self.cipmea_webhook_intake_queue, 180
        )

        self.create_mea_fetch_lambda()

        self.create_mea_webhook_lambda()

        self.create_docingestion_submitter_lambda()

        self.create_api_gateway()

        self.assign_permissions()
"""
This is the CDK Stack creation file
"""
import json
import os
import re
import sys
import subprocess
from pathlib import Path
from constructs import Construct
from botocore.config import Config
import aws_cdk as core
import boto3
from dotenv import dotenv_values
from aws_cdk import (
    Stack,
    CfnOutput,
    Duration,
    aws_s3 as s3,
    aws_s3_deployment as s3deploy,
    aws_logs as logs,
    aws_iam as iam,
    aws_ssm as ssm,
    aws_lambda as lambda_,
    aws_s3_notifications as s3_notify,
    aws_cloudwatch as cloudwatch,
    aws_events_targets as targets,
    aws_events as events,
    aws_cognito as cognito,
    aws_dynamodb as dynamodb,
    aws_cloudwatch_actions,
    aws_sns as sns,
    aws_apigateway as apigateway,
    aws_sqs as sqs,
    aws_lambda_event_sources as les,
)

PYTHON_RUNTIME = lambda_.Runtime.PYTHON_3_9
PYTHON_VERSION = "3.9"


class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        response = self.ssm_client.get_parameter(Name=path, WithDecryption=True)
        return response["Parameter"]["Value"]

    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        print(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()

        data = json.loads(data)
        return data

    # Function to create an CloudWatch Alarm for Lambda Failures
    def create_lambda_failure_cloudwatch_alarm(self, lambda_function, lambda_name):
        """
        This function will create cloudwatch alarm for all lambda functions
        """
        lambda_errors_metric = cloudwatch.Metric(
            metric_name="Errors",
            statistic="max",
            namespace="AWS/Lambda",
            dimensions_map={"FunctionName": lambda_function.function_name},
        )
        alarm_name = lambda_name
        alarm_lambda_errors = cloudwatch.Alarm(
            self,
            metric=lambda_errors_metric,
            id=f"{alarm_name}_errors",
            alarm_name=f"{alarm_name}_errors",
            treat_missing_data=cloudwatch.TreatMissingData.NOT_BREACHING,
            evaluation_periods=1,
            threshold=1,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_OR_EQUAL_TO_THRESHOLD,
            datapoints_to_alarm=1,
        )

        alarm_lambda_errors.add_alarm_action(
            aws_cloudwatch_actions.SnsAction(self.failure_notification_topic)
        )

    def common_setup_for_lambda(
        self, lambda_function, lambda_name, policy_file, asset_location
    ):
        """
        This function is for apply common properties to Lambda
        """
        # Add permissions for lambda
        lambda_policy = self.get_access_policy(
            Path(self.policies_path).joinpath(policy_file)
        )
        for policy in lambda_policy:
            lambda_function.add_to_role_policy(iam.PolicyStatement.from_json(policy))

        # Add environment vairables
        lambda_function.add_environment("APP_ENVIRONMENT", self.app_environment)
        lambda_function.add_environment("APP_NAME_TAG", self.app_name_tag)
        lambda_function.add_environment("MODULE_NAME_TAG", self.module_name_tag)
        lambda_function.add_environment("LOG_LEVEL", self.log_level)
        for envs in dotenv_values(Path(asset_location).joinpath(".env")).items():
            key, value = envs
            lambda_function.add_environment(key, value)

        # Set Lambda Retries
        lambda_function.configure_async_invoke(retry_attempts=0)

        # Create an CloudWatch Alarm for Lambda Failures
        self.create_lambda_failure_cloudwatch_alarm(lambda_function, lambda_name)

        return lambda_function

    def lambda_attributes(self, name):
        name = name.split("_")[1:]
        name = "_".join(name)
        return (
            getattr(self, name),
            getattr(self, f"{name}_description"),
            f"{name}_policy.json",
        )

    def env_vars_quick_load(self):
        print(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def create_boto_clients(self, clients):
        for client in clients:
            formatted_client = re.sub("[^A-Za-z0-9]+", "", client)
            setattr(
                self,
                f"{formatted_client}_client",
                boto3.client(
                    client,
                    region_name=self.region_name,
                    config=Config(
                        retries={"max_attempts": 2, "mode": "standard"},
                        connect_timeout=30,
                    ),
                ),
            )

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        sts = boto3.client("sts")
        self.account_id = sts.get_caller_identity()["Account"]

        self.failure_notification_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.failure_notification_topic_parameter}"
        )
        self.failure_notification_topic = sns.Topic.from_topic_arn(
            self,
            id="mainalarmtopic",
            topic_arn=self.failure_notification_topic,
        )
        self.client_identifier = self.from_ssm(self.client_identifier_parameter)
        self.status_update_lambda = lambda_.Function.from_function_arn(
            self, "statusupdatelambda", function_arn=self.status_update_lambda_arn
        )
        self.propensity_queue = (
            f"arn:aws:sqs:{self.region}:{self.account_id}:{self.propensity_queue}"
        )
        self.propensity_queue = sqs.Queue.from_queue_arn(
            self, "propensityqueue", queue_arn=self.propensity_queue
        )
        self.comparative_queue = (
            f"arn:aws:sqs:{self.region}:{self.account_id}:{self.comparative_queue}"
        )
        self.comparative_queue = sqs.Queue.from_queue_arn(
            self, "comparativequeue", queue_arn=self.comparative_queue
        )
        self.submission_queue = (
            f"arn:aws:sqs:{self.region}:{self.account_id}:{self.submission_queue}"
        )
        self.submission_queue = sqs.Queue.from_queue_arn(
            self, "submissionqueue", queue_arn=self.submission_queue
        )
        self.auto_decline_lambda = lambda_.Function.from_function_arn(
            self, "autodeclinelambda", function_arn=self.auto_decline_lambda_arn
        )
        self.set_cors_response_lambda = lambda_.Function.from_function_arn(
            self, "setcorsresponselambda", function_arn=self.set_cors_response_lambda
        )
        self.lambda_authorizer = lambda_.Function.from_function_arn(
            self, "lambdaauthorizer", function_arn=self.lambda_authorizer
        )
        self.cors_validator_lambda = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.cors_validator_lambda_parameter}"
        )
        self.cors_validator_lambda = lambda_.Function.from_function_arn(
            self, "corsvalidator", function_arn=self.cors_validator_lambda
        )

    def assign_lambda_layers(self, layers=["All"]):
        if layers[0] == "All":
            lambda_layers = [layer_info[1] for layer_info in self.lambda_layers]
            return lambda_layers

        lambda_layers = []
        for layer_name in layers:
            for layer_info in self.lambda_layers:
                if layer_name in layer_info[0]:
                    lambda_layers.append(layer_info[1])
        return lambda_layers

    def create_lambda_layers(self):
        """
        This function will create lambda layers
        """
        self.lambda_layers = []

        for layer in list(filter(None, self.lambda_layers_build.split(";"))):
            asset_location = str(Path(self.code_location).joinpath(layer))
            print(f"Code Location for {layer}: {asset_location}")
            cmds = f"pip3 install --platform manylinux2014_x86_64 --implementation cp --python {PYTHON_VERSION} --only-binary=:all: -r {asset_location}/python/requirements.txt -t {asset_location}/python"
            subprocess.check_call([cmds], shell=True)
            lambda_layers_build = lambda_.LayerVersion(
                self,
                layer,
                layer_version_name=layer,
                code=lambda_.Code.from_asset(asset_location),
                compatible_runtimes=[PYTHON_RUNTIME],
                compatible_architectures=[lambda_.Architecture.X86_64],
                description=f"{self.app_name_tag}-{self.module_name_tag}-{layer}",
            )
            self.lambda_layers.append((layer, lambda_layers_build))

        for layer in list(filter(None, self.lambda_layers_existing.split(";"))):
            layer_arn = self.from_ssm(f"{self.cfnoutput_parameter}/{layer}")
            lambda_layers_existing = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer_arn
            )
            self.lambda_layers.append((layer, lambda_layers_existing))

        for layer in list(filter(None, self.lambda_layers_arn.split(";"))):
            self.lambda_client.get_layer_version_by_arn(Arn=layer)
            lambda_layers_arn = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer
            )
            self.lambda_layers.append((layer, lambda_layers_arn))

    def create_appetite_rules_data_lambda(self):
        """
        This function will create appetite data lambda
        """

        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(
                ["idfbin38", "AWSDataWrangler", "customexceptions"]
            ),
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )

        self.appetite_rules_data_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_appetite_rules_execute_lambda(self):
        """
        This function will create appetite execute lambda
        """
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(),
            timeout=Duration.seconds(300),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )
        lambda_function.add_environment(
            "AUTO_DECLINE_LAMBDA_ARN", self.auto_decline_lambda.function_arn
        )
        lambda_function.add_environment(
            "STATUS_UPDATE_LAMBDA_ARN", self.status_update_lambda.function_arn
        )

        self.appetite_rules_execute_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_appetite_rules_manage_lambda(self):
        """
        This function will create appetite manage lambda
        """
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(
                ["idfbin38", "AWSDataWrangler", "customexceptions"]
            ),
            timeout=Duration.seconds(300),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )

        self.appetite_rules_manage_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_dynamic_json_lambda(self):
        """
        This function will create dynamic json
        """
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["idfbin38", "AWSDataWrangler"]),
            timeout=Duration.seconds(600),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add Queue Trigger to Lambda
        lambda_function.add_event_source(les.SqsEventSource(self.submission_queue))

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )
        lambda_function.add_environment("FIRST_PULL_PATH", self.first_pull_path)
        lambda_function.add_environment("SECOND_PULL_PATH", self.second_pull_path)

        self.dynamic_json_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_firstpull_flatten_lambda(self):
        """
        This function will create first pull flatten lambda
        """
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["idfbin38", "AWSDataWrangler"]),
            timeout=Duration.seconds(180),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.appetite_landing_bucket)
        self.appetite_landing_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(prefix=self.first_pull_path, suffix=".json"),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "STM_DATALOAD_LAMBDA_ARN",
            self.stm_dataload_lambda.function_arn,
        )
        lambda_function.add_environment(
            "STATUS_UPDATE_LAMBDA_ARN", self.status_update_lambda.function_arn
        )
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )

        self.firstpull_flatten_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_secondpull_flatten_lambda(self):
        """
        This function will create kick off second pull
        """
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(
                ["idfbin38", "AWSDataWrangler", "customexceptions"]
            ),
            timeout=Duration.seconds(60),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment("SECOND_PULL_PATH", self.second_pull_path)
        lambda_function.add_environment(
            "FIRSTPULL_FLATTEN_LAMBDA_ARN", self.firstpull_flatten_lambda.function_arn
        )
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )

        self.secondpull_flatten_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_recommended_focus_widget_lambda(self):
        """
        This function will provide data for recommendation widget
        """
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(
                ["idfbin38", "AWSDataWrangler", "customexceptions"]
            ),
            timeout=Duration.seconds(60),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add CRON Trigger to Lambda
        lambda_rule = events.Rule(
            self,
            f"{lambda_name}trigger",
            rule_name=f"{lambda_name}trigger",
            schedule=events.Schedule.expression(
                self.recommended_focus_widget_frequency_in_minutes
            ),
        )
        lambda_rule.add_target(targets.LambdaFunction(lambda_function))

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )

        self.recommended_focus_widget_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_stm_dataload_lambda(self):
        """
        This function will create load data into the tables
        """
        (lambda_name, lambda_desc, lambda_policy_file) = self.lambda_attributes(
            sys._getframe().f_code.co_name
        )

        asset_location = str(Path(self.code_location).joinpath(lambda_name))
        print(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["idfbin38", "AWSDataWrangler"]),
            timeout=Duration.seconds(180),
            memory_size=256,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "STATUS_UPDATE_LAMBDA_ARN", self.status_update_lambda.function_arn
        )
        lambda_function.add_environment(
            "APPETITE_RULES_EXECUTE_LAMBDA",
            self.appetite_rules_execute_lambda.function_arn,
        )
        lambda_function.add_environment(
            "PROPENSITY_QUEUE",
            self.propensity_queue.queue_name,
        )
        lambda_function.add_environment(
            "COMPARATIVE_QUEUE",
            self.comparative_queue.queue_name,
        )
        lambda_function.add_environment(
            "APPETITE_LANDING_BUCKET", self.appetite_landing_bucket.bucket_name
        )

        self.stm_dataload_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_queue(
        self,
        queue_name,
        visibility_timeout,
        encryption_master_key=None,
    ):
        """
        This function will create queue and dlqs
        """
        queue_dlq = sqs.Queue(
            self,
            f"DLQ_{queue_name}",
            queue_name=f"DLQ_{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            retention_period=Duration.seconds(1209600),
        )
        queue = sqs.Queue(
            self,
            f"{queue_name}",
            queue_name=f"{queue_name}",
            encryption=sqs.QueueEncryption.KMS_MANAGED,
            encryption_master_key=encryption_master_key,
            visibility_timeout=Duration.seconds(visibility_timeout),
            dead_letter_queue=sqs.DeadLetterQueue(max_receive_count=3, queue=queue_dlq),
        )
        return queue

    @staticmethod
    def enable_ssl(bucket):
        bucket.add_to_resource_policy(
            iam.PolicyStatement(
                effect=iam.Effect.DENY,
                actions=["s3:*"],
                resources=[
                    f"{bucket.bucket_arn}/*",
                    f"{bucket.bucket_arn}",
                ],
                principals=[iam.StarPrincipal()],
                conditions={"Bool": {"aws:SecureTransport": False}},
            )
        )

    def create_buckets(self):
        """
        This function will create buckets
        """
        self.appetite_landing_bucket = s3.Bucket(
            self,
            f"{self.appetite_landing_bucket}-{self.client_identifier}",
            bucket_name=f"{self.appetite_landing_bucket}-{self.client_identifier}",
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
        )
        self.enable_ssl(self.appetite_landing_bucket)

        CfnOutput(
            self,
            "appetitelandingbucket",
            value=self.appetite_landing_bucket.bucket_name,
        )

        ssm.StringParameter(
            self,
            "appetitelandingbucketparameter",
            parameter_name=f"{self.cfnoutput_parameter}/appetitelandingbucket",
            string_value=self.appetite_landing_bucket.bucket_name,
            type=ssm.ParameterType.STRING,
        )

    @staticmethod
    def add_lambda_to_options_to_apigateway_method(apimethod, cors_lambda):
        apimethod.add_method(
            "OPTIONS",
            apigateway.LambdaIntegration(cors_lambda, allow_test_invoke=False),
        )
        return apimethod

    @staticmethod
    def add_options_to_apigateway_method(apimethod):
        """
        This function will create OPTIONS method for API Gateway resource
        """
        options_ig = apigateway.IntegrationResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": "'Content-Type,Authorization,X-Amz-Date,X-Api-Key,X-Amz-Security-Token'",
                "method.response.header.Access-Control-Allow-Methods": "'DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT'",
                "method.response.header.Access-Control-Allow-Origin": "'*'",
            },
            # response_templates={"application/json": ""},
        )

        options_mr = apigateway.MethodResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": True,
                "method.response.header.Access-Control-Allow-Methods": True,
                "method.response.header.Access-Control-Allow-Origin": True,
            },
            # response_models={"application/json": ""},
        )

        apimethod.add_method(
            "OPTIONS",
            integration=apigateway.MockIntegration(
                request_templates={"application/json": '{ "statusCode": 200 }'},
                integration_responses=[options_ig],
            ),
            method_responses=[options_mr],
        )

        return apimethod

    def create_api_gateway(self):
        """
        This function will create API Gateway and resources
        """
        log_group = logs.LogGroup(
            self,
            f"{self.rules_engine_api_name}apilogs",
            log_group_name=f"{self.rules_engine_api_name}apilogs",
        )

        # response = self.cognitoidp_client.list_user_pools(MaxResults=50)
        # user_pools = [
        #     ip["Id"]
        #     for ip in response["UserPools"]
        #     if ip["Name"] == self.cognito_authorizer
        # ]
        # user_pool = cognito.UserPool.from_user_pool_id(
        #     self, "cognitouserpool", user_pool_id=user_pools[0]
        # )

        # auth = apigateway.CognitoUserPoolsAuthorizer(
        #     self, "cognitoauthorizer", cognito_user_pools=[user_pool]
        # )
        auth = apigateway.TokenAuthorizer(
            self, "tokenauthorizer", handler=self.lambda_authorizer
        )

        rules_engine_api = apigateway.RestApi(
            self,
            self.rules_engine_api_name,
            rest_api_name=self.rules_engine_api_name,
            retain_deployments=False,
            deploy=False,
            endpoint_types=[apigateway.EndpointType.REGIONAL],
            disable_execute_api_endpoint=True,
            api_key_source_type=apigateway.ApiKeySourceType.AUTHORIZER,
        )

        appetite_data_request_validator = rules_engine_api.add_request_validator(
            "DefaultValidator", validate_request_parameters=True
        )

        # GetRulesData
        appetite_data_one = rules_engine_api.root.add_resource(
            self.appetite_data_api_resource_name_one
        )
        appetite_data_one.add_method(
            "GET",
            apigateway.LambdaIntegration(
                self.appetite_rules_data_lambda, allow_test_invoke=False
            ),
            api_key_required=True,
            request_validator=appetite_data_request_validator,
            request_parameters={
                "method.request.querystring.parameter": True,
                "method.request.querystring.lob": False,
            },
            authorizer=auth,
            # authorization_type=apigateway.AuthorizationType.COGNITO,
            authorization_type=apigateway.AuthorizationType.CUSTOM,
        )
        # appetite_data_one = self.add_options_to_apigateway_method(appetite_data_one)
        appetite_data_one = self.add_lambda_to_options_to_apigateway_method(
            appetite_data_one, self.cors_validator_lambda
        )

        # GetRulesOutcome
        appetite_data_two = rules_engine_api.root.add_resource(
            self.appetite_data_api_resource_name_two
        )
        appetite_data_two.add_method(
            "GET",
            apigateway.LambdaIntegration(
                self.appetite_rules_data_lambda, allow_test_invoke=False
            ),
            api_key_required=True,
            request_validator=appetite_data_request_validator,
            request_parameters={
                "method.request.querystring.application_number": True,
                "method.request.querystring.business_rule_type": True,
                "method.request.querystring.transaction_type": True,
            },
            authorizer=auth,
            authorization_type=apigateway.AuthorizationType.CUSTOM,
        )
        # appetite_data_two = self.add_options_to_apigateway_method(appetite_data_two)
        appetite_data_two = self.add_lambda_to_options_to_apigateway_method(
            appetite_data_two, self.cors_validator_lambda
        )

        # GetRules
        for resource in [
            self.appetite_manage_api_resource_name_one,
        ]:
            get_resource = rules_engine_api.root.add_resource(resource)
            get_resource.add_method(
                "GET",
                apigateway.LambdaIntegration(
                    self.appetite_rules_manage_lambda, allow_test_invoke=False
                ),
                api_key_required=True,
                request_validator=appetite_data_request_validator,
                request_parameters={
                    "method.request.querystring.parameter": True,
                    "method.request.querystring.rule_id": False,
                },
                authorizer=auth,
                authorization_type=apigateway.AuthorizationType.CUSTOM,
            )
            # get_resource = self.add_options_to_apigateway_method(get_resource)
            get_resource = self.add_lambda_to_options_to_apigateway_method(
                get_resource, self.cors_validator_lambda
            )

        # ManageRules
        for resource in [
            self.appetite_manage_api_resource_name_two,
        ]:
            post_resource = rules_engine_api.root.add_resource(resource)
            post_resource.add_method(
                "POST",
                apigateway.LambdaIntegration(
                    self.appetite_rules_manage_lambda, allow_test_invoke=False
                ),
                api_key_required=True,
                request_validator=appetite_data_request_validator,
                request_parameters={
                    "method.request.querystring.parameter": True,
                },
                authorizer=auth,
                authorization_type=apigateway.AuthorizationType.CUSTOM,
            )
            # post_resource = self.add_options_to_apigateway_method(post_resource)
            post_resource = self.add_lambda_to_options_to_apigateway_method(
                post_resource, self.cors_validator_lambda
            )

        # RulesDuplicateCheck
        for resource in [
            self.appetite_manage_api_resource_name_three,
        ]:
            post_resource = rules_engine_api.root.add_resource(resource)
            post_resource.add_method(
                "POST",
                apigateway.LambdaIntegration(
                    self.appetite_rules_manage_lambda, allow_test_invoke=False
                ),
                api_key_required=True,
                authorizer=auth,
                authorization_type=apigateway.AuthorizationType.CUSTOM,
            )
            # post_resource = self.add_options_to_apigateway_method(post_resource)
            post_resource = self.add_lambda_to_options_to_apigateway_method(
                post_resource, self.cors_validator_lambda
            )

        # # FetchMoreFeatures
        # for resource in [
        #     self.second_flatten_api_resource_name_one,
        # ]:
        #     post_resource = rules_engine_api.root.add_resource(resource)
        #     post_resource.add_method(
        #         "POST",
        #         apigateway.LambdaIntegration(
        #             self.second_flatten_lambda, allow_test_invoke=False
        #         ),
        #         authorizer=auth,
        #         authorization_type=apigateway.AuthorizationType.COGNITO,
        #     )
        #     post_resource = self.add_options_to_apigateway_method(post_resource)
        #     # post_resource = self.add_lambda_to_options_to_apigateway_method(
        #     #     post_resource, self.cors_validator_lambda
        #     # )

        deployment = apigateway.Deployment(self, "Deployment12", api=rules_engine_api)

        stage = apigateway.Stage(
            self,
            self.app_environment,
            stage_name=self.app_environment,
            deployment=deployment,
            logging_level=apigateway.MethodLoggingLevel.INFO,
            data_trace_enabled=True,
            access_log_destination=apigateway.LogGroupLogDestination(log_group),
            access_log_format=apigateway.AccessLogFormat.json_with_standard_fields(
                caller=False,
                http_method=True,
                ip=True,
                protocol=True,
                request_time=True,
                resource_path=True,
                response_length=True,
                status=True,
                user=True,
            ),
        )

        response = self.apigateway_client.get_domain_name(
            domainName=self.api_custom_domain_name
        )

        domain_name = apigateway.DomainName.from_domain_name_attributes(
            self,
            f"{self.rules_engine_api_name}domainname",
            domain_name=response["domainName"],
            domain_name_alias_hosted_zone_id=response["distributionHostedZoneId"],
            domain_name_alias_target=response["distributionDomainName"],
        )

        apigateway.BasePathMapping(
            self,
            f"{self.rules_engine_api_name}pathmapping",
            domain_name=domain_name,
            rest_api=rules_engine_api,
            base_path=self.rules_engine_api_name,
            stage=stage,
        )

    def assign_permissions(self):
        """
        Add permissions to the resources
        """
        self.appetite_landing_bucket.grant_read_write(self.appetite_rules_data_lambda)
        self.appetite_landing_bucket.grant_read_write(
            self.appetite_rules_execute_lambda
        )
        self.appetite_landing_bucket.grant_read_write(
            self.appetite_rules_execute_lambda
        )
        self.appetite_landing_bucket.grant_read_write(self.appetite_rules_manage_lambda)
        self.appetite_landing_bucket.grant_read_write(self.dynamic_json_lambda)
        self.appetite_landing_bucket.grant_read_write(self.firstpull_flatten_lambda)
        self.appetite_landing_bucket.grant_read_write(self.secondpull_flatten_lambda)
        self.appetite_landing_bucket.grant_read(self.stm_dataload_lambda)
        self.appetite_landing_bucket.grant_read(self.recommended_focus_widget_lambda)

        self.status_update_lambda.grant_invoke(self.appetite_rules_execute_lambda)
        self.status_update_lambda.grant_invoke(self.stm_dataload_lambda)
        self.stm_dataload_lambda.grant_invoke(self.firstpull_flatten_lambda)
        self.firstpull_flatten_lambda.grant_invoke(self.secondpull_flatten_lambda)
        self.appetite_rules_execute_lambda.grant_invoke(self.stm_dataload_lambda)
        self.auto_decline_lambda.grant_invoke(self.appetite_rules_execute_lambda)

        self.propensity_queue.grant_send_messages(self.stm_dataload_lambda)
        self.comparative_queue.grant_send_messages(self.stm_dataload_lambda)

    def load_buckets(self):
        s3deploy.BucketDeployment(
            self,
            "loadconfigs",
            sources=[
                s3deploy.Source.asset(
                    f"{self.asset_location}/appetitelandingbucket/config"
                )
            ],
            destination_bucket=self.appetite_landing_bucket,
            destination_key_prefix=f"cip/standardization/config",
        )

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.config = vars(kwargs.pop("config"))
        super().__init__(scope, construct_id, **kwargs)

        self.create_boto_clients(["ssm", "cognito-idp", "apigateway", "lambda"])

        self.initialize_constants()

        self.create_lambda_layers()

        self.create_buckets()

        self.create_dynamic_json_lambda()

        self.create_appetite_rules_execute_lambda()

        self.create_stm_dataload_lambda()

        self.create_firstpull_flatten_lambda()

        self.create_secondpull_flatten_lambda()

        self.create_recommended_focus_widget_lambda()

        self.create_appetite_rules_data_lambda()

        self.create_appetite_rules_manage_lambda()

        # Order
        # 0. Flattening (if second pull)
        # 1. Flattening (1st pull)
        # 2. Standardization
        # 3. STM
        # 4. Rules Execute

        self.create_api_gateway()

        self.assign_permissions()

        # TODO: enable this after hardcoding in s3 files have been removed. fixme
        self.load_buckets()


"""
This file is to create step functions
"""
import aws_cdk.aws_stepfunctions as sfn
import aws_cdk.aws_stepfunctions_tasks as sfn_tasks
from aws_cdk import Duration


class StepFunctionService:
    """
    This is a Step Function Class
    """

    @staticmethod
    def create_classification_step_function(stack, first_lambda):
        """
        This function will create comprehend classification step function
        """
        detect_language_invoke_translate = sfn_tasks.LambdaInvoke(
            stack,
            "Detect Language and Invoke Translate API",
            lambda_function=first_lambda,
            output_path="$.Payload",
        )

        invoke_classification = sfn_tasks.LambdaInvoke(
            stack,
            "Invoke Classification API",
            lambda_function=first_lambda,
            output_path="$.Payload",
        )

        job_failed = sfn.Fail(
            stack,
            "Classification failed",
            cause="AWS Lambda Failed",
            error="Classification FAILED",
        )

        classifcation_complete = sfn.Pass(stack, "Classification complete")

        definition = detect_language_invoke_translate.next(
            sfn.Choice(stack, "Check Translation Status?")
            .when(sfn.Condition.string_equals("$.Status", "Failed"), job_failed)
            .otherwise(
                invoke_classification.next(
                    sfn.Choice(stack, "Check Classification Status?")
                    .when(sfn.Condition.string_equals("$.Status", "Failed"), job_failed)
                    .otherwise(classifcation_complete)
                )
            )
        )

        # TODO: enable CW logs later # pylint: disable=fixme
        state_machine = sfn.StateMachine(
            stack,
            stack.classification_step_function,
            state_machine_name=stack.classification_step_function,
            definition=definition,
            timeout=Duration.minutes(
                int(stack.classification_step_function_timeout_in_mins)
            ),
            tracing_enabled=True,
        )
        return state_machine

"""
This is the CDK Stack creation file
"""
from pathlib import Path
import json
import os
import re
import sys
import subprocess
import glob
from dotenv import dotenv_values
from aws_cdk import (
    Stack,
    Duration,
    CfnOutput,
    RemovalPolicy,
    aws_s3 as s3,
    aws_certificatemanager as acm,
    aws_s3_deployment as s3deploy,
    aws_quicksight as quicksight,
    aws_glue as glue,
    aws_dynamodb as dynamodb,
    aws_logs as logs,
    aws_iam as iam,
    aws_lambda as lambda_,
    aws_events as events,
    aws_events_targets as targets,
    aws_apigateway as apigateway,
    aws_s3_notifications as s3_notify,
    aws_cloudwatch as cloudwatch,
    aws_cloudwatch_actions,
    aws_lambda_event_sources as les,
    aws_sns as sns,
    aws_ssm as ssm,
    aws_ec2 as ec2,
)
from constructs import Construct
from botocore.config import Config
from .stepfunctions_service import StepFunctionService

PYTHON_RUNTIME = lambda_.Runtime.PYTHON_3_9
PYTHON_VERSION = "3.9"


class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value

    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        self.logging.info(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()
        # try:
        #     data = data.replace(
        #         "ETL_INBOUND_BUCKET", self.etl_inbound_bucket.bucket_name
        #     )
        # except Exception:
        #     pass
        data = json.loads(data)
        return data

    # Function to create an CloudWatch Alarm for Lambda Failures
    def create_lambda_failure_cloudwatch_alarm(self, lambda_function, lambda_name):
        """
        This function will create cloudwatch alarm for all lambda functions
        """
        lambda_errors_metric = cloudwatch.Metric(
            metric_name="Errors",
            statistic="max",
            namespace="AWS/Lambda",
            dimensions_map={"FunctionName": lambda_function.function_name},
        )
        # alarm_name = re.sub("[\W_]+", "", lambda_name)
        alarm_name = lambda_name
        alarm_lambda_errors = cloudwatch.Alarm(
            self,
            metric=lambda_errors_metric,
            id=f"{alarm_name}_errors",
            alarm_name=f"{alarm_name}_errors",
            treat_missing_data=cloudwatch.TreatMissingData.NOT_BREACHING,
            evaluation_periods=1,
            threshold=1,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_OR_EQUAL_TO_THRESHOLD,
            datapoints_to_alarm=1,
        )
        alarm_lambda_errors.add_alarm_action(
            aws_cloudwatch_actions.SnsAction(self.failure_notification_topic)
        )

    def common_setup_for_lambda(
        self, lambda_function, lambda_name, policy_file, asset_location
    ):
        """
        This function is for apply common properties to Lambda
        """
        # Add permissions for lambda
        lambda_policy = self.get_access_policy(
            Path(self.policies_path).joinpath(policy_file)
        )
        for policy in lambda_policy:
            lambda_function.add_to_role_policy(iam.PolicyStatement().from_json(policy))

        # Add environment vairables
        lambda_function.add_environment("APP_ENVIRONMENT", self.app_environment)
        lambda_function.add_environment("APP_NAME_TAG", self.app_name_tag)
        lambda_function.add_environment("MODULE_NAME_TAG", self.module_name_tag)
        lambda_function.add_environment("LOG_LEVEL", self.log_level)
        for envs in dotenv_values(Path(asset_location).joinpath(".env")).items():
            key, value = envs
            lambda_function.add_environment(key, value)

        # Set Lambda Retries
        lambda_function.configure_async_invoke(retry_attempts=0)

        # Create an CloudWatch Alarm for Lambda Failures
        self.create_lambda_failure_cloudwatch_alarm(lambda_function, lambda_name)

        return lambda_function

    def lambda_attributes(self, name):
        name = name.split("_")[1:]
        name = "_".join(name)
        return (
            f"{self.app_name}{getattr(self, name)}",
            getattr(self, f"{name}_description"),
            f"{name}_policy.json",
            getattr(self, name),
        )

    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.cfnoutput_parameter = self.cfnoutput_parameter.replace("#", self.app_name)
        self.database_connection_parameter = (
            f"{self.cfnoutput_parameter}{self.database_connection_parameter}"
        )

        self.failure_notification_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.failure_notification_topic_parameter}"
        )
        self.failure_notification_topic = sns.Topic.from_topic_arn(
            self,
            id="mainalarmtopic",
            topic_arn=self.failure_notification_topic,
        )

        self.datasetready_topic = self.from_ssm(
            f"{self.cfnoutput_parameter}/{self.datasetready_topic_parameter}"
        )
        self.datasetready_topic = sns.Topic.from_topic_arn(
            self,
            f"{self.app_name}{self.datasetready_topic_parameter}",
            topic_arn=self.datasetready_topic,
        )

        self.api_custom_domain_name = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/name",
        )
        self.api_custom_domain_name_alias = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/alias",
        )
        self.api_custom_domain_name_zoneid = ssm.StringParameter.value_from_lookup(
            self,
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/zoneid",
        )

        self.compute_security_group = ec2.SecurityGroup.from_security_group_id(
            self,
            "LambdaSecurityGroup",
            security_group_id=self.from_ssm(
                f"{self.cfnoutput_parameter}/{self.compute_security_group_parameter}"
            ),
        )

        vpc_id = ssm.StringParameter.value_from_lookup(
            self, parameter_name=f"{self.cfnoutput_parameter}/{self.vpc_parameter}"
        )
        self.vpc = ec2.Vpc.from_lookup(self, "VPC", vpc_id=vpc_id)

        # self.classification_step_function_arn = f"arn:aws:states:{self.region_name}:{self.account_id}:stateMachine:{self.classification_step_function}"

    def assign_lambda_layers(self, layers=["All"]):
        if layers[0] == "All":
            lambda_layers = [layer_info[1] for layer_info in self.lambda_layers]
            return lambda_layers

        lambda_layers = []
        for layer_name in layers:
            for layer_info in self.lambda_layers:
                if layer_name in layer_info[0]:
                    lambda_layers.append(layer_info[1])
        return lambda_layers

    def create_lambda_layers(self):
        """
        This function will create lambda layers
        """
        self.lambda_layers = []

        for layer in list(filter(None, self.lambda_layers_build.split(";"))):
            asset_location = str(Path(self.code_location).joinpath(layer))
            self.logging.info(f"Code Location for {layer}: {asset_location}")
            if not self.is_velocity:
                cmds = f"pip3 install --platform manylinux2014_x86_64 --implementation cp --python {PYTHON_VERSION} --only-binary=:all: -r {asset_location}/python/requirements.txt -t {asset_location}/python"
                subprocess.check_call([cmds], shell=True)
            lambda_layers_build = lambda_.LayerVersion(
                self,
                f"{self.app_name}{layer}",
                layer_version_name=f"{self.app_name}{layer}",
                code=lambda_.Code.from_asset(asset_location),
                compatible_runtimes=[PYTHON_RUNTIME],
                compatible_architectures=[lambda_.Architecture.X86_64],
                description=f"{self.app_name_tag}-{self.module_name_tag}-{layer}",
            )
            self.lambda_layers.append((layer, lambda_layers_build))

        for layer in list(filter(None, self.lambda_layers_existing.split(";"))):
            layer_arn = self.from_ssm(f"{self.cfnoutput_parameter}/{layer}")
            lambda_layers_existing = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer_arn
            )
            self.lambda_layers.append((layer, lambda_layers_existing))

        for layer in list(filter(None, self.lambda_layers_arn.split(";"))):
            # self.lambda_client.get_layer_version_by_arn(Arn=layer)
            lambda_layers_arn = lambda_.LayerVersion.from_layer_version_arn(
                self, layer, layer
            )
            self.lambda_layers.append((layer, lambda_layers_arn))

    def create_classification_orchestrator_lambda(self):
        """
        This function will orchestrate with Language Detection, Language Translation and Text Classification
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["dbutilities"]),
            timeout=Duration.seconds(900),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "MODEL_INFERENCE_PARAMETER",
            f"{self.cfnoutput_parameter}/{self.model_inference_parameter}",
        )
        lambda_function.add_environment(
            "DETECT_LANGUAGE_LAMBDA", self.language_detection_lambda.function_arn
        )
        lambda_function.add_environment(
            "TRANSLATE_LAMBDA", self.language_translation_lambda.function_arn
        )

        self.classification_orchestrator_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_classification_receiver_lambda(self):
        """
        This function will recieve the request from website to perform classification
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["dbutilities"]),
            timeout=Duration.seconds(600),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "CLASSIFICATION_STEP_FUNCTION_CONFIG",
            self.classification_state_machine.state_machine_arn,
        )

        self.classification_receiver_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_classification_errors_lambda(self):
        """
        This function will start the dataset creation process for model training
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["dbutilities"]),
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
            vpc=self.vpc,
            security_groups=[self.compute_security_group],
        )

        # Add CRON Trigger to Lambda
        lambda_rule = events.Rule(
            self,
            f"{lambda_name}trigger",
            rule_name=f"{lambda_name}trigger",
            schedule=events.Schedule.expression(
                self.classification_errors_frequency_in_minutes
            ),
        )
        lambda_rule.add_target(targets.LambdaFunction(lambda_function))

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "CUSTOM_DOMAIN_NAME_URL_PARAMETER",
            f"{self.cfnoutput_parameter}/{self.api_custom_domain_name_parameter}/name",
        )

        self.classification_errors_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_classification_dataset_handler_lambda(self):
        """
        This function will perform post processing after training dataset is created
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add SNS Trigger to Lambda
        lambda_function.add_event_source(les.SnsEventSource(self.datasetready_topic))

        # Add custom environment variables
        # lambda_function.add_environment("X", self.X)

        self.classification_dataset_handler_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_document_data_lambda(self):
        """
        This function will retreive document data
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["idfbin38", "AWSDataWrangler"]),
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )

        self.document_data_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_document_processor_lambda(self):
        """
        This function will receive the request to process documents through image quality, classification and extraction
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["AWSDataWrangler", "idfbin38"]),
            timeout=Duration.seconds(600),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(
                prefix=self.outgoing_prefix_nosplit, suffix=".pdf"
            ),
        )

        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(
                prefix=self.outgoing_prefix_nosplit, suffix=".csv"
            ),
        )

        # ASP changes done for pncdidev
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(
                prefix=self.outgoing_prefix_nosplit, suffix=".txt"
            ),
        )

        # ASP changes for html to appear on front end portal
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(
                prefix=self.outgoing_prefix_nosplit, suffix=".html"
            ),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )
        lambda_function.add_environment(
            "IMAGE_QUALITY_URL_PARAMETER",
            f"{self.cfnoutput_parameter}/{self.image_quality_url_parameter}",
        )
        lambda_function.add_environment(
            "CLASSIFICATION_URL",
            self.stage.url_for_path(f"/{self.receiver_api_resource_name}"),
        )
        lambda_function.add_environment(
            "CLASSIFICATION_MODEL", self.classification_model
        )

        self.document_processor_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_imagequality_lambda(self):
        """
        This function will receive trigger from image quality module and load the json
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["AWSDataWrangler", "idfbin38"]),
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(
                prefix=self.outgoing_prefix_nosplit, suffix=".pdf"
            ),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )

        self.imagequality_load_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_language_detection_lambda(self):
        """
        This function will create language detection lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.DockerImageFunction(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.DockerImageCode().from_image_asset(directory=asset_location),
            description=lambda_desc,
            timeout=Duration.seconds(900),
            memory_size=10240,
            tracing=lambda_.Tracing.ACTIVE,
        )

        # Add custom environment variables
        # lambda_function.add_environment("ENDPOINT_PARAMETER", self.endpoint_parameter)
        # lambda_function.add_environment(
        #     "SAGEMAKER_DEPLOY_ROLE_ARN", self.sagemaker_deploy_role.role_arn
        # )

        self.language_detection_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

        self.language_detection_lambda = lambda_.Alias(
            self,
            f"{lambda_name}alias",
            alias_name="provisioned",
            version=self.language_detection_lambda.current_version,
            provisioned_concurrent_executions=1,
            retry_attempts=0,
        )

    def create_language_translation_lambda(self):
        """
        This function will create language translation lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.DockerImageFunction(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.DockerImageCode().from_image_asset(directory=asset_location),
            description=lambda_desc,
            timeout=Duration.seconds(900),
            memory_size=10240,
            tracing=lambda_.Tracing.ACTIVE,
        )

        # Add custom environment variables
        # lambda_function.add_environment("ENDPOINT_PARAMETER", self.endpoint_parameter)
        # lambda_function.add_environment(
        #     "SAGEMAKER_DEPLOY_ROLE_ARN", self.sagemaker_deploy_role.role_arn
        # )

        self.language_translation_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

        self.language_translation_lambda = lambda_.Alias(
            self,
            f"{lambda_name}alias",
            alias_name="provisioned",
            version=self.language_translation_lambda.current_version,
            provisioned_concurrent_executions=1,
            retry_attempts=0,
        )

    def create_imagequality_load_lambda(self):
        """
        This function will load the json from image quality module to RDS
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["AWSDataWrangler", "idfbin38"]),
            timeout=Duration.seconds(600),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(
                prefix=self.outgoing_prefix_nosplit, suffix=".imagequality.json"
            ),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "DATABASE_CONNECTION_PARAMETER", self.database_connection_parameter
        )

        self.imagequality_load_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_pdfconverter_lambda(self):
        """
        This function will create pdf converter lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["pdfconverter", "libreoffice-brotli"]),
            timeout=Duration.seconds(300),
            memory_size=1024,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add S3 trigger
        # notification = s3_notify.LambdaDestination(lambda_function)
        # notification.bind(self, self.document_bucket)
        # self.document_bucket.add_object_created_notification(
        #     notification, s3.NotificationKeyFilter(prefix=self.pdfconversion_prefix)
        # )

        # Add custom environment variables
        lambda_function.add_environment(
            "OUTGOING_PREFIX_SPLIT", self.outgoing_prefix_split
        )
        lambda_function.add_environment(
            "OUTGOING_PREFIX_NOSPLIT", self.outgoing_prefix_nosplit
        )

        self.pdfconverter_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_excel_splitter_lambda(self):
        """
        This function will split excel into its individual sheets
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["excelutils"]),
            timeout=Duration.seconds(60),
            memory_size=256,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(prefix=self.incoming_prefix, suffix=".xlsx"),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "PDFCONVERSION_PREFIX", self.pdfconversion_prefix
        )

        self.excel_splitter_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_file_router_lambda(self):
        """
        This function will route the files based on extension to the correct folders
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            timeout=Duration.seconds(30),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        for extension in self.file_router_allowed_extensions.split(";"):
            self.document_bucket.add_object_created_notification(
                notification,
                s3.NotificationKeyFilter(prefix=self.incoming_prefix, suffix=extension),
            )

        # Add custom environment variables
        lambda_function.add_environment(
            "PDFCONVERSION_PREFIX", self.pdfconversion_prefix
        )

        self.file_router_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_pdfconverter_container_lambda(self):
        """
        This function will create pdf converter lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}: {asset_location}")
        lambda_function = lambda_.DockerImageFunction(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.DockerImageCode().from_image_asset(directory=asset_location),
            description=lambda_desc,
            timeout=Duration.seconds(300),
            memory_size=1024,
            tracing=lambda_.Tracing.ACTIVE,
        )

        # Add S3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(prefix=f"{self.pdfconversion_prefix}"),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "OUTGOING_PREFIX_SPLIT", self.outgoing_prefix_split
        )
        lambda_function.add_environment(
            "OUTGOING_PREFIX_NOSPLIT", self.outgoing_prefix_nosplit
        )

        self.pdfconverter_container_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    @staticmethod
    def enable_ssl(bucket):
        bucket.add_to_resource_policy(
            iam.PolicyStatement(
                effect=iam.Effect.DENY,
                actions=["s3:*"],
                resources=[
                    f"{bucket.bucket_arn}/*",
                    f"{bucket.bucket_arn}",
                ],
                principals=[iam.StarPrincipal()],
                conditions={"Bool": {"aws:SecureTransport": False}},
            )
        )

    # ASP
    def create_email_extractor_lambda(self):
        """
        This function will create email extractor lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}:{asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["dbutilities"]),
            timeout=Duration.seconds(900),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add s3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(prefix=self.incoming_prefix, suffix=".eml"),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "EMAIL_ALLOWED_ATTACHMENT_EXTENSIONS",
            self.email_allowed_attachment_extensions,
        )

        self.email_extractor_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    # ASP
    def create_msg_extractor_lambda(self):
        """
        This function will create .msg extractor lambda
        """

        (
            lambda_name,
            lambda_desc,
            lambda_policy_file,
            lambda_original_name,
        ) = self.lambda_attributes(sys._getframe().f_code.co_name)

        asset_location = str(Path(self.code_location).joinpath(lambda_original_name))
        self.logging.info(f"Code Location for {lambda_name}:{asset_location}")
        lambda_function = lambda_.Function(
            self,
            lambda_name,
            function_name=lambda_name,
            code=lambda_.Code.from_asset(asset_location),
            description=lambda_desc,
            handler="index.lambda_handler",
            layers=self.assign_lambda_layers(["dbutilities"]),
            timeout=Duration.seconds(900),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=PYTHON_RUNTIME,
        )

        # Add s3 trigger
        notification = s3_notify.LambdaDestination(lambda_function)
        notification.bind(self, self.document_bucket)
        self.document_bucket.add_object_created_notification(
            notification,
            s3.NotificationKeyFilter(prefix=self.incoming_prefix, suffix=".msg"),
        )

        # Add custom environment variables
        lambda_function.add_environment(
            "EMAIL_ALLOWED_ATTACHMENT_EXTENSIONS",
            self.email_allowed_attachment_extensions,
        )

        self.msg_extractor_lambda = self.common_setup_for_lambda(
            lambda_function, lambda_name, lambda_policy_file, asset_location
        )

    def create_buckets(self):
        """
        This function will create buckets
        """
        self.document_bucket = s3.Bucket(
            self,
            f"{self.app_name}-{self.document_bucket}",
            bucket_name=f"{self.app_name}-{self.document_bucket}",
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
            removal_policy=RemovalPolicy.DESTROY,
            cors=[
                s3.CorsRule(
                    allowed_methods=[
                        s3.HttpMethods.GET,
                        s3.HttpMethods.POST,
                        s3.HttpMethods.PUT,
                    ],
                    allowed_origins=["*"],
                    allowed_headers=["*"],
                )
            ],
        )
        self.enable_ssl(self.document_bucket)

        CfnOutput(
            self,
            f"{self.app_name}documentbucket",
            export_name=f"{self.app_name}documentbucket",
            value=self.document_bucket.bucket_name,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}documentbucketparameter",
            parameter_name=f"{self.cfnoutput_parameter}/documentingestionbucket",
            string_value=self.document_bucket.bucket_name,
            type=ssm.ParameterType.STRING,
        )

    @staticmethod
    def add_options_to_apigateway_method(apimethod):
        """
        This function will create OPTIONS method for API Gateway resource
        """
        options_ig = apigateway.IntegrationResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": "'Content-Type,Authorization,X-Amz-Date,X-Api-Key,X-Amz-Security-Token'",
                "method.response.header.Access-Control-Allow-Methods": "'DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT'",
                "method.response.header.Access-Control-Allow-Origin": "'*'",
            },
            # response_templates={"application/json": ""},
        )

        options_mr = apigateway.MethodResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Access-Control-Allow-Headers": True,
                "method.response.header.Access-Control-Allow-Methods": True,
                "method.response.header.Access-Control-Allow-Origin": True,
            },
            # response_models={"application/json": ""},
        )

        apimethod.add_method(
            "OPTIONS",
            integration=apigateway.MockIntegration(
                request_templates={"application/json": '{ "statusCode": 200 }'},
                integration_responses=[options_ig],
            ),
            method_responses=[options_mr],
        )

        return apimethod

    @staticmethod
    def add_method_response_security_headers():
        method_response_output = apigateway.MethodResponse(
            response_models={
                "application/json": apigateway.Model.EMPTY_MODEL,
            },
            response_parameters={
                "method.response.header.Strict-Transport-Security": True,
                "method.response.header.X-Content-Type-Options": True,
                "method.response.header.X-XSS-Protection": True,
                "method.response.header.Cache-Control": True,
                "method.response.header.X-Frame-Options": True,
                "method.response.header.Content-Security-Policy": True,
            },
            status_code="200",
        )

        return method_response_output

    @staticmethod
    def add_integration_response_security_headers():
        method_response_output = apigateway.IntegrationResponse(
            status_code="200",
            response_parameters={
                "method.response.header.Strict-Transport-Security": "'max-age=31536000; includeSubDomains;'"
                # "method.response.header.Strict-Transport-Security": "max-age=31536000; includeSubdomains; preload",
                # "method.response.header.X-Content-Type-Options": "nosniff",
                # "method.response.header.X-XSS-Protection": "1; mode=block",
                # "method.response.header.Cache-Control": "no-cache",
                # "method.response.header.X-Frame-Options": "DENY",
                # "method.response.header.Content-Security-Policy": "default-src *;",
            },
        )

        return method_response_output

    def create_api_gateway_receiver(self):
        """
        This function will create API Gateway and resources
        """

        # taken from https://oozio.medium.com/serverless-discord-bot-55f95f26f743
        requestSchema = '##  See http:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/api-gateway-mapping-template-reference.html\r\n##  This template will pass through all parameters including path, querystring, header, stage variables, and context through to the integration endpoint via the body\/payload\r\n##  \'rawBody\' allows passthrough of the (unsurprisingly) raw request body; similar to flask.request.data\r\n#set($allParams = $input.params())\r\n{\r\n"rawBody": "$util.escapeJavaScript($input.body).replace("\\\'", "\'")",\r\n"body-json" : $input.json(\'$\'),\r\n"params" : {\r\n#foreach($type in $allParams.keySet())\r\n    #set($params = $allParams.get($type))\r\n"$type" : {\r\n    #foreach($paramName in $params.keySet())\r\n    "$paramName" : "$util.escapeJavaScript($params.get($paramName))"\r\n        #if($foreach.hasNext),#end\r\n    #end\r\n}\r\n    #if($foreach.hasNext),#end\r\n#end\r\n},\r\n"stage-variables" : {\r\n#foreach($key in $stageVariables.keySet())\r\n"$key" : "$util.escapeJavaScript($stageVariables.get($key))"\r\n    #if($foreach.hasNext),#end\r\n#end\r\n},\r\n"context" : {\r\n    "account-id" : "$context.identity.accountId",\r\n    "api-id" : "$context.apiId",\r\n    "api-key" : "$context.identity.apiKey",\r\n    "authorizer-principal-id" : "$context.authorizer.principalId",\r\n    "caller" : "$context.identity.caller",\r\n    "cognito-authentication-provider" : "$context.identity.cognitoAuthenticationProvider",\r\n    "cognito-authentication-type" : "$context.identity.cognitoAuthenticationType",\r\n    "cognito-identity-id" : "$context.identity.cognitoIdentityId",\r\n    "cognito-identity-pool-id" : "$context.identity.cognitoIdentityPoolId",\r\n    "http-method" : "$context.httpMethod",\r\n    "stage" : "$context.stage",\r\n    "source-ip" : "$context.identity.sourceIp",\r\n    "user" : "$context.identity.user",\r\n    "user-agent" : "$context.identity.userAgent",\r\n    "user-arn" : "$context.identity.userArn",\r\n    "request-id" : "$context.requestId",\r\n    "resource-id" : "$context.resourceId",\r\n    "resource-path" : "$context.resourcePath"\r\n    }\r\n}'

        self.receiver_api_name = f"{self.app_name}{self.receiver_api_name}"

        log_group = logs.LogGroup(
            self,
            f"{self.receiver_api_name}apilogs",
            log_group_name=f"{self.receiver_api_name}apilogs",
        )

        receiver_api = apigateway.RestApi(
            self,
            self.receiver_api_name,
            rest_api_name=self.receiver_api_name,
            retain_deployments=False,
            deploy=False,
            endpoint_types=[apigateway.EndpointType.REGIONAL],
            # disable_execute_api_endpoint=True,
        )

        receiver_data_request_validator = receiver_api.add_request_validator(
            "DefaultValidator", validate_request_parameters=True
        )

        receiver_resource = receiver_api.root.add_resource(
            self.receiver_api_resource_name
        )
        receiver_resource.add_method(
            "POST",
            apigateway.LambdaIntegration(
                self.classification_receiver_lambda,
                allow_test_invoke=False,
                proxy=False,
                integration_responses=[
                    self.add_integration_response_security_headers()
                ],
            ),
            method_responses=[self.add_method_response_security_headers()],
        )
        receiver_resource.add_method(
            "GET",
            apigateway.LambdaIntegration(
                self.classification_receiver_lambda,
                allow_test_invoke=False,
                proxy=False,
                integration_responses=[
                    self.add_integration_response_security_headers()
                ],
            ),
            method_responses=[self.add_method_response_security_headers()],
        )
        receiver_resource = self.add_options_to_apigateway_method(receiver_resource)

        receiver_resource_1 = receiver_api.root.add_resource(
            self.receiver_api_resource_name_1
        )
        receiver_resource_1.add_method(
            "GET",
            apigateway.LambdaIntegration(
                self.document_data_lambda,
                allow_test_invoke=False,
                proxy=False,
                request_parameters={
                    "integration.request.querystring.bucketname": "method.request.querystring.bucketname",
                    "integration.request.querystring.keyname": "method.request.querystring.keyname",
                },
                request_templates={"application/json": requestSchema},
                integration_responses=[
                    self.add_integration_response_security_headers()
                ],
            ),
            request_validator=receiver_data_request_validator,
            request_parameters={
                "method.request.querystring.bucketname": True,
                "method.request.querystring.keyname": True,
            },
            method_responses=[self.add_method_response_security_headers()],
        )
        receiver_resource_1 = self.add_options_to_apigateway_method(receiver_resource_1)

        deployment = apigateway.Deployment(
            self, f"{self.receiver_api_name}Deployment", api=receiver_api
        )

        self.stage = apigateway.Stage(
            self,
            f"{self.receiver_api_name}{self.app_environment}",
            stage_name=self.app_environment,
            deployment=deployment,
            logging_level=apigateway.MethodLoggingLevel.INFO,
            data_trace_enabled=True,
            access_log_destination=apigateway.LogGroupLogDestination(log_group),
            access_log_format=apigateway.AccessLogFormat.json_with_standard_fields(
                caller=False,
                http_method=True,
                ip=True,
                protocol=True,
                request_time=True,
                resource_path=True,
                response_length=True,
                status=True,
                user=True,
            ),
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}classificationapiparameter",
            parameter_name=f"{self.cfnoutput_parameter}/{self.classification_url_parameter}",
            string_value=self.stage.url_for_path(f"/{self.receiver_api_resource_name}"),
            type=ssm.ParameterType.STRING,
        )

        domain_name = apigateway.DomainName.from_domain_name_attributes(
            self,
            self.api_custom_domain_name_parameter,
            domain_name=self.api_custom_domain_name,
            domain_name_alias_hosted_zone_id=self.api_custom_domain_name_zoneid,
            domain_name_alias_target=self.api_custom_domain_name_alias,
        )

        apigateway.BasePathMapping(
            self,
            f"{self.receiver_api_name}pathmapping",
            domain_name=domain_name,
            rest_api=receiver_api,
            base_path=self.receiver_api_name.replace(self.app_name, ""),
            stage=self.stage,
        )

    def create_step_functions(self):
        """
        This function will create step functions
        """

        self.classification_step_function = (
            f"{self.app_name}{self.classification_step_function}"
        )

        # Create State Machine and trigger
        self.classification_state_machine = (
            StepFunctionService.create_classification_step_function(
                self,
                self.classification_orchestrator_lambda,
            )
        )

    def assign_permissions(self):
        """
        Add permissions to the resources
        """
        self.classification_state_machine.grant_start_execution(
            self.classification_receiver_lambda
        )
        self.document_bucket.grant_read_write(self.pdfconverter_lambda)
        self.document_bucket.grant_read_write(self.pdfconverter_container_lambda)
        self.document_bucket.grant_read(self.document_processor_lambda)
        self.document_bucket.grant_read_write(self.language_detection_lambda)
        self.document_bucket.grant_read_write(self.language_translation_lambda)
        self.document_bucket.grant_read_write(self.imagequality_load_lambda)
        self.document_bucket.grant_read_write(self.email_extractor_lambda)
        self.document_bucket.grant_read_write(self.msg_extractor_lambda)
        self.document_bucket.grant_read_write(self.file_router_lambda)
        # ASP
        self.document_bucket.grant_read_write(self.excel_splitter_lambda)

        self.language_translation_lambda.grant_invoke(
            self.classification_orchestrator_lambda
        )
        self.language_detection_lambda.grant_invoke(
            self.classification_orchestrator_lambda
        )

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.logging = kwargs.pop("logging")
        self.config = vars(kwargs.pop("config"))
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")
        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()

        self.create_lambda_layers()

        self.create_buckets()

        self.create_language_detection_lambda()

        self.create_language_translation_lambda()

        self.create_classification_orchestrator_lambda()

        self.create_pdfconverter_lambda()

        self.create_pdfconverter_container_lambda()

        # ASP   adding new lambda
        self.create_email_extractor_lambda()
        
        self.create_msg_extractor_lambda()

        self.create_excel_splitter_lambda()

        self.create_file_router_lambda()

        self.create_step_functions()

        self.create_classification_receiver_lambda()

        self.create_classification_errors_lambda()

        self.create_classification_dataset_handler_lambda()

        self.create_document_data_lambda()

        self.create_api_gateway_receiver()

        self.create_document_processor_lambda()

        self.create_imagequality_load_lambda()

        self.assign_permissions()

"""
This is the CDK Stack creation file
"""
from pathlib import Path
import json
import os
import re
import sys
import subprocess
import time
import shutil
from dotenv import dotenv_values
from aws_cdk import (
    Stack,
    Duration,
    Size,
    CfnOutput,
    RemovalPolicy,
    Fn,
    aws_s3 as s3,
    aws_s3_deployment as s3deploy,
    aws_s3_notifications as s3_notify,
    aws_logs as logs,
    aws_iam as iam,
    aws_lambda as lambda_,
    aws_cloudwatch as cloudwatch,
    aws_cloudwatch_actions,
    aws_sns as sns,
    aws_ssm as ssm,
    aws_cognito as cognito,
    aws_secretsmanager as secretsmanager,
    aws_cloudfront_origins as origins,
    aws_certificatemanager as acm,
    aws_cloudfront as cloudfront,
    aws_codebuild as codebuild,
    aws_codepipeline as pipeline,
    aws_codepipeline_actions as codepipeline_actions,
)
from constructs import Construct

from aws_cdk.custom_resources import (
    AwsCustomResource,
    AwsCustomResourcePolicy,
    AwsSdkCall,
    PhysicalResourceId,
)

class CDKAppStack(Stack):
    """
    This class will create main stack
    """

    def from_ssm(self, path):
        """
        This function will return ssm value for parameter
        """
        # function to get parameters stored in ssm
        return ssm.StringParameter.from_string_parameter_attributes(
            self, path, parameter_name=path
        ).string_value
    
    def get_access_policy(self, policy_file):
        """
        This function will generate the policy file
        """
        self.logging.info(f"Applying policy file {policy_file}")
        data = {}
        with open(policy_file, "r") as pol_file:
            data = pol_file.read()
        
        data = json.loads(data)
        return data
        
    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass

    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.cfnoutput_parameter = self.cfnoutput_parameter.replace("#", self.app_name)
    
    @staticmethod
    def enable_ssl(bucket):
        bucket.add_to_resource_policy(
            iam.PolicyStatement(
                effect=iam.Effect.DENY,
                actions=["s3:*"],
                resources=[
                    f"{bucket.bucket_arn}/*",
                    f"{bucket.bucket_arn}",
                ],
                principals=[iam.StarPrincipal()],
                conditions={"Bool": {"aws:SecureTransport": False}},
            )
        )
        
    def create_buckets(self):
        """
        Create s3 bucket
        """
        self.asset_bucket = s3.Bucket(
            self,
            f"{self.app_name}-{self.asset_bucket}-test",
            bucket_name=f"{self.app_name}-{self.asset_bucket}-{self.region_name}-test"[:63].strip("-"),
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
        )
        self.enable_ssl(self.asset_bucket)
        
        self.code_bucket = s3.Bucket(
            self,
            f"{self.app_name}codebucket",
            encryption=s3.BucketEncryption.S3_MANAGED,
            enforce_ssl=True,
            versioned=True,
        )
        self.enable_ssl(self.code_bucket)
    
    @staticmethod
    def zip_directory(self,source_path,destn_path):
        try:
            shutil.make_archive(f"code", "zip", source_path)
            shutil.move("code.zip", f"{destn_path}/code.zip")
        except Exception as e:
            self.logging.info(f"Failed to zip folder due to: {e}")
            
    def load_bucket(self):
        """
        Load zip file to bucket
        """
        source_path = f"{self.code_location}/cip-portal-frontend/cip-portal-frontend"
        destn_path = f"{self.code_location}/cip-portal-frontend"
        self.zip_directory(self,source_path,destn_path)
        self.s3bucketdeployment = s3deploy.BucketDeployment(
            self,
            f"{self.app_name}copyfrontendcode",
            sources=[
                s3deploy.Source.asset(f"{destn_path}/code.zip")
            ],
            destination_bucket=self.code_bucket,
            extract=False
        )
        
    def create_codepipeline(self):
        """
        Create codebuild project and codepipeline for assets deployment to s3
        """
        
        buildspec_location = str(Path(self.code_location).joinpath("cip-portal-frontend/buildspec.yml"))

        self.frontend_codebuild_project = codebuild.PipelineProject(
            self,
            f"{self.app_name}cipportalfrontendcodebuild",
            project_name=f"{self.app_name}-cip-portalfrontend-codebuild",
            description="CodeBuild Project for Portal Frontend asset.",
            environment=codebuild.BuildEnvironment(
                build_image=codebuild.LinuxBuildImage.STANDARD_5_0
            ),
            build_spec=codebuild.BuildSpec.from_asset(buildspec_location)
        )
       
        source_artifact = pipeline.Artifact("SourceArtifact")
        build_artifact = pipeline.Artifact("BuildArtifact")

        # Create source stage for CodePipeline
        source_stage = pipeline.StageProps(
            stage_name="Source",
            actions=[
                codepipeline_actions.S3SourceAction(
                    action_name="S3Trigger",
                    bucket=self.code_bucket,
                    bucket_key=Fn.select(0, self.s3bucketdeployment.object_keys),
                    output=source_artifact,
                    trigger=codepipeline_actions.S3Trigger.POLL)
            ]
        )

        # Create build stage for CodePipeline
        build_stage = pipeline.StageProps(
            stage_name="Build",
            actions=[
                codepipeline_actions.CodeBuildAction(
                    action_name="Build",
                    input=source_artifact,
                    project=self.frontend_codebuild_project,
                    outputs=[build_artifact]
                )
            ]
        )

        # Create deploy stage for CodePipeline
        deploy_stage = pipeline.StageProps(
            stage_name="Deploy",
            actions=[
                codepipeline_actions.S3DeployAction(
                action_name="S3Deploy",
                bucket=self.asset_bucket,
                input=build_artifact
                )]
        )

        # Create CodePipeline 
        self.frontend_codepipeline = pipeline.Pipeline(
            self, 
            f"{self.app_name}cipportalfrontendpipeline",
            pipeline_name=f"{self.app_name}-cip-portalfrontend-pipeline",
            stages=[source_stage, build_stage, deploy_stage]
        )
        
    def import_edge_lambda(self):
        """
        Custom resource to import edge lambda arn from SSM parameter in another region
        """
        
        self.get_parameter = AwsCustomResource(
            self, 
            f"{self.app_name}importedgelambda",
            on_update=AwsSdkCall( 
                service="SSM",
                action="getParameter",
                parameters={
                    "Name": f"{self.cfnoutput_parameter}/{self.edge_lambda_version_parameter_name}"
                },
                region="us-east-1",
                physical_resource_id=PhysicalResourceId.of(str(time.time())),
            ),
            policy=AwsCustomResourcePolicy.from_statements(
                statements=[
                    iam.PolicyStatement(
                        effect=iam.Effect.ALLOW,
                        actions=["ssm:GetParameter"],
                        resources=[
                            f"arn:aws:ssm:us-east-1:{self.account_id}:parameter{self.cfnoutput_parameter}/{self.edge_lambda_version_parameter_name}"
                        ],
                    )
                ]
            ),
        )
        
    def create_cloudfront(self):
        """
        Create cloudfront distribution , response policy and cache policy
        """

        response_headers_policy = cloudfront.ResponseHeadersPolicy(
            self,
            f"{self.app_name}frontendresponseheaderspolicy-test",
            comment=f"Response header Policy for Portal frontend Cloudfront for app {self.app_name}",
            response_headers_policy_name=f"{self.app_name}FrontendSecurityHeadersPolicy-test",
            security_headers_behavior=cloudfront.ResponseSecurityHeadersBehavior(
                content_security_policy=cloudfront.ResponseHeadersContentSecurityPolicy(
                    content_security_policy="default-src 'self' 'unsafe-eval' 'unsafe-inline' *.accenture.com *.meaplatform.com https://fonts.gstatic.com; worker-src blob:; script-src 'self' 'unsafe-inline' 'unsafe-eval'; img-src 'self' *.cognitiveinsurance.accenture.com *.amazonaws.com *.googleusercontent.com data:; connect-src 'self' *.cognitiveinsurance.accenture.com *.amazoncognito.com *.googleusercontent.com *.meaplatform.com *.amazonaws.com; font-src 'self' https://fonts.gstatic.com data:; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com;",
                    override=True,
                ),
                content_type_options=cloudfront.ResponseHeadersContentTypeOptions(
                    override=True
                ),
                frame_options=cloudfront.ResponseHeadersFrameOptions(
                    frame_option=cloudfront.HeadersFrameOption.SAMEORIGIN, override=True
                ),
                strict_transport_security=cloudfront.ResponseHeadersStrictTransportSecurity(
                    access_control_max_age=Duration.seconds(31536000),
                    include_subdomains=True,
                    override=True,
                ),
                xss_protection=cloudfront.ResponseHeadersXSSProtection(
                    protection=True,
                    mode_block=True,
                    override=True,
                ),
            ),
            custom_headers_behavior=cloudfront.ResponseCustomHeadersBehavior(
                custom_headers=[cloudfront.ResponseCustomHeader(header="Cache-Control", value="private, max-age=0", override=True)]
            ),
        )

        cache_policy = cloudfront.CachePolicy(
            self,
            f"{self.app_name}frontendcachepolicy-test",
            cache_policy_name=f"{self.app_name}frontendcachepolicy-test", 
            comment=f"Cache Policy for Portal frontend Cloudfront for app {self.app_name}", 
            default_ttl=Duration.seconds(604800), 
            enable_accept_encoding_brotli=True, 
            enable_accept_encoding_gzip=True,
            max_ttl=Duration.seconds(604800), 
            min_ttl=Duration.seconds(0), 
            )
        
        cf_logs_bucket = s3.Bucket(
            self,
            f"{self.app_name}frontendcflogsbucket-test",
            bucket_name=f"{self.app_name}-frontendcflogs-{self.region_name}-test"[:63].strip("-"),
            block_public_access=s3.BlockPublicAccess.BLOCK_ALL,
            object_ownership=s3.ObjectOwnership.BUCKET_OWNER_PREFERRED,
            removal_policy=RemovalPolicy.DESTROY,
            encryption=s3.BucketEncryption.S3_MANAGED,
        )
        cf_logs_bucket.add_to_resource_policy(
            iam.PolicyStatement(
                actions=["s3:GetBucketAcl", "s3:PutBucketAcl"],
                resources=[cf_logs_bucket.bucket_arn],
                principals=[iam.ServicePrincipal("cloudfront.amazonaws.com")],
            )
        )
        
        bucket_origin = origins.S3Origin(self.asset_bucket)
        
        frontend_distribution = cloudfront.Distribution(
            self,
            f"{self.app_name}cloudfront-test",
            comment="test cloudfront",
            default_behavior=cloudfront.BehaviorOptions(
                origin=bucket_origin,
                viewer_protocol_policy=cloudfront.ViewerProtocolPolicy.HTTPS_ONLY,
                cache_policy=cloudfront.CachePolicy.CACHING_DISABLED,
                allowed_methods=cloudfront.AllowedMethods.ALLOW_GET_HEAD,
                response_headers_policy=response_headers_policy,
                edge_lambdas=[cloudfront.EdgeLambda(
                    function_version=lambda_.Version.from_version_arn(
                        self,
                        "edgelambda",
                        self.get_parameter.get_response_field("Parameter.Value")),
                    event_type=cloudfront.LambdaEdgeEventType.ORIGIN_REQUEST
                )
                ]
            ),
            additional_behaviors={
                "assets/*": cloudfront.BehaviorOptions(
                origin=bucket_origin,
                viewer_protocol_policy=cloudfront.ViewerProtocolPolicy.HTTPS_ONLY,
                cache_policy=cache_policy,
                allowed_methods=cloudfront.AllowedMethods.ALLOW_GET_HEAD,
            )} ,
            enable_logging=True,
            log_bucket=cf_logs_bucket,
        )

        frontend_distribution.node.add_dependency(cf_logs_bucket)

    def create_cognito_userpool(self):
        """
        Create Cognito User Pool and client
        """
        
        self.userpool = cognito.UserPool(
            self,
            f"{self.app_name}-userpool",
            user_pool_name=f"{self.app_name}-userpool",
            sign_in_case_sensitive=False,
            account_recovery=cognito.AccountRecovery.NONE,
            custom_attributes={
                "role": cognito.StringAttribute(min_len=1, max_len=256, mutable=True)
                }
            )
            
        cfn_user_pool_domain = cognito.CfnUserPoolDomain(
            self, 
            f"{self.app_name}-userpooldomain",
            domain=f"{self.app_name}-userpooldomain",
            user_pool_id=self.userpool.user_pool_id,
        )

        self.userpool_client = self.userpool.add_client(
            f"{self.app_name}-portal-client",
            user_pool_client_name=f"{self.app_name}-portal-client",
            refresh_token_validity=Duration.days(1),
            auth_flows=cognito.AuthFlow(
                user_password=True
            ),
            o_auth=cognito.OAuthSettings(
                flows=cognito.OAuthFlows(
                    authorization_code_grant=True
                ),
                scopes=[cognito.OAuthScope.OPENID,cognito.OAuthScope.PROFILE,cognito.OAuthScope.EMAIL],
                callback_urls=[f"https://{self.frontend_custom_domain_name}/redirect"],
                logout_urls=[f"https://{self.frontend_custom_domain_name}/user-logged-out"]
            )
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}cognitouserpoolparameter",
            parameter_name=f"{self.cfnoutput_parameter}/{self.cognitouserpool_parameter_name}",
            description=f"{self.cognitouserpool_parameter_description}",
            string_value=json.dumps(
                {"user_pool_id":self.userpool.user_pool_id}
            ),
        )
        
        ssm.StringParameter(
            self,
            f"{self.app_name}authorizerissuerparameter",
            parameter_name=f"{self.cfnoutput_parameter}/{self.authorizerissuer_parameter_name}",
            description=f"{self.authorizerissuer_parameter_description}",
            string_value=json.dumps(
                {self.app_name:{"iss":self.userpool.user_pool_provider_url,"aud":[self.userpool_client.user_pool_client_id]}}
            ),
        )
    
    def create_identity_pool(self):
        """
        Create Cognito Identity Pool
        """
        
        self.cognito_identity_pool = cognito.CfnIdentityPool(
            self, 
            f"{self.app_name}-identitypool",
            identity_pool_name= f"{self.app_name}-identitypool",
            cognito_identity_providers=[cognito.CfnIdentityPool.CognitoIdentityProviderProperty(
                client_id=self.userpool_client.user_pool_client_id,
                provider_name=self.userpool.user_pool_provider_name,
            )],
            allow_unauthenticated_identities=False
            )
        
        identitypool_policy = self.get_access_policy(
            Path(self.policies_path).joinpath("identitypoolrole_policy.json")
        )
        
        all_statements = []
        for statement in identitypool_policy:
            all_statements.append(iam.PolicyStatement().from_json(statement))
        
        identity_pool_role = iam.Role(
            self,
            f"{self.app_name}identitypoolrole",
            role_name=f"{self.app_name}-identitypoolrole",
            assumed_by=iam.WebIdentityPrincipal(
                "cognito-identity.amazonaws.com", {
                "StringEquals": {"cognito-identity.amazonaws.com:aud": self.cognito_identity_pool.ref},
                "ForAnyValue:StringLike": {"cognito-identity.amazonaws.com:amr": "authenticated"}
            }),
            description="IAM role for cognito identity pool",
            inline_policies=dict(
                inline_policy_from_cdk=iam.PolicyDocument(statements=all_statements)
            ),
        )
        
        cfn_identity_pool_role_attachment = cognito.CfnIdentityPoolRoleAttachment(
            self, 
            f"{self.app_name}-identitypoolroleattachment",
            identity_pool_id=self.cognito_identity_pool.ref,
            roles={'authenticated': identity_pool_role.role_arn}
        )

    def assign_permissions(self):
        """
        Add permissions to the resources
        """
        self.code_bucket.grant_read_write(self.frontend_codepipeline.role)
        self.code_bucket.grant_read_write(self.frontend_codebuild_project)  
        
    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.logging = kwargs.pop("logging")
        self.config = vars(kwargs.pop("config"))
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")
        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()
        
        self.create_cognito_userpool()
        
        self.create_identity_pool()
        
        self.create_buckets()
        
        self.load_bucket()
        
        self.create_codepipeline()
        
        self.import_edge_lambda()
        
        self.create_cloudfront()
        
        self.assign_permissions()
        
class CDKAppStackEdgeLambda(Stack):
    """
    This stack will create Edge lambda in us-east-1 region
    """
    def env_vars_quick_load(self):
        self.logging.info(f"Applying .env(s) to stack")
        for k, v in self.config.items():
            try:
                setattr(self, k.lower(), v)
            except:
                pass
            
    def initialize_constants(self):
        """
        This function will initialize constants
        """
        # constants
        self.env_vars_quick_load()

        self.cfnoutput_parameter = self.cfnoutput_parameter.replace("#", self.app_name)
    
    def create_edge_lambda(self):
        """
        Creae edge lambda in N Virginia region
        """
        
        edgelambda_role = iam.Role(
            self,
            f"{self.app_name}edgelambdarole-test",
            assumed_by=iam.CompositePrincipal(
                iam.ServicePrincipal("edgelambda.amazonaws.com"),
                iam.ServicePrincipal("lambda.amazonaws.com"),
            ),
            description="This role will be assumed by edge lambda.",
            role_name=f"{self.app_name}edgelambdarole-test",
        )
        edgelambda_role.add_managed_policy(
            iam.ManagedPolicy.from_aws_managed_policy_name(
                "service-role/AWSLambdaBasicExecutionRole"
            )
        )
        
        asset_location = str(Path(self.code_location).joinpath(self.edge_lambda_name))
        #does it need xray?
        edge_lambda = lambda_.Function(
            self,
            f"{self.app_name}{self.edge_lambda_name}-test",
            function_name=f"{self.app_name}{self.edge_lambda_name}-test",
            code=lambda_.Code.from_asset(asset_location),
            description=self.edge_lambda_description,
            handler="index.handler",
            role=edgelambda_role,
            timeout=Duration.seconds(3),
            memory_size=128,
            tracing=lambda_.Tracing.ACTIVE,
            runtime=lambda_.Runtime.NODEJS_14_X,
        )

        ssm.StringParameter(
            self,
            f"{self.app_name}{self.edge_lambda_version_parameter_name}",
            parameter_name=f"{self.cfnoutput_parameter}/{self.edge_lambda_version_parameter_name}",
            description=self.edge_lambda_version_parameter_description,
            string_value=edge_lambda.current_version.function_arn,
        )

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        """
        Initialization for CDK
        """
        self.invocation_directory = kwargs.pop("INVOCATION_DIRECTORY")
        self.code_location = kwargs.pop("CODE_LOCATION")
        self.region_name = kwargs["env"].region
        self.app_environment = kwargs.pop("app_environment")
        self.config = vars(kwargs.pop("config"))
        self.logging = kwargs.pop("logging")
        self.is_velocity = kwargs.pop("is_velocity")
        self.account_id = kwargs.pop("account")

        super().__init__(scope, construct_id, **kwargs)

        self.initialize_constants()
        
        self.create_edge_lambda()